{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import dlc_practical_prologue as prologue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper.py\n",
    "\n",
    "# Count the number of parameters\n",
    "def count_param(model):\n",
    "    return sum([torch.numel(param) for param in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader.py\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def load_data(N=1000, batch_size=50, seed=42):\n",
    "    # Load data\n",
    "    train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(N)\n",
    "    \n",
    "    # Normalize data\n",
    "    mean, std = train_input.mean(), train_input.std()\n",
    "    train_input.sub_(mean).div_(std)\n",
    "    test_input.sub_(mean).div_(std)\n",
    "    \n",
    "    # Generate dataset\n",
    "    train_data = TensorDataset(train_input, train_target, train_classes)\n",
    "    test_data = TensorDataset(test_input, test_target, test_classes)\n",
    "    \n",
    "    # For reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Generate data loader\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = load_data(N=1000, batch_size=50, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, data_loader):\n",
    "\n",
    "    nb_data_errors = 0\n",
    "\n",
    "    for data_input, data_target, data_classes in data_loader:\n",
    "        data_target = torch.nn.functional.one_hot(data_target)\n",
    "        output = model(data_input)\n",
    "        nb_error = torch.sum(torch.argmax(output, dim=1, keepdim=True) != torch.argmax(data_target, dim=1, keepdim=True))\n",
    "        nb_data_errors += nb_error\n",
    "        \n",
    "    return nb_data_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 32, kernel_size=5)    # size [nb, 32, 10, 10]\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=2)   # size [nb, 64, 4, 4]\n",
    "        self.fc1 = nn.Linear(256, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "        self.fc3 = nn.Linear(10, 2)\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2)) # size [nb, 32, 5, 5]      \n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2)) # size [nb, 64, 2, 2]\n",
    "        x = x.view(-1, 256) # size [nb, 256]\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63320"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BaseNet = BaseNet()\n",
    "\n",
    "# Calculate the number of parameters in the model\n",
    "count_param(BaseNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, eta, n_epochs=25, verbose=False):\n",
    "\n",
    "    binary_crit = nn.CrossEntropyLoss()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=eta)\n",
    "\n",
    "    tr_losses = []\n",
    "    tr_accuracies = []\n",
    "\n",
    "    for e in range(n_epochs):\n",
    "        # Reset training/validation loss\n",
    "        tr_loss = 0\n",
    "\n",
    "        # Training model\n",
    "        model.train()\n",
    "\n",
    "        for train_input, train_target, train_classes in iter(train_loader):\n",
    "            # Forward pass\n",
    "            output = model(train_input)\n",
    "\n",
    "            # Binary classification loss\n",
    "            binary_loss = binary_crit(output, train_target)\n",
    "        \n",
    "            # Total loss = Binary loss\n",
    "            tr_loss += binary_loss\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            binary_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Collect accuracy data\n",
    "        tr_accuracies.append(compute_nb_errors(model, train_loader)/1000)\n",
    "\n",
    "        # Collect loss data\n",
    "        tr_losses.append(tr_loss)\n",
    "\n",
    "        if verbose:\n",
    "            print('Epoch %d/%d, Binary loss: %.3f' %\n",
    "                  (e+1, n_epochs, tr_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spend 7.052693e+00 s\n",
      "tensor(0.9850) tensor(0.8090)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "time1 = time.perf_counter()\n",
    "model = BaseNet()\n",
    "train(model, train_loader, 0.001, 25, verbose=False)\n",
    "time2 = time.perf_counter()\n",
    "print('Spend {:e} s'.format(time2 - time1))\n",
    "\n",
    "tr_accuracy = 1 - compute_nb_errors(model, train_loader)/1000\n",
    "te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "print(tr_accuracy, te_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.8240), tensor(0.8310), tensor(0.8280), tensor(0.8300), tensor(0.8270), tensor(0.8190), tensor(0.8210), tensor(0.8160), tensor(0.8190), tensor(0.8060)] tensor(0.8221) tensor(0.0076)\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "model = BaseNet()\n",
    "for i in range(10):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=50, seed=42)\n",
    "    train(model, train_loader, 0.001, 25, verbose=False)\n",
    "    te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "    accuracies.append(te_accuracy)\n",
    "print(accuracies, torch.Tensor(accuracies).mean(), torch.Tensor(accuracies).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
