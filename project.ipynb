{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import dlc_practical_prologue as prologue\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\jizhu\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10000). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from MLP_models import *\n",
    "from CNN_models import *\n",
    "from Resnet_models import *\n",
    "from helpers import *\n",
    "from data_loader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of parameters of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters in model_1 is 73314\n",
      "The number of parameters in model_2 is 33172\n",
      "The number of parameters in model_3 is 66302\n",
      "The number of parameters in model_4 is 33172\n",
      "The number of parameters in model_5 is 72536\n",
      "The number of parameters in model_6 is 72268\n",
      "The number of parameters in model_7 is 144494\n",
      "The number of parameters in model_8 is 72268\n",
      "The number of parameters in model_9 is 75746\n",
      "The number of parameters in model_10 is 77812\n",
      "The number of parameters in model_11 is 152692\n",
      "The number of parameters in model_12 is 77812\n"
     ]
    }
   ],
   "source": [
    "model_1 = MLP()\n",
    "model_2 = SiameseMLP()\n",
    "model_3 = AuxMLP()\n",
    "model_4 = AuxsiameseMLP()\n",
    "model_5 = CNN()\n",
    "model_6 = SiameseCNN()\n",
    "model_7 = AuxCNN()\n",
    "model_8 = AuxsiameseCNN()\n",
    "model_9 = ResNet()\n",
    "model_10 = SiameseResNet()\n",
    "model_11 = AuxResNet()\n",
    "model_12 = AuxsiameseResNet()\n",
    "\n",
    "models = [model_1, model_2, model_3, model_4, model_5, model_6, model_7, model_8, model_9, model_10, model_11, model_12]\n",
    "for i in range(len(models)):\n",
    "    #print('The number of parameters in' count_param(i))\n",
    "    print('The number of parameters in model_%d is %d' %\n",
    "                  (i+1, count_param(models[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, eta, decay, n_epochs=25, verbose=False, siamese=False, aux=False, alpha = 0):\n",
    "    '''\n",
    "    model: learning model\n",
    "    '''\n",
    "    binary_crit = torch.nn.BCELoss()\n",
    "    aux_crit = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=eta, weight_decay=decay)\n",
    "    tr_losses = []\n",
    "    tr_accuracies = []\n",
    "\n",
    "    for e in range(n_epochs):\n",
    "        # Reset training/validation loss\n",
    "        tr_loss = 0\n",
    "\n",
    "        # Training model\n",
    "        model.train()\n",
    "\n",
    "        for train_input, train_target, train_classes in iter(train_loader):\n",
    "            # Forward pass\n",
    "            \n",
    "            if aux == True:\n",
    "                train_1, train_2 = train_input.unbind(1)\n",
    "                output, aux1, aux2 = model(train_1.unsqueeze(1), train_2.unsqueeze(1))\n",
    "                \n",
    "            elif siamese == True:\n",
    "                train_1, train_2 = train_input.unbind(1)\n",
    "                output = model(train_1.unsqueeze(1), train_2.unsqueeze(1))\n",
    "                \n",
    "            else:\n",
    "                output = model(train_input)\n",
    "                \n",
    "            # Binary classification loss\n",
    "            binary_loss = binary_crit(output, train_target.float())\n",
    "            total_loss = binary_loss\n",
    "            \n",
    "            # Auxiliary loss\n",
    "            if aux == True:\n",
    "\n",
    "                aux_loss1 = aux_crit(aux1, train_classes[:,0])\n",
    "                aux_loss2 = aux_crit(aux2, train_classes[:,1])\n",
    "                aux_loss = aux_loss1 + aux_loss2\n",
    "                \n",
    "                # Total loss = Binary loss + aux loss * alpha\n",
    "                total_loss = binary_loss + aux_loss * alpha\n",
    "            \n",
    "            tr_loss += total_loss\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Collect loss data\n",
    "        tr_losses.append(tr_loss)\n",
    "\n",
    "        if verbose:\n",
    "            print('Epoch %d/%d, Binary loss: %.3f' %\n",
    "                  (e+1, n_epochs, tr_loss))\n",
    "    return tr_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accu(model, train_loader, test_loader, siamese = False, aux = False):\n",
    "    \n",
    "    if siamese == False and aux == False:\n",
    "        tr_accuracy = 1 - compute_nb_errors(model, train_loader)/1000\n",
    "        te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "    elif aux == True:\n",
    "        tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "        te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "    else:\n",
    "        tr_accuracy = 1 - compute_nb_errors_siamese(model, train_loader)/1000\n",
    "        te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "            \n",
    "    return tr_accuracy, te_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_model = [[MLP, False, False],\n",
    "              [SiameseMLP, True, False],\n",
    "              [AuxMLP, False, True],\n",
    "              [AuxsiameseMLP, True, True],\n",
    "              [CNN, False, False],\n",
    "              [SiameseCNN, True, False],\n",
    "              [AuxCNN, False, True],\n",
    "              [AuxsiameseCNN, True, True],\n",
    "              [ResNet, False, False],\n",
    "              [SiameseResNet, True, False],\n",
    "              [AuxResNet, False, True],\n",
    "              [AuxsiameseResNet, True, True]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune learning rate and batch size for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal parameters for model_1: learning rate 0.00100, batch size: 16\n",
      "The optimal parameters for model_2: learning rate 0.00500, batch size: 8\n",
      "The optimal parameters for model_3: learning rate 0.00500, batch size: 32\n",
      "The optimal parameters for model_4: learning rate 0.00500, batch size: 8\n",
      "The optimal parameters for model_5: learning rate 0.00050, batch size: 8\n",
      "The optimal parameters for model_6: learning rate 0.00010, batch size: 128\n",
      "The optimal parameters for model_7: learning rate 0.00100, batch size: 32\n",
      "The optimal parameters for model_8: learning rate 0.00010, batch size: 128\n",
      "The optimal parameters for model_9: learning rate 0.00100, batch size: 32\n",
      "The optimal parameters for model_10: learning rate 0.00500, batch size: 8\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:73] data. DefaultCPUAllocator: not enough memory: you tried to allocate 31360000 bytes. Buy new RAM!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-eb7dfb4281be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m                 \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                 \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_sizes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m                 \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgammas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msiamese\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maux\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[0mtr_accuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mte_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msiamese\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maux\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\Project1-main\\Project1-main\\data_loader.py\u001b[0m in \u001b[0;36mload_data\u001b[1;34m(N, batch_size, seed)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Load data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mtrain_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprologue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_pair_sets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mtrain_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mtest_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\Project1-main\\Project1-main\\dlc_practical_prologue.py\u001b[0m in \u001b[0;36mgenerate_pair_sets\u001b[1;34m(nb)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[0mtest_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMNIST\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/mnist/'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m     \u001b[0mtest_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m     \u001b[0mtest_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:73] data. DefaultCPUAllocator: not enough memory: you tried to allocate 31360000 bytes. Buy new RAM!"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "std = []\n",
    "model_number = 0\n",
    "for models in Train_model:\n",
    "    gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "    batch_sizes = [8, 16, 32, 64, 128]\n",
    "    test_accuracies = torch.empty((len(gammas), len(batch_sizes)))\n",
    "    test_stds = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "    for j in range(len(gammas)):\n",
    "        for k in range(len(batch_sizes)):\n",
    "            accurate = []\n",
    "            for i in range(10):\n",
    "                model = models[0]()\n",
    "                model.to(device)\n",
    "                train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "                train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=models[1], aux=models[2])\n",
    "                tr_accuracy, te_accuracy = accu(model, train_loader, test_loader, siamese=models[1], aux=models[2])\n",
    "                accurate.append(te_accuracy)\n",
    "            test_accuracies[j,k] =  torch.FloatTensor(accurate).mean()\n",
    "            test_stds[j,k] =  torch.FloatTensor(accurate).std()\n",
    "    accuracy.append(test_accuracies)\n",
    "    std.append(test_stds)\n",
    "    max_index = test_accuracies.argmax() \n",
    "    model_number += 1\n",
    "    print('The optimal parameters for model_%d: learning rate %.5f, batch size: %d' %(model_number, gammas[(max_index+1)//5], batch_sizes[(max_index+1)%5-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune alpha for models with auxiliary loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.5550e-01, 8.2610e-01, 8.3650e-01, 8.1070e-01, 8.3090e-01],\n",
       "        [1.0561e-38, 7.3470e-39, 1.0194e-38, 9.2755e-39, 1.0653e-38],\n",
       "        [4.1327e-39, 8.9082e-39, 1.0102e-38, 7.3470e-39, 1.0194e-38],\n",
       "        [9.2755e-39, 1.0653e-38, 4.1327e-39, 8.9082e-39, 1.0102e-38]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.empty(4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5%3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.) tensor(0.8110)\n",
      "tensor(1.) tensor(0.8030)\n",
      "tensor(1.) tensor(0.8140)\n",
      "tensor(1.) tensor(0.8200)\n",
      "tensor(1.) tensor(0.8110)\n",
      "tensor(1.) tensor(0.8140)\n",
      "tensor(1.) tensor(0.8180)\n",
      "tensor(1.) tensor(0.8200)\n",
      "tensor(1.) tensor(0.8110)\n",
      "tensor(1.) tensor(0.8110)\n",
      "Mean: 0.813, Std: 0.005\n"
     ]
    }
   ],
   "source": [
    "accuracies8 = []\n",
    "times8 = []\n",
    "losses8 = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=8, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    #model = SiameseBaseNet()\n",
    "    model = MLP()\n",
    "    model.to(device)\n",
    "    losses8[i-10, :] = torch.tensor(train(model, train_loader, 5e-4, 0, 25, verbose=False, siamese=False))\n",
    "    time2 = time.perf_counter()\n",
    "    times8.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies8.append(te_accuracy)\n",
    "\n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies8).mean(), torch.tensor(accuracies8).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times8).mean(), torch.tensor(times8).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies9 = []\n",
    "times9 = []\n",
    "losses9 = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=8, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    \n",
    "    model = SiameseMLP()\n",
    "    model.to(device)\n",
    "\n",
    "    losses9[i-10,:] = torch.tensor(train(model, train_loader, 5e-3, 0, 25, verbose=False, siamese=True))\n",
    "    time2 = time.perf_counter()\n",
    "    times9.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_siamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies9.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies9).mean(), torch.tensor(accuracies9).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times9).mean(), torch.tensor(times9).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies10 = []\n",
    "times10 = []\n",
    "losses10 = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=16, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "\n",
    "    model = AuxMLP()\n",
    "    model.to(device)\n",
    "    losses10[i-10, :] = torch.tensor(train(model, train_loader, 5e-3, 0, 25, verbose=False, siamese=False, aux=True, alpha = 0.9))\n",
    "    time2 = time.perf_counter()\n",
    "    times10.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies10.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies10).mean(), torch.tensor(accuracies10).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times10).mean(), torch.tensor(times10).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies11 = []\n",
    "times11 = []\n",
    "losses11 = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=8, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "\n",
    "    model = AuxsiameseMLP()\n",
    "    model.to(device)\n",
    "    losses11[i-10, :] = torch.tensor(train(model, train_loader, 5e-3, 0, 25, verbose=False, siamese=True, aux=True, alpha = 0.7))\n",
    "    time2 = time.perf_counter()\n",
    "    times11.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies11.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies11).mean(), torch.tensor(accuracies11).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times11).mean(), torch.tensor(times11).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "times = []\n",
    "losses = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=16, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "\n",
    "    model = BaseNet()\n",
    "    model.to(device)\n",
    "    losses[i-10, :] = torch.tensor(train(model, train_loader, 5e-4, 0, 25, verbose=False, siamese=False))\n",
    "    time2 = time.perf_counter()\n",
    "    times.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies.append(te_accuracy)\n",
    "\n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies).mean(), torch.tensor(accuracies).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times).mean(), torch.tensor(times).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies1 = []\n",
    "times1 = []\n",
    "losses1 = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=16, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    model = SiameseBaseNet()\n",
    "    model.to(device)\n",
    "\n",
    "    losses1[i-10, :] = torch.tensor(train(model, train_loader, 1e-3, 0, 25, verbose=False, siamese=True))\n",
    "    time2 = time.perf_counter()\n",
    "    times1.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_siamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies1.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies1).mean(), torch.tensor(accuracies1).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times1).mean(), torch.tensor(times1).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies2 = []\n",
    "times2 = []\n",
    "losses2 = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=8, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "\n",
    "    model = AuxBaseNet()\n",
    "    model.to(device)\n",
    "    losses2[i-10, :] = torch.tensor(train(model, train_loader, 5e-4, 0, 25, verbose=False, siamese=False, aux=True, alpha = 1.0))\n",
    "    time2 = time.perf_counter()\n",
    "    times2.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies2.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies2).mean(), torch.tensor(accuracies2).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times2).mean(), torch.tensor(times2).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies3 = []\n",
    "times3 = []\n",
    "losses3 = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=32, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    \n",
    "    model = AuxsiameseBaseNet()\n",
    "    model.to(device)\n",
    "    losses3[i-10, :] = torch.tensor(train(model, train_loader, 5e-3, 0, 25, verbose=False, siamese=True, aux=True, alpha = 0.6))\n",
    "    time2 = time.perf_counter()\n",
    "    times3.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies3.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies3).mean(), torch.tensor(accuracies3).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times3).mean(), torch.tensor(times3).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.) tensor(0.8200)\n",
      "tensor(1.) tensor(0.8120)\n",
      "tensor(1.) tensor(0.8090)\n",
      "tensor(1.) tensor(0.8200)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f9310f3622ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_residual_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mlosses4\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msiamese\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtime2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtimes4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-3d6d9653608e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, eta, decay, n_epochs, verbose, siamese, aux, alpha)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "accuracies4 = []\n",
    "times4 = []\n",
    "losses4 = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=32, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "\n",
    "    model = ResNet(nb_residual_blocks = 4, input_channels = 2, nb_channels = 16, kernel_size = 3, nb_classes = 2)\n",
    "    model.to(device)\n",
    "    losses4[i-10, :] = torch.tensor(train(model, train_loader, 1e-3, 0, 25, verbose=False, siamese=False))\n",
    "    time2 = time.perf_counter()\n",
    "    times4.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies4.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies4).mean(), torch.tensor(accuracies4).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times4).mean(), torch.tensor(times4).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies5 = []\n",
    "times5 = []\n",
    "losses5 = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=32, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    \n",
    "    model = SiameseResNet(nb_residual_blocks = 4, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "    model.to(device)\n",
    "    losses5[i-10, :] = torch.tensor(train(model, train_loader, 5e-3, 0, 25, verbose=False, siamese=True))\n",
    "    time2 = time.perf_counter()\n",
    "    times5.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_siamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies5.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies5).mean(), torch.tensor(accuracies5).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times5).mean(), torch.tensor(times5).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies6 = []\n",
    "times6 = []\n",
    "losses6 = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=16, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    \n",
    "    model = AuxResNet(nb_residual_blocks = 10, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "    model.to(device)\n",
    "    losses6[i-10, :] = torch.tensor(train(model, train_loader, 1e-3, 0, 25, verbose=False, siamese=False, aux=True, alpha = 0.6))\n",
    "    time2 = time.perf_counter()\n",
    "    times6.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies6.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies6).mean(), torch.tensor(accuracies6).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times6).mean(), torch.tensor(times6).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies7 = []\n",
    "times7 = []\n",
    "losses7 = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=32, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    #model = SiameseBaseNet()\n",
    "    #model = BaseNet()\n",
    "    model = AuxsiameseResNet(nb_residual_blocks = 4, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "    model.to(device)\n",
    "    losses7[i-10, :] = torch.tensor(train(model, train_loader, 5e-3, 0, 25, verbose=False, siamese=True, aux=True, alpha = 0.6))\n",
    "    time2 = time.perf_counter()\n",
    "    times7.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies7.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies7).mean(), torch.tensor(accuracies7).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times7).mean(), torch.tensor(times7).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize learning rate and batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies1 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = MLP()\n",
    "            model.to(device)\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=False)\n",
    "            te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies1[j,k] =  torch.cuda.FloatTensor(accurate).mean()\n",
    "print(test_accuracies1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies2 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = SiameseMLP()\n",
    "            model.to(device)\n",
    "            #model = BaseNet()\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=True)\n",
    "            te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies2[j,k] =  torch.cuda.FloatTensor(accurate).mean()\n",
    "print(test_accuracies2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies3 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = AuxMLP()\n",
    "            model.to(device)\n",
    "            #model = BaseNet()\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=False, aux=True, alpha = 0.0)\n",
    "            te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies3[j,k] =  torch.cuda.FloatTensor(accurate).mean()\n",
    "print(test_accuracies3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies5 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = BaseNet()\n",
    "            model.to(device)\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=False)\n",
    "            te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies5[j,k] =  torch.cuda.FloatTensor(accurate).mean()\n",
    "print(test_accuracies5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies6 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = SiameseBaseNet()\n",
    "            model.to(device)\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=True)\n",
    "            te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies6[j,k] =  torch.cuda.FloatTensor(accurate).mean()\n",
    "print(test_accuracies6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies7 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = AuxBaseNet()\n",
    "            model.to(device)\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=False, aux=True, alpha = 0.0)\n",
    "            te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies7[j,k] =  torch.cuda.FloatTensor(accurate).mean()\n",
    "print(test_accuracies7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies9 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = ResNet(nb_residual_blocks = 4, input_channels = 2, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "            model.to(device)\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=False)\n",
    "            te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies9[j,k] =  torch.cuda.FloatTensor(accurate).mean()\n",
    "print(test_accuracies9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies10 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = SiameseResNet(nb_residual_blocks = 4, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "            model.to(device)\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            loss = train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=True)\n",
    "            te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies10[j,k] =  torch.FloatTensor(accurate).mean()\n",
    "print(test_accuracies10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies11 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = AuxResNet(nb_residual_blocks = 4, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "            model.to(device)\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            loss = train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=False, aux=True, alpha = 0.0)\n",
    "            te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies11[j,k] =  torch.FloatTensor(accurate).mean()\n",
    "print(test_accuracies11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize alpha for auxiliary loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(11):\n",
    "    accuracies100 = []\n",
    "\n",
    "    for i in range(10):\n",
    "        train_loader, test_loader = load_data(N=1000, batch_size=16, seed=i)\n",
    "        \n",
    "        model = AuxMLP()\n",
    "        model.to(device)\n",
    "        loss = train(model, train_loader, 5e-3, 0, 25, verbose=False, siamese=False, aux=True, alpha = j/10)\n",
    "\n",
    "        te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "        accuracies100.append(te_accuracy)\n",
    "    print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies100).mean(), torch.tensor(accuracies100).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(11):\n",
    "    accuracies101 = []\n",
    "\n",
    "    for i in range(10):\n",
    "        train_loader, test_loader = load_data(N=1000, batch_size=8, seed=i)\n",
    "\n",
    "        model = AuxsiameseMLP()\n",
    "        model.to(device)\n",
    "        loss = train(model, train_loader, 5e-3, 0, 25, verbose=False, siamese=True, aux=True, alpha = j/10)\n",
    "\n",
    "        te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "        accuracies101.append(te_accuracy)\n",
    "    print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies101).mean(), torch.tensor(accuracies101).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(11):\n",
    "    accuracies102 = []\n",
    "\n",
    "    for i in range(10):\n",
    "        train_loader, test_loader = load_data(N=1000, batch_size=32, seed=i)\n",
    "\n",
    "        model = AuxsiameseBaseNet()\n",
    "        model.to(device)\n",
    "        loss = train(model, train_loader, 5e-3, 0, 25, verbose=False, siamese=True, aux=True, alpha = j/10)\n",
    "\n",
    "        te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "        accuracies102.append(te_accuracy)\n",
    "    print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies102).mean(), torch.tensor(accuracies102).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(11):\n",
    "    accuracies103 = []\n",
    "\n",
    "    for i in range(10):\n",
    "        train_loader, test_loader = load_data(N=1000, batch_size=8, seed=i)\n",
    "\n",
    "        model = AuxBaseNet()\n",
    "        model.to(device)\n",
    "        train(model, train_loader, 5e-4, 0, 25, verbose=False, siamese=False, aux=True, alpha = j/10)\n",
    "\n",
    "        te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "        accuracies103.append(te_accuracy)\n",
    "    print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies103).mean(), torch.tensor(accuracies103).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(11):\n",
    "    accuracies104 = []\n",
    "\n",
    "    for i in range(10):\n",
    "        train_loader, test_loader = load_data(N=1000, batch_size=16, seed=i)\n",
    "\n",
    "        model = AuxsiameseResNet(nb_residual_blocks = 4, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "        model.to(device)\n",
    "        train(model, train_loader, 1e-3, 0, 25, verbose=False, siamese=True, aux=True, alpha = j/10)\n",
    "\n",
    "        te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "        accuracies104.append(te_accuracy)\n",
    "    print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies104).mean(), torch.tensor(accuracies104).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(11):\n",
    "    accuracies105 = []\n",
    "\n",
    "    for i in range(10):\n",
    "        train_loader, test_loader = load_data(N=1000, batch_size=64, seed=i)\n",
    "\n",
    "        model = AuxResNet(nb_residual_blocks = 4, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "        model.to(device)\n",
    "        train(model, train_loader, 1e-3, 0, 25, verbose=False, siamese=False, aux=True, alpha = j/10)\n",
    "\n",
    "        te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "        accuracies105.append(te_accuracy)\n",
    "    print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies105).mean(), torch.tensor(accuracies105).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.2021, -0.2152,  0.0717],\n",
       "          [-0.0338,  0.1757,  0.1063],\n",
       "          [-0.2043, -0.0632, -0.1443]],\n",
       "\n",
       "         [[ 0.1711, -0.2197, -0.0794],\n",
       "          [-0.0367, -0.1827,  0.1783],\n",
       "          [-0.0457, -0.1880, -0.1476]]],\n",
       "\n",
       "\n",
       "        [[[-0.1065,  0.2347,  0.0664],\n",
       "          [ 0.0989,  0.2181, -0.2000],\n",
       "          [-0.2090,  0.1658, -0.0477]],\n",
       "\n",
       "         [[ 0.1767, -0.0043,  0.0595],\n",
       "          [ 0.0525,  0.2171,  0.1257],\n",
       "          [ 0.0687, -0.2002,  0.1470]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0411, -0.1230,  0.2138],\n",
       "          [ 0.0970,  0.0338,  0.1252],\n",
       "          [ 0.0936, -0.1127, -0.1848]],\n",
       "\n",
       "         [[ 0.1284, -0.2110,  0.1211],\n",
       "          [-0.1395,  0.1456, -0.2234],\n",
       "          [-0.1436, -0.2100,  0.0385]]]], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> import torch\n",
    ">>> from torch import nn\n",
    ">>> layer = nn.Conv2d(2, 3, kernel_size=3, stride=1, padding=0)\n",
    ">>> layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
