{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import data_loader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import dlc_practical_prologue as prologue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def load_data(N=1000, batch_size=50, seed=42):\n",
    "    # Load data\n",
    "    train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(N)\n",
    "    train_target = torch.nn.functional.one_hot(train_target)\n",
    "    test_target = torch.nn.functional.one_hot(test_target)\n",
    "    \n",
    "    train_input = train_input.to(device)\n",
    "    train_target = train_target.to(device)\n",
    "    train_classes = train_classes.to(device)\n",
    "    test_input = test_input.to(device)\n",
    "    test_target = test_target.to(device)\n",
    "    test_classes = test_classes.to(device)\n",
    "    # Normalize data\n",
    "    mean, std = train_input.mean(), train_input.std()\n",
    "    train_input.sub_(mean).div_(std)\n",
    "    test_input.sub_(mean).div_(std)\n",
    "    \n",
    "    # Generate dataset\n",
    "    train_data = TensorDataset(train_input, train_target, train_classes)\n",
    "    test_data = TensorDataset(test_input, test_target, test_classes)\n",
    "    \n",
    "    # For reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Generate data loader\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper.py\n",
    "\n",
    "# Count the number of parameters\n",
    "def count_param(model):\n",
    "    return sum([torch.numel(param) for param in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = load_data(N=1000, batch_size=50, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, data_loader):\n",
    "\n",
    "    nb_data_errors = 0\n",
    "\n",
    "    for data_input, data_target, data_classes in data_loader:\n",
    "        output = model(data_input)\n",
    "        nb_error = torch.sum(torch.argmax(output, dim=1, keepdim=True) != torch.argmax(data_target, dim=1, keepdim=True))\n",
    "        nb_data_errors += nb_error\n",
    "        \n",
    "    return nb_data_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors_siamese(model, data_loader):\n",
    "\n",
    "    nb_data_errors = 0\n",
    "    for data_input, data_target, data_classes in data_loader:\n",
    "        data_1, data_2 = data_input.unbind(1)               \n",
    "        output = model(data_1.unsqueeze(1), data_2.unsqueeze(1))\n",
    "        nb_error = torch.sum(torch.argmax(output, dim=1, keepdim=True) != torch.argmax(data_target, dim=1, keepdim=True))\n",
    "        nb_data_errors += nb_error\n",
    "        \n",
    "    return nb_data_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors_auxsiamese(model, data_loader):\n",
    "\n",
    "    nb_data_errors = 0\n",
    "    for data_input, data_target, data_classes in data_loader:\n",
    "        data_1, data_2 = data_input.unbind(1)               \n",
    "        output, aux1, aux2 = model(data_1.unsqueeze(1), data_2.unsqueeze(1))\n",
    "        nb_error = torch.sum(torch.argmax(output, dim=1, keepdim=True) != torch.argmax(data_target, dim=1, keepdim=True))\n",
    "        nb_data_errors += nb_error\n",
    "        \n",
    "    return nb_data_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(392, 160)\n",
    "        self.fc2 = nn.Linear(160, 64) \n",
    "        self.fc3 = nn.Linear(64, 2) \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,392) # flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AuxMLP, self).__init__()\n",
    "        \n",
    "        self.fc11 = nn.Linear(196, 160)\n",
    "        self.fc12 = nn.Linear(196, 160)\n",
    "        self.fc21 = nn.Linear(160, 10)\n",
    "        self.fc22 = nn.Linear(160, 10)\n",
    "        self.fc3 = nn.Linear(20, 2) \n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = x1.view(-1,196) # flatten\n",
    "        x1 = F.relu(self.fc11(x1))\n",
    "        x1 = self.fc21(x1)\n",
    "        aux1 = F.softmax(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        \n",
    "        x2 = x2.view(-1,196) # flatten\n",
    "        x2 = F.relu(self.fc12(x2))\n",
    "        x2 = self.fc22(x2)\n",
    "        aux2 = F.softmax(x2)\n",
    "        x2 = F.relu(x2)\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "\n",
    "        #x = torch.abs(x1 - x2)\n",
    "        #x = F.relu(self.fc1(x.flatten(start_dim=1)))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        \n",
    "        return x, aux1, aux2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseMLP, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(196, 160)\n",
    "        self.fc2 = nn.Linear(160, 10)\n",
    "        self.fc3 = nn.Linear(20, 2) \n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = x1.view(-1,196) # flatten\n",
    "        x1 = F.relu(self.fc1(x1))\n",
    "        x1 = self.fc2(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        \n",
    "        x2 = x2.view(-1,196) # flatten\n",
    "        x2 = F.relu(self.fc1(x2))\n",
    "        x2 = self.fc2(x2)\n",
    "        x2 = F.relu(x2)\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "\n",
    "        #x = torch.abs(x1 - x2)\n",
    "        #x = F.relu(self.fc1(x.flatten(start_dim=1)))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxsiameseMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AuxsiameseMLP, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(196, 160)\n",
    "        self.fc2 = nn.Linear(160, 10)\n",
    "        self.fc3 = nn.Linear(20, 2) \n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = x1.view(-1,196) # flatten\n",
    "        x1 = F.relu(self.fc1(x1))\n",
    "        x1 = self.fc2(x1)\n",
    "        aux1 = F.softmax(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        \n",
    "        x2 = x2.view(-1,196) # flatten\n",
    "        x2 = F.relu(self.fc1(x2))\n",
    "        x2 = self.fc2(x2)\n",
    "        aux2 = F.softmax(x2)\n",
    "        x2 = F.relu(x2)\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "\n",
    "        #x = torch.abs(x1 - x2)\n",
    "        #x = F.relu(self.fc1(x.flatten(start_dim=1)))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        \n",
    "        return x, aux1, aux2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 32, kernel_size=3)    # size [nb, 32, 12, 12]\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)   # size [nb, 64, 4, 4]\n",
    "        self.fc1 = nn.Linear(256, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "        self.fc3 = nn.Linear(10, 2)\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2)) # size [nb, 32, 6, 6]      \n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2)) # size [nb, 64, 2, 2]\n",
    "        x = x.view(-1, 256) # size [nb, 256]\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseBaseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseBaseNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)    # size [nb, 32, 10, 10]\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)   # size [nb, 64, 4, 4]\n",
    "        self.fc1 = nn.Linear(256, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "        self.fc3 = nn.Linear(20, 2)\n",
    "        \n",
    "    def convs(self, x):        \n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2)) # size [nb, 32, 5, 5]      \n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2)) # size [nb, 64, 2, 2]\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.convs(x1)\n",
    "        x1 = x1.view(-1, 256)\n",
    "        x1 = F.relu((self.fc1(x1)))\n",
    "        x1 = F.relu(self.fc2(x1))\n",
    "        \n",
    "        x2 = self.convs(x2)\n",
    "        x2 = x2.view(-1, 256)\n",
    "        x2 = F.relu(self.fc1(x2))\n",
    "        x2 = F.relu(self.fc2(x2))\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        #x = torch.abs(x1 - x2)\n",
    "        #x = F.relu(self.fc1(x.flatten(start_dim=1)))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        \n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxsiameseBaseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AuxsiameseBaseNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)    # size [nb, 32, 10, 10]\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)   # size [nb, 64, 4, 4]\n",
    "        self.fc1 = nn.Linear(256, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "        self.fc3 = nn.Linear(20, 2)\n",
    "        \n",
    "    def convs(self, x):        \n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2)) # size [nb, 32, 5, 5]      \n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2)) # size [nb, 64, 2, 2]\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.convs(x1)\n",
    "        x1 = x1.view(-1, 256)\n",
    "        x1 = F.relu((self.fc1(x1)))\n",
    "        x1 = self.fc2(x1)\n",
    "        aux1 = F.softmax(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        \n",
    "        x2 = self.convs(x2)\n",
    "        x2 = x2.view(-1, 256)\n",
    "        x2 = F.relu(self.fc1(x2))\n",
    "        x2 = self.fc2(x2)\n",
    "        aux2 = F.softmax(x2)\n",
    "        x2 = F.relu(x2)\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        #x = torch.abs(x1 - x2)\n",
    "        #x = F.relu(self.fc1(x.flatten(start_dim=1)))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        \n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        \n",
    "        return x, aux1, aux2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxBaseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AuxBaseNet, self).__init__()\n",
    "        self.conv11 = nn.Conv2d(1, 32, kernel_size=3)    # size [nb, 32, 10, 10]\n",
    "        self.conv21 = nn.Conv2d(32, 64, kernel_size=3)   # size [nb, 64, 4, 4]\n",
    "        self.fc11 = nn.Linear(256, 200)\n",
    "        self.fc21 = nn.Linear(200, 10)\n",
    "        self.conv12 = nn.Conv2d(1, 32, kernel_size=5)    # size [nb, 32, 10, 10]\n",
    "        self.conv22 = nn.Conv2d(32, 64, kernel_size=2)   # size [nb, 64, 4, 4]\n",
    "        self.fc12 = nn.Linear(256, 200)\n",
    "        self.fc22 = nn.Linear(200, 10)\n",
    "        self.fc3 = nn.Linear(20, 2)\n",
    "        \n",
    "    def convs(self, x):        \n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2)) # size [nb, 32, 5, 5]      \n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2)) # size [nb, 64, 2, 2]\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = F.relu(F.max_pool2d(self.conv11(x1), kernel_size=2)) # size [nb, 32, 5, 5]  \n",
    "        x1 = F.relu(F.max_pool2d(self.conv21(x1), kernel_size=2)) # size [nb, 64, 2, 2]\n",
    "        x1 = x1.view(-1, 256)\n",
    "        x1 = F.relu((self.fc11(x1)))\n",
    "        x1 = self.fc21(x1)\n",
    "        aux1 = F.softmax(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        \n",
    "        x2 = F.relu(F.max_pool2d(self.conv12(x2), kernel_size=2)) # size [nb, 32, 5, 5]  \n",
    "        x2 = F.relu(F.max_pool2d(self.conv22(x2), kernel_size=2)) # size [nb, 64, 2, 2]\n",
    "        x2 = x2.view(-1, 256)\n",
    "        x2 = F.relu((self.fc12(x2)))\n",
    "        x2 = self.fc22(x2)\n",
    "        aux2 = F.softmax(x2)\n",
    "        x2 = F.relu(x2)\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        #x = torch.abs(x1 - x2)\n",
    "        #x = F.relu(self.fc1(x.flatten(start_dim=1)))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        \n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        \n",
    "        return x, aux1, aux2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNetBlock with skip-connection and batch normalization\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, nb_channels, kernel_size, dropout = 0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(nb_channels, nb_channels,\n",
    "                               kernel_size = kernel_size,\n",
    "                               padding = (kernel_size - 1) // 2)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(nb_channels)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(nb_channels, nb_channels,\n",
    "                               kernel_size = kernel_size,\n",
    "                               padding = (kernel_size - 1) // 2)\n",
    "\n",
    "        self.bn2 = nn.BatchNorm2d(nb_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        y = self.bn1(y)\n",
    "        y = self.dropout(y)\n",
    "        y = F.relu(y)\n",
    "        y = self.conv2(y)\n",
    "        y = self.bn2(y)\n",
    "        y = self.dropout(y)\n",
    "        y = y + x\n",
    "        y = F.relu(y)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, nb_residual_blocks, input_channels, nb_channels,\n",
    "                 kernel_size = 3, nb_classes = 10, dropout = 0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(input_channels, nb_channels,\n",
    "                              kernel_size = kernel_size,\n",
    "                              padding = (kernel_size - 1) // 2)\n",
    "        self.bn = nn.BatchNorm2d(nb_channels)\n",
    "\n",
    "        self.resnet_blocks = nn.Sequential(\n",
    "            *(ResNetBlock(nb_channels, kernel_size, dropout)\n",
    "              for _ in range(nb_residual_blocks))\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(288, nb_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn(self.conv(x)))\n",
    "        #x = F.relu(self.dropout(self.bn(self.conv(x))))\n",
    "        x = self.resnet_blocks(x)\n",
    "        x = F.avg_pool2d(x, 4).view(x.size(0), -1)\n",
    "        x = torch.sigmoid(self.fc(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, nb_residual_blocks, input_channels, nb_channels,\n",
    "                 kernel_size = 3, nb_classes = 10, dropout = 0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(input_channels, nb_channels,\n",
    "                              kernel_size = kernel_size,\n",
    "                              padding = (kernel_size - 1) // 2)\n",
    "        self.bn = nn.BatchNorm2d(nb_channels)\n",
    "\n",
    "        self.resnet_blocks = nn.Sequential(\n",
    "            *(ResNetBlock(nb_channels, kernel_size, dropout)\n",
    "              for _ in range(nb_residual_blocks))\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(20, nb_classes)\n",
    "        self.fc1 = nn.Linear(288, 10)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = F.relu(self.bn(self.conv(x1)))\n",
    "        #x = F.relu(self.dropout(self.bn(self.conv(x))))\n",
    "        x1 = self.resnet_blocks(x1)\n",
    "        x1 = F.avg_pool2d(x1, 4).view(x1.size(0), -1)\n",
    "        x1 = F.relu(self.fc1(x1))\n",
    "        \n",
    "        x2 = F.relu(self.bn(self.conv(x2)))\n",
    "        #x = F.relu(self.dropout(self.bn(self.conv(x))))\n",
    "        x2 = self.resnet_blocks(x2)\n",
    "        x2 = F.avg_pool2d(x2, 4).view(x2.size(0), -1)\n",
    "        x2 = F.relu(self.fc1(x2))\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        #aux1 = F.softmax(self.fc1(x1)\n",
    "        #aux2 = F.softmax(self.fc1(x2)\n",
    "        #x = torch.abs(x1 - x2)\n",
    "        #x = F.relu(self.fc1(x.flatten(start_dim=1)))\n",
    "        #x = F.relu(self.fc2(x))      \n",
    "        x = torch.sigmoid(self.fc(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, nb_residual_blocks, input_channels, nb_channels,\n",
    "                 kernel_size = 3, nb_classes = 10, dropout = 0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(input_channels, nb_channels,\n",
    "                              kernel_size = kernel_size,\n",
    "                              padding = (kernel_size - 1) // 2)\n",
    "        self.conv2 = nn.Conv2d(input_channels, nb_channels,\n",
    "                              kernel_size = kernel_size,\n",
    "                              padding = (kernel_size - 1) // 2)\n",
    "        self.bn1 = nn.BatchNorm2d(nb_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(nb_channels)\n",
    "\n",
    "        self.resnet_blocks1 = nn.Sequential(\n",
    "            *(ResNetBlock(nb_channels, kernel_size, dropout)\n",
    "              for _ in range(nb_residual_blocks))\n",
    "        )\n",
    "        self.resnet_blocks2 = nn.Sequential(\n",
    "            *(ResNetBlock(nb_channels, kernel_size, dropout)\n",
    "              for _ in range(nb_residual_blocks))\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(20, nb_classes)\n",
    "        self.fc1 = nn.Linear(288, 10)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = F.relu(self.bn1(self.conv1(x1)))\n",
    "        #x = F.relu(self.dropout(self.bn(self.conv(x))))\n",
    "        x1 = self.resnet_blocks1(x1)\n",
    "        x1 = F.avg_pool2d(x1, 4).view(x1.size(0), -1)\n",
    "        x1 = self.fc1(x1)\n",
    "        \n",
    "        x2 = F.relu(self.bn2(self.conv2(x2)))\n",
    "        #x = F.relu(self.dropout(self.bn(self.conv(x))))\n",
    "        x2 = self.resnet_blocks2(x2)\n",
    "        x2 = F.avg_pool2d(x2, 4).view(x2.size(0), -1)\n",
    "        x2 = self.fc1(x2)\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        aux1 = F.softmax(x1)\n",
    "        aux2 = F.softmax(x2)\n",
    "        x1 = F.relu(x1)\n",
    "        x2 = F.relu(x2)\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        #x = torch.abs(x1 - x2)\n",
    "        #x = F.relu(self.fc1(x.flatten(start_dim=1)))\n",
    "        #x = F.relu(self.fc2(x))      \n",
    "        x = torch.sigmoid(self.fc(x))    \n",
    "        \n",
    "        return x, aux1, aux2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxsiameseResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, nb_residual_blocks, input_channels, nb_channels,\n",
    "                 kernel_size = 3, nb_classes = 10, dropout = 0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(input_channels, nb_channels,\n",
    "                              kernel_size = kernel_size,\n",
    "                              padding = (kernel_size - 1) // 2)\n",
    "        self.bn = nn.BatchNorm2d(nb_channels)\n",
    "\n",
    "        self.resnet_blocks = nn.Sequential(\n",
    "            *(ResNetBlock(nb_channels, kernel_size, dropout)\n",
    "              for _ in range(nb_residual_blocks))\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(20, nb_classes)\n",
    "        self.fc1 = nn.Linear(288, 10)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = F.relu(self.bn(self.conv(x1)))\n",
    "        #x = F.relu(self.dropout(self.bn(self.conv(x))))\n",
    "        x1 = self.resnet_blocks(x1)\n",
    "        x1 = F.avg_pool2d(x1, 4).view(x1.size(0), -1)\n",
    "        x1 = self.fc1(x1)\n",
    "        \n",
    "        x2 = F.relu(self.bn(self.conv(x2)))\n",
    "        #x = F.relu(self.dropout(self.bn(self.conv(x))))\n",
    "        x2 = self.resnet_blocks(x2)\n",
    "        x2 = F.avg_pool2d(x2, 4).view(x2.size(0), -1)\n",
    "        x2 = self.fc1(x2)\n",
    "      \n",
    "        aux1 = F.softmax(x1)\n",
    "        aux2 = F.softmax(x2)\n",
    "        x1 = F.relu(x1)\n",
    "        x2 = F.relu(x2)\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        #x = torch.abs(x1 - x2)\n",
    "        #x = F.relu(self.fc1(x.flatten(start_dim=1)))\n",
    "        #x = F.relu(self.fc2(x))      \n",
    "        x = torch.sigmoid(self.fc(x))\n",
    "        \n",
    "        return x, aux1, aux2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of parameters of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73314\n",
      "33172\n",
      "66302\n",
      "33172\n",
      "72536\n",
      "72268\n",
      "134766\n",
      "72268\n",
      "75746\n",
      "77812\n",
      "152692\n",
      "77812\n"
     ]
    }
   ],
   "source": [
    "model_1 = MLP()\n",
    "model_2 = SiameseMLP()\n",
    "model_3 = AuxMLP()\n",
    "model_4 = AuxsiameseMLP()\n",
    "model_5 = BaseNet()\n",
    "model_6 = SiameseBaseNet()\n",
    "model_7 = AuxBaseNet()\n",
    "model_8 = AuxsiameseBaseNet()\n",
    "model_9 = ResNet(nb_residual_blocks = 4, input_channels = 2, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "model_10 = SiameseResNet(nb_residual_blocks = 4, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "model_11 = AuxResNet(nb_residual_blocks = 4, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "model_12 = AuxsiameseResNet(nb_residual_blocks = 4, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "\n",
    "model = [model_1, model_2, model_3, model_4, model_5, model_6, model_7, model_8, model_9, model_10, model_11, model_12]\n",
    "for i in model:\n",
    "    print(count_param(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, eta, decay, n_epochs=25, verbose=False, siamese=False, aux=False, alpha = 0):\n",
    "\n",
    "    #binary_crit = nn.CrossEntropyLoss()\n",
    "    binary_crit = torch.nn.BCELoss()\n",
    "    aux_crit = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=eta, weight_decay=decay)\n",
    "    #optimizer = torch.optim.LBFGS(model.parameters(), lr=eta)\n",
    "    tr_losses = []\n",
    "    tr_accuracies = []\n",
    "\n",
    "    for e in range(n_epochs):\n",
    "        # Reset training/validation loss\n",
    "        tr_loss = 0\n",
    "\n",
    "        # Training model\n",
    "        model.train()\n",
    "\n",
    "        for train_input, train_target, train_classes in iter(train_loader):\n",
    "            #train_target = torch.nn.functional.one_hot(train_target)\n",
    "            # Forward pass\n",
    "            \n",
    "            if siamese == True:\n",
    "                train_1, train_2 = train_input.unbind(1)\n",
    "                if aux == True:\n",
    "                    output, aux1, aux2 = model(train_1.unsqueeze(1), train_2.unsqueeze(1))\n",
    "                else:\n",
    "                    output = model(train_1.unsqueeze(1), train_2.unsqueeze(1))\n",
    "            elif aux == True:\n",
    "                train_1, train_2 = train_input.unbind(1)\n",
    "                output, aux1, aux2 = model(train_1.unsqueeze(1), train_2.unsqueeze(1))\n",
    "            else:\n",
    "                output = model(train_input)\n",
    "                \n",
    "            # Binary classification loss\n",
    "            binary_loss = binary_crit(output, train_target.float())\n",
    "            total_loss = binary_loss\n",
    "            \n",
    "            # Auxiliary loss\n",
    "            if aux == True:\n",
    "\n",
    "                aux_loss1 = aux_crit(aux1, train_classes[:,0])\n",
    "                aux_loss2 = aux_crit(aux2, train_classes[:,1])\n",
    "                aux_loss = aux_loss1 + aux_loss2\n",
    "                total_loss = binary_loss + aux_loss * alpha\n",
    "        \n",
    "            # Total loss = Binary loss + aux loss * alpha\n",
    "            \n",
    "            tr_loss += total_loss\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Collect accuracy data\n",
    "        # tr_accuracies.append(compute_nb_errors_siamese(model, train_loader)/1000)\n",
    "\n",
    "        # Collect loss data\n",
    "        tr_losses.append(tr_loss)\n",
    "\n",
    "        if verbose:\n",
    "            print('Epoch %d/%d, Binary loss: %.3f' %\n",
    "                  (e+1, n_epochs, tr_loss))\n",
    "    return tr_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.) tensor(0.8220)\n",
      "tensor(1.) tensor(0.8030)\n",
      "tensor(1.) tensor(0.8100)\n",
      "tensor(1.) tensor(0.8220)\n",
      "tensor(1.) tensor(0.8130)\n",
      "tensor(1.) tensor(0.8140)\n",
      "tensor(1.) tensor(0.8180)\n",
      "tensor(1.) tensor(0.8200)\n",
      "tensor(1.) tensor(0.8110)\n",
      "tensor(1.) tensor(0.8110)\n",
      "Mean: 0.814, Std: 0.006\n"
     ]
    }
   ],
   "source": [
    "accuracies8 = []\n",
    "times8 = []\n",
    "losses8 = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=8, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    #model = SiameseBaseNet()\n",
    "    model = MLP()\n",
    "    model.to(device)\n",
    "    losses8[i-10, :] = torch.tensor(train(model, train_loader, 5e-4, 0, 25, verbose=False, siamese=False))\n",
    "    time2 = time.perf_counter()\n",
    "    times8.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies8.append(te_accuracy)\n",
    "\n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies8).mean(), torch.tensor(accuracies8).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 9.767, Std: 2.086\n"
     ]
    }
   ],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times8).mean(), torch.tensor(times8).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0') tensor(0.8400, device='cuda:0')\n",
      "tensor(0.9980, device='cuda:0') tensor(0.8370, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8480, device='cuda:0')\n",
      "tensor(0.9980, device='cuda:0') tensor(0.8360, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8350, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8460, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8570, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8640, device='cuda:0')\n",
      "tensor(0.9990, device='cuda:0') tensor(0.8420, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8490, device='cuda:0')\n",
      "Mean: 0.845, Std: 0.009\n"
     ]
    }
   ],
   "source": [
    "accuracies9 = []\n",
    "times9 = []\n",
    "losses9 = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=8, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    \n",
    "    model = SiameseMLP()\n",
    "    model.to(device)\n",
    "\n",
    "    losses9[i-10,:] = torch.tensor(train(model, train_loader, 5e-3, 0, 25, verbose=False, siamese=True))\n",
    "    time2 = time.perf_counter()\n",
    "    times9.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_siamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies9.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies9).mean(), torch.tensor(accuracies9).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 54.148, Std: 0.246\n"
     ]
    }
   ],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times9).mean(), torch.tensor(times9).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9940, device='cuda:0') tensor(0.8390, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8440, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8450, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8730, device='cuda:0')\n",
      "tensor(0.9980, device='cuda:0') tensor(0.8640, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8390, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8570, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8580, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8800, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8720, device='cuda:0')\n",
      "Mean: 0.857, Std: 0.015\n"
     ]
    }
   ],
   "source": [
    "accuracies10 = []\n",
    "times10 = []\n",
    "losses10 = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=16, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "\n",
    "    model = AuxMLP()\n",
    "    model.to(device)\n",
    "    losses10[i-10, :] = torch.tensor(train(model, train_loader, 5e-3, 0, 25, verbose=False, siamese=False, aux=True, alpha = 0.9))\n",
    "    time2 = time.perf_counter()\n",
    "    times10.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies10.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies10).mean(), torch.tensor(accuracies10).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 43.098, Std: 0.224\n"
     ]
    }
   ],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times10).mean(), torch.tensor(times10).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9540, device='cuda:0') tensor(0.8670, device='cuda:0')\n",
      "tensor(0.9800, device='cuda:0') tensor(0.8880, device='cuda:0')\n",
      "tensor(0.9840, device='cuda:0') tensor(0.8680, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8720, device='cuda:0')\n",
      "tensor(0.9730, device='cuda:0') tensor(0.8740, device='cuda:0')\n",
      "tensor(0.9910, device='cuda:0') tensor(0.8770, device='cuda:0')\n",
      "tensor(0.9890, device='cuda:0') tensor(0.8740, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8740, device='cuda:0')\n",
      "tensor(0.9950, device='cuda:0') tensor(0.8950, device='cuda:0')\n",
      "tensor(0.9610, device='cuda:0') tensor(0.8810, device='cuda:0')\n",
      "Mean: 0.877, Std: 0.009\n"
     ]
    }
   ],
   "source": [
    "accuracies11 = []\n",
    "times11 = []\n",
    "losses11 = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=8, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "\n",
    "    model = AuxsiameseMLP()\n",
    "    model.to(device)\n",
    "    losses11[i-10, :] = torch.tensor(train(model, train_loader, 5e-3, 0, 25, verbose=False, siamese=True, aux=True, alpha = 0.7))\n",
    "    time2 = time.perf_counter()\n",
    "    times11.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies11.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies11).mean(), torch.tensor(accuracies11).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 68.603, Std: 0.362\n"
     ]
    }
   ],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times11).mean(), torch.tensor(times11).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0') tensor(0.8380, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8300, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8140, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8320, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8300, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8030, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8350, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8150, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8260, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8260, device='cuda:0')\n",
      "Mean: 0.825, Std: 0.011\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "times = []\n",
    "losses = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=16, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "\n",
    "    model = BaseNet()\n",
    "    model.to(device)\n",
    "    losses[i-10, :] = torch.tensor(train(model, train_loader, 5e-4, 0, 25, verbose=False, siamese=False))\n",
    "    time2 = time.perf_counter()\n",
    "    times.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies.append(te_accuracy)\n",
    "\n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies).mean(), torch.tensor(accuracies).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 38.077, Std: 0.187\n"
     ]
    }
   ],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times).mean(), torch.tensor(times).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0') tensor(0.8530, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8460, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8440, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8640, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8520, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8520, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8580, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8660, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8520, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8440, device='cuda:0')\n",
      "Mean: 0.853, Std: 0.008\n"
     ]
    }
   ],
   "source": [
    "accuracies1 = []\n",
    "times1 = []\n",
    "losses1 = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=16, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    model = SiameseBaseNet()\n",
    "    model.to(device)\n",
    "\n",
    "    losses1[i-10, :] = torch.tensor(train(model, train_loader, 1e-3, 0, 25, verbose=False, siamese=True))\n",
    "    time2 = time.perf_counter()\n",
    "    times1.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_siamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies1.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies1).mean(), torch.tensor(accuracies1).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 50.379, Std: 1.647\n"
     ]
    }
   ],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times1).mean(), torch.tensor(times1).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0') tensor(0.8590, device='cuda:0')\n",
      "tensor(0.9980, device='cuda:0') tensor(0.9040, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8590, device='cuda:0')\n",
      "tensor(0.9920, device='cuda:0') tensor(0.8200, device='cuda:0')\n",
      "tensor(0.9980, device='cuda:0') tensor(0.8560, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8490, device='cuda:0')\n",
      "tensor(0.9990, device='cuda:0') tensor(0.8550, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8840, device='cuda:0')\n",
      "tensor(0.9930, device='cuda:0') tensor(0.9020, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8470, device='cuda:0')\n",
      "Mean: 0.863, Std: 0.026\n"
     ]
    }
   ],
   "source": [
    "accuracies2 = []\n",
    "times2 = []\n",
    "losses2 = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=8, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "\n",
    "    model = AuxBaseNet()\n",
    "    model.to(device)\n",
    "    losses2[i-10, :] = torch.tensor(train(model, train_loader, 5e-4, 0, 25, verbose=False, siamese=False, aux=True, alpha = 1.0))\n",
    "    time2 = time.perf_counter()\n",
    "    times2.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies2.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies2).mean(), torch.tensor(accuracies2).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 145.856, Std: 0.234\n"
     ]
    }
   ],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times2).mean(), torch.tensor(times2).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9990, device='cuda:0') tensor(0.8840, device='cuda:0')\n",
      "tensor(0.9970, device='cuda:0') tensor(0.9210, device='cuda:0')\n",
      "tensor(0.9960, device='cuda:0') tensor(0.8990, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.9140, device='cuda:0')\n",
      "tensor(0.9920, device='cuda:0') tensor(0.9010, device='cuda:0')\n",
      "tensor(0.9980, device='cuda:0') tensor(0.9350, device='cuda:0')\n",
      "tensor(0.9810, device='cuda:0') tensor(0.8880, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.9120, device='cuda:0')\n",
      "tensor(0.9780, device='cuda:0') tensor(0.8600, device='cuda:0')\n",
      "tensor(0.9930, device='cuda:0') tensor(0.9000, device='cuda:0')\n",
      "Mean: 0.901, Std: 0.021\n"
     ]
    }
   ],
   "source": [
    "accuracies3 = []\n",
    "times3 = []\n",
    "losses3 = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=32, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    \n",
    "    model = AuxsiameseBaseNet()\n",
    "    model.to(device)\n",
    "    losses3[i-10, :] = torch.tensor(train(model, train_loader, 5e-3, 0, 25, verbose=False, siamese=True, aux=True, alpha = 0.6))\n",
    "    time2 = time.perf_counter()\n",
    "    times3.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies3.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies3).mean(), torch.tensor(accuracies3).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 30.110, Std: 0.261\n"
     ]
    }
   ],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times3).mean(), torch.tensor(times3).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0') tensor(0.8220, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8200, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8280, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8300, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8460, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8520, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8400, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8230, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8510, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8180, device='cuda:0')\n",
      "Mean: 0.833, Std: 0.013\n"
     ]
    }
   ],
   "source": [
    "accuracies4 = []\n",
    "times4 = []\n",
    "losses4 = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=32, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "\n",
    "    model = ResNet(nb_residual_blocks = 4, input_channels = 2, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "    model.to(device)\n",
    "    losses4[i-10, :] = torch.tensor(train(model, train_loader, 1e-3, 0, 25, verbose=False, siamese=False))\n",
    "    time2 = time.perf_counter()\n",
    "    times4.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies4.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies4).mean(), torch.tensor(accuracies4).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 65.422, Std: 0.141\n"
     ]
    }
   ],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times4).mean(), torch.tensor(times4).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9970, device='cuda:0') tensor(0.8830, device='cuda:0')\n",
      "tensor(0.9850, device='cuda:0') tensor(0.8550, device='cuda:0')\n",
      "tensor(0.9550, device='cuda:0') tensor(0.8460, device='cuda:0')\n",
      "tensor(0.9910, device='cuda:0') tensor(0.8790, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8620, device='cuda:0')\n",
      "tensor(0.9450, device='cuda:0') tensor(0.8460, device='cuda:0')\n",
      "tensor(0.9840, device='cuda:0') tensor(0.8610, device='cuda:0')\n",
      "tensor(0.9980, device='cuda:0') tensor(0.8610, device='cuda:0')\n",
      "tensor(0.9860, device='cuda:0') tensor(0.8690, device='cuda:0')\n",
      "tensor(0.9970, device='cuda:0') tensor(0.8630, device='cuda:0')\n",
      "Mean: 0.863, Std: 0.012\n"
     ]
    }
   ],
   "source": [
    "accuracies5 = []\n",
    "times5 = []\n",
    "losses5 = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=32, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    \n",
    "    model = SiameseResNet(nb_residual_blocks = 4, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "    model.to(device)\n",
    "    losses5[i-10, :] = torch.tensor(train(model, train_loader, 5e-3, 0, 25, verbose=False, siamese=True))\n",
    "    time2 = time.perf_counter()\n",
    "    times5.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_siamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies5.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies5).mean(), torch.tensor(accuracies5).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 90.997, Std: 0.326\n"
     ]
    }
   ],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times5).mean(), torch.tensor(times5).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0') tensor(0.8700, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8690, device='cuda:0')\n",
      "tensor(0.9990, device='cuda:0') tensor(0.8790, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8630, device='cuda:0')\n",
      "tensor(0.9990, device='cuda:0') tensor(0.8870, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8380, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8570, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8690, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8680, device='cuda:0')\n",
      "tensor(0.9990, device='cuda:0') tensor(0.8690, device='cuda:0')\n",
      "Mean: 0.867, Std: 0.013\n"
     ]
    }
   ],
   "source": [
    "accuracies6 = []\n",
    "times6 = []\n",
    "losses6 = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=16, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    \n",
    "    model = AuxResNet(nb_residual_blocks = 10, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "    model.to(device)\n",
    "    losses6[i-10, :] = torch.tensor(train(model, train_loader, 1e-3, 0, 25, verbose=False, siamese=False, aux=True, alpha = 0.6))\n",
    "    time2 = time.perf_counter()\n",
    "    times6.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies6.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies6).mean(), torch.tensor(accuracies6).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 559.451, Std: 0.711\n"
     ]
    }
   ],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times6).mean(), torch.tensor(times6).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9990, device='cuda:0') tensor(0.8970, device='cuda:0')\n",
      "tensor(0.9970, device='cuda:0') tensor(0.8760, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8990, device='cuda:0')\n",
      "tensor(0.9990, device='cuda:0') tensor(0.8930, device='cuda:0')\n",
      "tensor(0.9920, device='cuda:0') tensor(0.8880, device='cuda:0')\n",
      "tensor(0.9990, device='cuda:0') tensor(0.8770, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.9000, device='cuda:0')\n",
      "tensor(0.9920, device='cuda:0') tensor(0.8790, device='cuda:0')\n",
      "tensor(0.9740, device='cuda:0') tensor(0.8770, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8940, device='cuda:0')\n",
      "Mean: 0.888, Std: 0.010\n"
     ]
    }
   ],
   "source": [
    "accuracies7 = []\n",
    "times7 = []\n",
    "losses7 = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=32, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    #model = SiameseBaseNet()\n",
    "    #model = BaseNet()\n",
    "    model = AuxsiameseResNet(nb_residual_blocks = 4, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "    model.to(device)\n",
    "    losses7[i-10, :] = torch.tensor(train(model, train_loader, 5e-3, 0, 25, verbose=False, siamese=True, aux=True, alpha = 0.6))\n",
    "    time2 = time.perf_counter()\n",
    "    times7.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies7.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies7).mean(), torch.tensor(accuracies7).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 98.518, Std: 0.228\n"
     ]
    }
   ],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times7).mean(), torch.tensor(times7).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize learning rate and batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8014, 0.8035, 0.8015, 0.8056, 0.8019],\n",
      "        [0.8044, 0.8070, 0.8068, 0.8042, 0.8011],\n",
      "        [0.8072, 0.8071, 0.8053, 0.8004, 0.7953],\n",
      "        [0.7977, 0.7913, 0.7888, 0.7791, 0.7683]])\n"
     ]
    }
   ],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies1 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = MLP()\n",
    "            model.to(device)\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=False)\n",
    "            te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies1[j,k] =  torch.cuda.FloatTensor(accurate).mean()\n",
    "print(test_accuracies1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8475, 0.8444, 0.8468, 0.8429, 0.8374],\n",
      "        [0.8423, 0.8384, 0.8367, 0.8363, 0.8296],\n",
      "        [0.8406, 0.8350, 0.8342, 0.8300, 0.8140],\n",
      "        [0.8195, 0.8094, 0.7905, 0.7710, 0.7423]])\n"
     ]
    }
   ],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies2 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = SiameseMLP()\n",
    "            model.to(device)\n",
    "            #model = BaseNet()\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=True)\n",
    "            te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies2[j,k] =  torch.cuda.FloatTensor(accurate).mean()\n",
    "print(test_accuracies2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8184, 0.8225, 0.8223, 0.8176, 0.8120],\n",
      "        [0.8139, 0.8119, 0.8114, 0.8063, 0.8106],\n",
      "        [0.8130, 0.8087, 0.8095, 0.8112, 0.8017],\n",
      "        [0.8060, 0.7966, 0.7876, 0.7730, 0.7571]])\n"
     ]
    }
   ],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies3 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = AuxMLP()\n",
    "            model.to(device)\n",
    "            #model = BaseNet()\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=False, aux=True, alpha = 0.0)\n",
    "            te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies3[j,k] =  torch.cuda.FloatTensor(accurate).mean()\n",
    "print(test_accuracies3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7246, 0.7686, 0.7808, 0.8034, 0.8205],\n",
      "        [0.8241, 0.8299, 0.8253, 0.8273, 0.8256],\n",
      "        [0.8287, 0.8311, 0.8293, 0.8291, 0.8189],\n",
      "        [0.8303, 0.8192, 0.7826, 0.7837, 0.7460]])\n"
     ]
    }
   ],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies5 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = BaseNet()\n",
    "            model.to(device)\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=False)\n",
    "            te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies5[j,k] =  torch.cuda.FloatTensor(accurate).mean()\n",
    "print(test_accuracies5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8408, 0.8178, 0.8411, 0.8424, 0.8413],\n",
      "        [0.8546, 0.8564, 0.8523, 0.8487, 0.8513],\n",
      "        [0.8520, 0.8536, 0.8530, 0.8507, 0.8529],\n",
      "        [0.8486, 0.8511, 0.8439, 0.8258, 0.7881]])\n"
     ]
    }
   ],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies6 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = SiameseBaseNet()\n",
    "            model.to(device)\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=True)\n",
    "            te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies6[j,k] =  torch.cuda.FloatTensor(accurate).mean()\n",
    "print(test_accuracies6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7255, 0.7897, 0.8042, 0.7796, 0.8148],\n",
      "        [0.8354, 0.8351, 0.8372, 0.8389, 0.8329],\n",
      "        [0.8410, 0.8391, 0.8380, 0.8347, 0.8325],\n",
      "        [0.8324, 0.8310, 0.8204, 0.8055, 0.7755]])\n"
     ]
    }
   ],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies7 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = AuxBaseNet()\n",
    "            model.to(device)\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=False, aux=True, alpha = 0.0)\n",
    "            te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies7[j,k] =  torch.cuda.FloatTensor(accurate).mean()\n",
    "print(test_accuracies7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8262, 0.8285, 0.8360, 0.8349, 0.8226],\n",
      "        [0.8398, 0.8384, 0.8403, 0.8343, 0.8302],\n",
      "        [0.8354, 0.8340, 0.8184, 0.8279, 0.8218],\n",
      "        [0.8152, 0.8161, 0.8157, 0.8150, 0.8147]])\n"
     ]
    }
   ],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies9 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = ResNet(nb_residual_blocks = 4, input_channels = 2, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "            model.to(device)\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=False)\n",
    "            te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies9[j,k] =  torch.cuda.FloatTensor(accurate).mean()\n",
    "print(test_accuracies9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8146, 0.8483, 0.8466, 0.8430, 0.8749],\n",
      "        [0.8823, 0.8833, 0.8772, 0.8711, 0.8649],\n",
      "        [0.8796, 0.8750, 0.8703, 0.8583, 0.8506],\n",
      "        [0.8509, 0.8482, 0.8454, 0.8483, 0.8466]])\n"
     ]
    }
   ],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies10 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = SiameseResNet(nb_residual_blocks = 4, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "            model.to(device)\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            loss = train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=True)\n",
    "            te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies10[j,k] =  torch.FloatTensor(accurate).mean()\n",
    "print(test_accuracies10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:44: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8023, 0.8195, 0.8178, 0.8232, 0.8373],\n",
      "        [0.8487, 0.8480, 0.8454, 0.8500, 0.8405],\n",
      "        [0.8492, 0.8465, 0.8447, 0.8417, 0.8321],\n",
      "        [0.8296, 0.8233, 0.8251, 0.8231, 0.8329]])\n"
     ]
    }
   ],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies11 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = AuxResNet(nb_residual_blocks = 4, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "            model.to(device)\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            loss = train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=False, aux=True, alpha = 0.0)\n",
    "            te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies11[j,k] =  torch.FloatTensor(accurate).mean()\n",
    "print(test_accuracies11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize alpha for auxiliary loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.823, Std: 0.021\n",
      "Mean: 0.836, Std: 0.012\n",
      "Mean: 0.842, Std: 0.007\n",
      "Mean: 0.847, Std: 0.012\n",
      "Mean: 0.849, Std: 0.016\n",
      "Mean: 0.851, Std: 0.015\n",
      "Mean: 0.848, Std: 0.015\n",
      "Mean: 0.856, Std: 0.014\n",
      "Mean: 0.854, Std: 0.008\n",
      "Mean: 0.864, Std: 0.013\n",
      "Mean: 0.857, Std: 0.012\n"
     ]
    }
   ],
   "source": [
    "for j in range(11):\n",
    "    accuracies100 = []\n",
    "\n",
    "    for i in range(10):\n",
    "        train_loader, test_loader = load_data(N=1000, batch_size=16, seed=i)\n",
    "        \n",
    "        model = AuxMLP()\n",
    "        model.to(device)\n",
    "        loss = train(model, train_loader, 5e-3, 0, 25, verbose=False, siamese=False, aux=True, alpha = j/10)\n",
    "\n",
    "        te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "        accuracies100.append(te_accuracy)\n",
    "    print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies100).mean(), torch.tensor(accuracies100).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.849, Std: 0.017\n",
      "Mean: 0.864, Std: 0.015\n",
      "Mean: 0.860, Std: 0.015\n",
      "Mean: 0.873, Std: 0.011\n",
      "Mean: 0.866, Std: 0.018\n",
      "Mean: 0.870, Std: 0.015\n",
      "Mean: 0.872, Std: 0.018\n",
      "Mean: 0.872, Std: 0.020\n",
      "Mean: 0.884, Std: 0.017\n",
      "Mean: 0.881, Std: 0.015\n",
      "Mean: 0.881, Std: 0.022\n"
     ]
    }
   ],
   "source": [
    "for j in range(11):\n",
    "    accuracies101 = []\n",
    "\n",
    "    for i in range(10):\n",
    "        train_loader, test_loader = load_data(N=1000, batch_size=8, seed=i)\n",
    "\n",
    "        model = AuxsiameseMLP()\n",
    "        model.to(device)\n",
    "        loss = train(model, train_loader, 5e-3, 0, 25, verbose=False, siamese=True, aux=True, alpha = j/10)\n",
    "\n",
    "        te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "        accuracies101.append(te_accuracy)\n",
    "    print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies101).mean(), torch.tensor(accuracies101).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.812, Std: 0.089\n",
      "Mean: 0.855, Std: 0.008\n",
      "Mean: 0.871, Std: 0.020\n",
      "Mean: 0.887, Std: 0.011\n",
      "Mean: 0.897, Std: 0.016\n",
      "Mean: 0.904, Std: 0.013\n",
      "Mean: 0.902, Std: 0.016\n",
      "Mean: 0.909, Std: 0.015\n",
      "Mean: 0.908, Std: 0.009\n",
      "Mean: 0.913, Std: 0.016\n",
      "Mean: 0.912, Std: 0.017\n"
     ]
    }
   ],
   "source": [
    "for j in range(11):\n",
    "    accuracies102 = []\n",
    "\n",
    "    for i in range(10):\n",
    "        train_loader, test_loader = load_data(N=1000, batch_size=32, seed=i)\n",
    "\n",
    "        model = AuxsiameseBaseNet()\n",
    "        model.to(device)\n",
    "        loss = train(model, train_loader, 5e-3, 0, 25, verbose=False, siamese=True, aux=True, alpha = j/10)\n",
    "\n",
    "        te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "        accuracies102.append(te_accuracy)\n",
    "    print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies102).mean(), torch.tensor(accuracies102).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.840, Std: 0.014\n",
      "Mean: 0.832, Std: 0.016\n",
      "Mean: 0.838, Std: 0.011\n",
      "Mean: 0.843, Std: 0.015\n",
      "Mean: 0.850, Std: 0.013\n",
      "Mean: 0.851, Std: 0.013\n",
      "Mean: 0.855, Std: 0.012\n",
      "Mean: 0.857, Std: 0.013\n",
      "Mean: 0.858, Std: 0.013\n",
      "Mean: 0.859, Std: 0.013\n",
      "Mean: 0.868, Std: 0.012\n"
     ]
    }
   ],
   "source": [
    "for j in range(11):\n",
    "    accuracies103 = []\n",
    "\n",
    "    for i in range(10):\n",
    "        train_loader, test_loader = load_data(N=1000, batch_size=8, seed=i)\n",
    "\n",
    "        model = AuxBaseNet()\n",
    "        model.to(device)\n",
    "        train(model, train_loader, 5e-4, 0, 25, verbose=False, siamese=False, aux=True, alpha = j/10)\n",
    "\n",
    "        te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "        accuracies103.append(te_accuracy)\n",
    "    print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies103).mean(), torch.tensor(accuracies103).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.881, Std: 0.011\n",
      "Mean: 0.889, Std: 0.016\n",
      "Mean: 0.894, Std: 0.015\n",
      "Mean: 0.893, Std: 0.011\n",
      "Mean: 0.891, Std: 0.012\n"
     ]
    }
   ],
   "source": [
    "for j in range(11):\n",
    "    accuracies104 = []\n",
    "\n",
    "    for i in range(10):\n",
    "        train_loader, test_loader = load_data(N=1000, batch_size=16, seed=i)\n",
    "\n",
    "        model = AuxsiameseResNet(nb_residual_blocks = 4, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "        model.to(device)\n",
    "        train(model, train_loader, 1e-3, 0, 25, verbose=False, siamese=True, aux=True, alpha = j/10)\n",
    "\n",
    "        te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "        accuracies104.append(te_accuracy)\n",
    "    print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies104).mean(), torch.tensor(accuracies104).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(11):\n",
    "    accuracies105 = []\n",
    "\n",
    "    for i in range(10):\n",
    "        train_loader, test_loader = load_data(N=1000, batch_size=64, seed=i)\n",
    "\n",
    "        model = AuxResNet(nb_residual_blocks = 4, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "        model.to(device)\n",
    "        train(model, train_loader, 1e-3, 0, 25, verbose=False, siamese=False, aux=True, alpha = j/10)\n",
    "\n",
    "        te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "        accuracies105.append(te_accuracy)\n",
    "    print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies105).mean(), torch.tensor(accuracies105).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
