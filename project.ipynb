{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import dlc_practical_prologue as prologue\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MLP_models import *\n",
    "from CNN_models import *\n",
    "from Resnet_models import *\n",
    "from helpers import *\n",
    "from data_loader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of parameters of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters in model_1 is 73314\n",
      "The number of parameters in model_2 is 33172\n",
      "The number of parameters in model_3 is 66302\n",
      "The number of parameters in model_4 is 33172\n",
      "The number of parameters in model_5 is 72536\n",
      "The number of parameters in model_6 is 72268\n",
      "The number of parameters in model_7 is 144494\n",
      "The number of parameters in model_8 is 72268\n",
      "The number of parameters in model_9 is 75746\n",
      "The number of parameters in model_10 is 77812\n",
      "The number of parameters in model_11 is 152692\n",
      "The number of parameters in model_12 is 77812\n"
     ]
    }
   ],
   "source": [
    "model_1 = MLP()\n",
    "model_2 = SiameseMLP()\n",
    "model_3 = AuxMLP()\n",
    "model_4 = AuxsiameseMLP()\n",
    "model_5 = CNN()\n",
    "model_6 = SiameseCNN()\n",
    "model_7 = AuxCNN()\n",
    "model_8 = AuxsiameseCNN()\n",
    "model_9 = ResNet()\n",
    "model_10 = SiameseResNet()\n",
    "model_11 = AuxResNet()\n",
    "model_12 = AuxsiameseResNet()\n",
    "\n",
    "models = [model_1, model_2, model_3, model_4, model_5, model_6, model_7, model_8, model_9, model_10, model_11, model_12]\n",
    "for i in range(len(models)):\n",
    "    #print('The number of parameters in' count_param(i))\n",
    "    print('The number of parameters in model_%d is %d' %\n",
    "                  (i+1, count_param(models[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, eta, decay, n_epochs=25, verbose=False, siamese=False, aux=False, alpha=0.0):\n",
    "    '''\n",
    "    model: learning model\n",
    "    '''\n",
    "    binary_crit = torch.nn.BCELoss()\n",
    "    aux_crit = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=eta, weight_decay=decay)\n",
    "    tr_losses = []\n",
    "    tr_accuracies = []\n",
    "\n",
    "    for e in range(n_epochs):\n",
    "        # Reset training/validation loss\n",
    "        tr_loss = 0\n",
    "\n",
    "        # Training model\n",
    "        model.train()\n",
    "\n",
    "        for train_input, train_target, train_classes in iter(train_loader):\n",
    "            # Forward pass\n",
    "            \n",
    "            if aux == True:\n",
    "                train_1, train_2 = train_input.unbind(1)\n",
    "                output, aux1, aux2 = model(train_1.unsqueeze(1), train_2.unsqueeze(1))\n",
    "                \n",
    "            elif siamese == True:\n",
    "                train_1, train_2 = train_input.unbind(1)\n",
    "                output = model(train_1.unsqueeze(1), train_2.unsqueeze(1))\n",
    "                \n",
    "            else:\n",
    "                output = model(train_input)\n",
    "                \n",
    "            # Binary classification loss\n",
    "            binary_loss = binary_crit(output, train_target.float())\n",
    "            total_loss = binary_loss\n",
    "            \n",
    "            # Auxiliary loss\n",
    "            if aux == True:\n",
    "\n",
    "                aux_loss1 = aux_crit(aux1, train_classes[:,0])\n",
    "                aux_loss2 = aux_crit(aux2, train_classes[:,1])\n",
    "                aux_loss = aux_loss1 + aux_loss2\n",
    "                \n",
    "                # Total loss = Binary loss + aux loss * alpha\n",
    "                total_loss = binary_loss + aux_loss * alpha\n",
    "            \n",
    "            tr_loss += total_loss\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Collect loss data\n",
    "        tr_losses.append(tr_loss)\n",
    "\n",
    "        if verbose:\n",
    "            print('Epoch %d/%d, Binary loss: %.3f' %\n",
    "                  (e+1, n_epochs, tr_loss))\n",
    "    return tr_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accu(model, train_loader, test_loader, siamese = False, aux = False):\n",
    "    \n",
    "    if aux == True:\n",
    "        tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "        te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "    elif siamese == True:\n",
    "        tr_accuracy = 1 - compute_nb_errors_siamese(model, train_loader)/1000\n",
    "        te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "    else:\n",
    "        tr_accuracy = 1 - compute_nb_errors(model, train_loader)/1000\n",
    "        te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "            \n",
    "    return tr_accuracy, te_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune learning rate and batch size for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_model = [[MLP, False, False],\n",
    "              [SiameseMLP, True, False],\n",
    "              [AuxMLP, False, True],\n",
    "              [AuxsiameseMLP, True, True],\n",
    "              [CNN, False, False],\n",
    "              [SiameseCNN, True, False],\n",
    "              [AuxCNN, False, True],\n",
    "              [AuxsiameseCNN, True, True],\n",
    "              [ResNet, False, False],\n",
    "              [SiameseResNet, True, False],\n",
    "              [AuxResNet, False, True],\n",
    "              [AuxsiameseResNet, True, True]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal parameters for model_1: learning rate 0.00100, batch size: 16\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-62802cc97f94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                 \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_sizes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m                 \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgammas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msiamese\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maux\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m                 \u001b[0mtr_accuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mte_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[0maccurate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mte_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-643deba1e55f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, eta, decay, n_epochs, verbose, siamese, aux, alpha)\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0mtotal_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;31m# Collect loss data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_64\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_64\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight_decay'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m                    \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m                    )\n\u001b[0;32m    121\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_64\\lib\\site-packages\\torch\\optim\\functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m             \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "std = []\n",
    "model_number = 0\n",
    "for models in Train_model:\n",
    "    gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "    batch_sizes = [8, 16, 32, 64, 128]\n",
    "    test_accuracies = torch.empty((len(gammas), len(batch_sizes)))\n",
    "    test_stds = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "    for j in range(len(gammas)):\n",
    "        for k in range(len(batch_sizes)):\n",
    "            accurate = []\n",
    "            for i in range(10):\n",
    "                model = models[0]()\n",
    "                model.to(device)\n",
    "                train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "                train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=models[1], aux=models[2])\n",
    "                tr_accuracy, te_accuracy = accu(model, train_loader, test_loader, models[1], models[2])\n",
    "                accurate.append(te_accuracy)\n",
    "            test_accuracies[j,k] =  torch.FloatTensor(accurate).mean()\n",
    "            test_stds[j,k] =  torch.FloatTensor(accurate).std()\n",
    "    accuracy.append(test_accuracies)\n",
    "    std.append(test_stds)\n",
    "    max_index = test_accuracies.argmax() \n",
    "    model_number += 1\n",
    "    print('The optimal parameters for model_%d: learning rate %.5f, batch size: %d' %(model_number, gammas[max_index//5], batch_sizes[(max_index+1)%5-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.7940, 0.7972, 0.8029, 0.8010, 0.8049],\n",
       "         [0.8049, 0.8090, 0.8051, 0.8017, 0.8021],\n",
       "         [0.8054, 0.8039, 0.8035, 0.8017, 0.7962],\n",
       "         [0.7956, 0.7924, 0.7865, 0.7810, 0.7699]])]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune alpha for models with auxiliary loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_auxmodel = [[AuxMLP, False, True, 0.005, 32],\n",
    "              [AuxsiameseMLP, True, True, 0.005, 32],\n",
    "              [AuxCNN, False, True, 0.001, 32],\n",
    "              [AuxsiameseCNN, True, True, 0.0001, 128],\n",
    "              [AuxResNet, False, True, 0.001, 32],\n",
    "              [AuxsiameseResNet, True, True, 0.001, 32]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'CNN_models.AuxsiameseCNN'>, True, True, 0.0001, 128]\n",
      "Epoch 1/25, Binary loss: 5.569\n",
      "Epoch 2/25, Binary loss: 5.521\n",
      "Epoch 3/25, Binary loss: 5.493\n",
      "Epoch 4/25, Binary loss: 5.470\n",
      "Epoch 5/25, Binary loss: 5.442\n",
      "Epoch 6/25, Binary loss: 5.423\n",
      "Epoch 7/25, Binary loss: 5.399\n",
      "Epoch 8/25, Binary loss: 5.370\n",
      "Epoch 9/25, Binary loss: 5.335\n",
      "Epoch 10/25, Binary loss: 5.296\n",
      "Epoch 11/25, Binary loss: 5.242\n",
      "Epoch 12/25, Binary loss: 5.183\n",
      "Epoch 13/25, Binary loss: 5.119\n",
      "Epoch 14/25, Binary loss: 5.038\n",
      "Epoch 15/25, Binary loss: 4.949\n",
      "Epoch 16/25, Binary loss: 4.880\n",
      "Epoch 17/25, Binary loss: 4.791\n",
      "Epoch 18/25, Binary loss: 4.705\n",
      "Epoch 19/25, Binary loss: 4.616\n",
      "Epoch 20/25, Binary loss: 4.530\n",
      "Epoch 21/25, Binary loss: 4.438\n",
      "Epoch 22/25, Binary loss: 4.361\n",
      "Epoch 23/25, Binary loss: 4.280\n",
      "Epoch 24/25, Binary loss: 4.180\n",
      "Epoch 25/25, Binary loss: 4.120\n",
      "Epoch 1/25, Binary loss: 5.565\n",
      "Epoch 2/25, Binary loss: 5.522\n",
      "Epoch 3/25, Binary loss: 5.476\n",
      "Epoch 4/25, Binary loss: 5.434\n",
      "Epoch 5/25, Binary loss: 5.385\n",
      "Epoch 6/25, Binary loss: 5.344\n",
      "Epoch 7/25, Binary loss: 5.298\n",
      "Epoch 8/25, Binary loss: 5.241\n",
      "Epoch 9/25, Binary loss: 5.174\n",
      "Epoch 10/25, Binary loss: 5.112\n",
      "Epoch 11/25, Binary loss: 5.039\n",
      "Epoch 12/25, Binary loss: 4.958\n",
      "Epoch 13/25, Binary loss: 4.871\n",
      "Epoch 14/25, Binary loss: 4.774\n",
      "Epoch 15/25, Binary loss: 4.685\n",
      "Epoch 16/25, Binary loss: 4.600\n",
      "Epoch 17/25, Binary loss: 4.506\n",
      "Epoch 18/25, Binary loss: 4.409\n",
      "Epoch 19/25, Binary loss: 4.325\n",
      "Epoch 20/25, Binary loss: 4.234\n",
      "Epoch 21/25, Binary loss: 4.143\n",
      "Epoch 22/25, Binary loss: 4.057\n",
      "Epoch 23/25, Binary loss: 3.978\n",
      "Epoch 24/25, Binary loss: 3.906\n",
      "Epoch 25/25, Binary loss: 3.804\n",
      "Epoch 1/25, Binary loss: 5.629\n",
      "Epoch 2/25, Binary loss: 5.528\n",
      "Epoch 3/25, Binary loss: 5.478\n",
      "Epoch 4/25, Binary loss: 5.453\n",
      "Epoch 5/25, Binary loss: 5.435\n",
      "Epoch 6/25, Binary loss: 5.418\n",
      "Epoch 7/25, Binary loss: 5.397\n",
      "Epoch 8/25, Binary loss: 5.372\n",
      "Epoch 9/25, Binary loss: 5.350\n",
      "Epoch 10/25, Binary loss: 5.316\n",
      "Epoch 11/25, Binary loss: 5.273\n",
      "Epoch 12/25, Binary loss: 5.223\n",
      "Epoch 13/25, Binary loss: 5.168\n",
      "Epoch 14/25, Binary loss: 5.102\n",
      "Epoch 15/25, Binary loss: 5.027\n",
      "Epoch 16/25, Binary loss: 4.940\n",
      "Epoch 17/25, Binary loss: 4.847\n",
      "Epoch 18/25, Binary loss: 4.734\n",
      "Epoch 19/25, Binary loss: 4.626\n",
      "Epoch 20/25, Binary loss: 4.511\n",
      "Epoch 21/25, Binary loss: 4.395\n",
      "Epoch 22/25, Binary loss: 4.285\n",
      "Epoch 23/25, Binary loss: 4.173\n",
      "Epoch 24/25, Binary loss: 4.088\n",
      "Epoch 25/25, Binary loss: 3.984\n",
      "Epoch 1/25, Binary loss: 5.548\n",
      "Epoch 2/25, Binary loss: 5.493\n",
      "Epoch 3/25, Binary loss: 5.455\n",
      "Epoch 4/25, Binary loss: 5.409\n",
      "Epoch 5/25, Binary loss: 5.381\n",
      "Epoch 6/25, Binary loss: 5.346\n",
      "Epoch 7/25, Binary loss: 5.319\n",
      "Epoch 8/25, Binary loss: 5.278\n",
      "Epoch 9/25, Binary loss: 5.242\n",
      "Epoch 10/25, Binary loss: 5.187\n",
      "Epoch 11/25, Binary loss: 5.125\n",
      "Epoch 12/25, Binary loss: 5.051\n",
      "Epoch 13/25, Binary loss: 4.970\n",
      "Epoch 14/25, Binary loss: 4.867\n",
      "Epoch 15/25, Binary loss: 4.768\n",
      "Epoch 16/25, Binary loss: 4.640\n",
      "Epoch 17/25, Binary loss: 4.520\n",
      "Epoch 18/25, Binary loss: 4.390\n",
      "Epoch 19/25, Binary loss: 4.253\n",
      "Epoch 20/25, Binary loss: 4.134\n",
      "Epoch 21/25, Binary loss: 4.024\n",
      "Epoch 22/25, Binary loss: 3.919\n",
      "Epoch 23/25, Binary loss: 3.809\n",
      "Epoch 24/25, Binary loss: 3.727\n",
      "Epoch 25/25, Binary loss: 3.638\n",
      "Epoch 1/25, Binary loss: 5.528\n",
      "Epoch 2/25, Binary loss: 5.522\n",
      "Epoch 3/25, Binary loss: 5.507\n",
      "Epoch 4/25, Binary loss: 5.493\n",
      "Epoch 5/25, Binary loss: 5.475\n",
      "Epoch 6/25, Binary loss: 5.454\n",
      "Epoch 7/25, Binary loss: 5.431\n",
      "Epoch 8/25, Binary loss: 5.406\n",
      "Epoch 9/25, Binary loss: 5.372\n",
      "Epoch 10/25, Binary loss: 5.330\n",
      "Epoch 11/25, Binary loss: 5.285\n",
      "Epoch 12/25, Binary loss: 5.225\n",
      "Epoch 13/25, Binary loss: 5.163\n",
      "Epoch 14/25, Binary loss: 5.079\n",
      "Epoch 15/25, Binary loss: 4.995\n",
      "Epoch 16/25, Binary loss: 4.890\n",
      "Epoch 17/25, Binary loss: 4.782\n",
      "Epoch 18/25, Binary loss: 4.665\n",
      "Epoch 19/25, Binary loss: 4.543\n",
      "Epoch 20/25, Binary loss: 4.423\n",
      "Epoch 21/25, Binary loss: 4.327\n",
      "Epoch 22/25, Binary loss: 4.203\n",
      "Epoch 23/25, Binary loss: 4.109\n",
      "Epoch 24/25, Binary loss: 4.010\n",
      "Epoch 25/25, Binary loss: 3.915\n",
      "Epoch 1/25, Binary loss: 5.589\n",
      "Epoch 2/25, Binary loss: 5.564\n",
      "Epoch 3/25, Binary loss: 5.538\n",
      "Epoch 4/25, Binary loss: 5.513\n",
      "Epoch 5/25, Binary loss: 5.479\n",
      "Epoch 6/25, Binary loss: 5.447\n",
      "Epoch 7/25, Binary loss: 5.407\n",
      "Epoch 8/25, Binary loss: 5.361\n",
      "Epoch 9/25, Binary loss: 5.305\n",
      "Epoch 10/25, Binary loss: 5.235\n",
      "Epoch 11/25, Binary loss: 5.150\n",
      "Epoch 12/25, Binary loss: 5.056\n",
      "Epoch 13/25, Binary loss: 4.942\n",
      "Epoch 14/25, Binary loss: 4.829\n",
      "Epoch 15/25, Binary loss: 4.678\n",
      "Epoch 16/25, Binary loss: 4.539\n",
      "Epoch 17/25, Binary loss: 4.412\n",
      "Epoch 18/25, Binary loss: 4.242\n",
      "Epoch 19/25, Binary loss: 4.079\n",
      "Epoch 20/25, Binary loss: 3.920\n",
      "Epoch 21/25, Binary loss: 3.773\n",
      "Epoch 22/25, Binary loss: 3.662\n",
      "Epoch 23/25, Binary loss: 3.546\n",
      "Epoch 24/25, Binary loss: 3.459\n",
      "Epoch 25/25, Binary loss: 3.370\n",
      "Epoch 1/25, Binary loss: 5.616\n",
      "Epoch 2/25, Binary loss: 5.568\n",
      "Epoch 3/25, Binary loss: 5.523\n",
      "Epoch 4/25, Binary loss: 5.504\n",
      "Epoch 5/25, Binary loss: 5.478\n",
      "Epoch 6/25, Binary loss: 5.460\n",
      "Epoch 7/25, Binary loss: 5.438\n",
      "Epoch 8/25, Binary loss: 5.410\n",
      "Epoch 9/25, Binary loss: 5.376\n",
      "Epoch 10/25, Binary loss: 5.337\n",
      "Epoch 11/25, Binary loss: 5.284\n",
      "Epoch 12/25, Binary loss: 5.220\n",
      "Epoch 13/25, Binary loss: 5.146\n",
      "Epoch 14/25, Binary loss: 5.071\n",
      "Epoch 15/25, Binary loss: 4.970\n",
      "Epoch 16/25, Binary loss: 4.871\n",
      "Epoch 17/25, Binary loss: 4.764\n",
      "Epoch 18/25, Binary loss: 4.652\n",
      "Epoch 19/25, Binary loss: 4.530\n",
      "Epoch 20/25, Binary loss: 4.396\n",
      "Epoch 21/25, Binary loss: 4.271\n",
      "Epoch 22/25, Binary loss: 4.173\n",
      "Epoch 23/25, Binary loss: 4.043\n",
      "Epoch 24/25, Binary loss: 3.945\n",
      "Epoch 25/25, Binary loss: 3.857\n",
      "Epoch 1/25, Binary loss: 5.553\n",
      "Epoch 2/25, Binary loss: 5.514\n",
      "Epoch 3/25, Binary loss: 5.479\n",
      "Epoch 4/25, Binary loss: 5.454\n",
      "Epoch 5/25, Binary loss: 5.436\n",
      "Epoch 6/25, Binary loss: 5.418\n",
      "Epoch 7/25, Binary loss: 5.397\n",
      "Epoch 8/25, Binary loss: 5.370\n",
      "Epoch 9/25, Binary loss: 5.344\n",
      "Epoch 10/25, Binary loss: 5.305\n",
      "Epoch 11/25, Binary loss: 5.259\n",
      "Epoch 12/25, Binary loss: 5.208\n",
      "Epoch 13/25, Binary loss: 5.144\n",
      "Epoch 14/25, Binary loss: 5.070\n",
      "Epoch 15/25, Binary loss: 4.987\n",
      "Epoch 16/25, Binary loss: 4.898\n",
      "Epoch 17/25, Binary loss: 4.793\n",
      "Epoch 18/25, Binary loss: 4.690\n",
      "Epoch 19/25, Binary loss: 4.603\n",
      "Epoch 20/25, Binary loss: 4.476\n",
      "Epoch 21/25, Binary loss: 4.378\n",
      "Epoch 22/25, Binary loss: 4.259\n",
      "Epoch 23/25, Binary loss: 4.144\n",
      "Epoch 24/25, Binary loss: 4.035\n",
      "Epoch 25/25, Binary loss: 3.925\n",
      "Epoch 1/25, Binary loss: 5.618\n",
      "Epoch 2/25, Binary loss: 5.582\n",
      "Epoch 3/25, Binary loss: 5.548\n",
      "Epoch 4/25, Binary loss: 5.511\n",
      "Epoch 5/25, Binary loss: 5.467\n",
      "Epoch 6/25, Binary loss: 5.417\n",
      "Epoch 7/25, Binary loss: 5.367\n",
      "Epoch 8/25, Binary loss: 5.301\n",
      "Epoch 9/25, Binary loss: 5.233\n",
      "Epoch 10/25, Binary loss: 5.152\n",
      "Epoch 11/25, Binary loss: 5.053\n",
      "Epoch 12/25, Binary loss: 4.937\n",
      "Epoch 13/25, Binary loss: 4.811\n",
      "Epoch 14/25, Binary loss: 4.660\n",
      "Epoch 15/25, Binary loss: 4.522\n",
      "Epoch 16/25, Binary loss: 4.367\n",
      "Epoch 17/25, Binary loss: 4.214\n",
      "Epoch 18/25, Binary loss: 4.075\n",
      "Epoch 19/25, Binary loss: 3.928\n",
      "Epoch 20/25, Binary loss: 3.805\n",
      "Epoch 21/25, Binary loss: 3.663\n",
      "Epoch 22/25, Binary loss: 3.554\n",
      "Epoch 23/25, Binary loss: 3.466\n",
      "Epoch 24/25, Binary loss: 3.354\n",
      "Epoch 25/25, Binary loss: 3.238\n",
      "Epoch 1/25, Binary loss: 5.557\n",
      "Epoch 2/25, Binary loss: 5.529\n",
      "Epoch 3/25, Binary loss: 5.503\n",
      "Epoch 4/25, Binary loss: 5.481\n",
      "Epoch 5/25, Binary loss: 5.463\n",
      "Epoch 6/25, Binary loss: 5.448\n",
      "Epoch 7/25, Binary loss: 5.423\n",
      "Epoch 8/25, Binary loss: 5.392\n",
      "Epoch 9/25, Binary loss: 5.361\n",
      "Epoch 10/25, Binary loss: 5.323\n",
      "Epoch 11/25, Binary loss: 5.277\n",
      "Epoch 12/25, Binary loss: 5.225\n",
      "Epoch 13/25, Binary loss: 5.157\n",
      "Epoch 14/25, Binary loss: 5.089\n",
      "Epoch 15/25, Binary loss: 4.999\n",
      "Epoch 16/25, Binary loss: 4.922\n",
      "Epoch 17/25, Binary loss: 4.809\n",
      "Epoch 18/25, Binary loss: 4.699\n",
      "Epoch 19/25, Binary loss: 4.587\n",
      "Epoch 20/25, Binary loss: 4.473\n",
      "Epoch 21/25, Binary loss: 4.362\n",
      "Epoch 22/25, Binary loss: 4.248\n",
      "Epoch 23/25, Binary loss: 4.140\n",
      "Epoch 24/25, Binary loss: 4.042\n",
      "Epoch 25/25, Binary loss: 3.931\n",
      "Epoch 1/25, Binary loss: 9.170\n",
      "Epoch 2/25, Binary loss: 9.152\n",
      "Epoch 3/25, Binary loss: 9.140\n",
      "Epoch 4/25, Binary loss: 9.112\n",
      "Epoch 5/25, Binary loss: 9.089\n",
      "Epoch 6/25, Binary loss: 9.063\n",
      "Epoch 7/25, Binary loss: 9.030\n",
      "Epoch 8/25, Binary loss: 8.993\n",
      "Epoch 9/25, Binary loss: 8.939\n",
      "Epoch 10/25, Binary loss: 8.895\n",
      "Epoch 11/25, Binary loss: 8.829\n",
      "Epoch 12/25, Binary loss: 8.767\n",
      "Epoch 13/25, Binary loss: 8.692\n",
      "Epoch 14/25, Binary loss: 8.615\n",
      "Epoch 15/25, Binary loss: 8.515\n",
      "Epoch 16/25, Binary loss: 8.388\n",
      "Epoch 17/25, Binary loss: 8.255\n",
      "Epoch 18/25, Binary loss: 8.127\n",
      "Epoch 19/25, Binary loss: 7.993\n",
      "Epoch 20/25, Binary loss: 7.880\n",
      "Epoch 21/25, Binary loss: 7.754\n",
      "Epoch 22/25, Binary loss: 7.631\n",
      "Epoch 23/25, Binary loss: 7.499\n",
      "Epoch 24/25, Binary loss: 7.369\n",
      "Epoch 25/25, Binary loss: 7.270\n",
      "Epoch 1/25, Binary loss: 9.249\n",
      "Epoch 2/25, Binary loss: 9.203\n",
      "Epoch 3/25, Binary loss: 9.158\n",
      "Epoch 4/25, Binary loss: 9.115\n",
      "Epoch 5/25, Binary loss: 9.068\n",
      "Epoch 6/25, Binary loss: 9.026\n",
      "Epoch 7/25, Binary loss: 8.984\n",
      "Epoch 8/25, Binary loss: 8.928\n",
      "Epoch 9/25, Binary loss: 8.862\n",
      "Epoch 10/25, Binary loss: 8.801\n",
      "Epoch 11/25, Binary loss: 8.730\n",
      "Epoch 12/25, Binary loss: 8.656\n",
      "Epoch 13/25, Binary loss: 8.570\n",
      "Epoch 14/25, Binary loss: 8.489\n",
      "Epoch 15/25, Binary loss: 8.400\n",
      "Epoch 16/25, Binary loss: 8.281\n",
      "Epoch 17/25, Binary loss: 8.175\n",
      "Epoch 18/25, Binary loss: 8.068\n",
      "Epoch 19/25, Binary loss: 7.976\n",
      "Epoch 20/25, Binary loss: 7.885\n",
      "Epoch 21/25, Binary loss: 7.786\n",
      "Epoch 22/25, Binary loss: 7.702\n",
      "Epoch 23/25, Binary loss: 7.622\n",
      "Epoch 24/25, Binary loss: 7.548\n",
      "Epoch 25/25, Binary loss: 7.449\n",
      "Epoch 1/25, Binary loss: 9.312\n",
      "Epoch 2/25, Binary loss: 9.210\n",
      "Epoch 3/25, Binary loss: 9.162\n",
      "Epoch 4/25, Binary loss: 9.136\n",
      "Epoch 5/25, Binary loss: 9.119\n",
      "Epoch 6/25, Binary loss: 9.101\n",
      "Epoch 7/25, Binary loss: 9.079\n",
      "Epoch 8/25, Binary loss: 9.052\n",
      "Epoch 9/25, Binary loss: 9.029\n",
      "Epoch 10/25, Binary loss: 8.994\n",
      "Epoch 11/25, Binary loss: 8.951\n",
      "Epoch 12/25, Binary loss: 8.901\n",
      "Epoch 13/25, Binary loss: 8.844\n",
      "Epoch 14/25, Binary loss: 8.778\n",
      "Epoch 15/25, Binary loss: 8.704\n",
      "Epoch 16/25, Binary loss: 8.616\n",
      "Epoch 17/25, Binary loss: 8.523\n",
      "Epoch 18/25, Binary loss: 8.412\n",
      "Epoch 19/25, Binary loss: 8.307\n",
      "Epoch 20/25, Binary loss: 8.192\n",
      "Epoch 21/25, Binary loss: 8.078\n",
      "Epoch 22/25, Binary loss: 7.963\n",
      "Epoch 23/25, Binary loss: 7.851\n",
      "Epoch 24/25, Binary loss: 7.766\n",
      "Epoch 25/25, Binary loss: 7.658\n",
      "Epoch 1/25, Binary loss: 9.232\n",
      "Epoch 2/25, Binary loss: 9.178\n",
      "Epoch 3/25, Binary loss: 9.139\n",
      "Epoch 4/25, Binary loss: 9.093\n",
      "Epoch 5/25, Binary loss: 9.061\n",
      "Epoch 6/25, Binary loss: 9.025\n",
      "Epoch 7/25, Binary loss: 8.995\n",
      "Epoch 8/25, Binary loss: 8.952\n",
      "Epoch 9/25, Binary loss: 8.913\n",
      "Epoch 10/25, Binary loss: 8.852\n",
      "Epoch 11/25, Binary loss: 8.783\n",
      "Epoch 12/25, Binary loss: 8.700\n",
      "Epoch 13/25, Binary loss: 8.607\n",
      "Epoch 14/25, Binary loss: 8.493\n",
      "Epoch 15/25, Binary loss: 8.388\n",
      "Epoch 16/25, Binary loss: 8.257\n",
      "Epoch 17/25, Binary loss: 8.138\n",
      "Epoch 18/25, Binary loss: 8.019\n",
      "Epoch 19/25, Binary loss: 7.893\n",
      "Epoch 20/25, Binary loss: 7.787\n",
      "Epoch 21/25, Binary loss: 7.687\n",
      "Epoch 22/25, Binary loss: 7.599\n",
      "Epoch 23/25, Binary loss: 7.501\n",
      "Epoch 24/25, Binary loss: 7.427\n",
      "Epoch 25/25, Binary loss: 7.342\n",
      "Epoch 1/25, Binary loss: 9.212\n",
      "Epoch 2/25, Binary loss: 9.206\n",
      "Epoch 3/25, Binary loss: 9.192\n",
      "Epoch 4/25, Binary loss: 9.177\n",
      "Epoch 5/25, Binary loss: 9.159\n",
      "Epoch 6/25, Binary loss: 9.139\n",
      "Epoch 7/25, Binary loss: 9.118\n",
      "Epoch 8/25, Binary loss: 9.093\n",
      "Epoch 9/25, Binary loss: 9.058\n",
      "Epoch 10/25, Binary loss: 9.012\n",
      "Epoch 11/25, Binary loss: 8.964\n",
      "Epoch 12/25, Binary loss: 8.899\n",
      "Epoch 13/25, Binary loss: 8.838\n",
      "Epoch 14/25, Binary loss: 8.757\n",
      "Epoch 15/25, Binary loss: 8.678\n",
      "Epoch 16/25, Binary loss: 8.584\n",
      "Epoch 17/25, Binary loss: 8.491\n",
      "Epoch 18/25, Binary loss: 8.393\n",
      "Epoch 19/25, Binary loss: 8.289\n",
      "Epoch 20/25, Binary loss: 8.186\n",
      "Epoch 21/25, Binary loss: 8.107\n",
      "Epoch 22/25, Binary loss: 7.996\n",
      "Epoch 23/25, Binary loss: 7.898\n",
      "Epoch 24/25, Binary loss: 7.811\n",
      "Epoch 25/25, Binary loss: 7.718\n",
      "Epoch 1/25, Binary loss: 9.274\n",
      "Epoch 2/25, Binary loss: 9.248\n",
      "Epoch 3/25, Binary loss: 9.222\n",
      "Epoch 4/25, Binary loss: 9.197\n",
      "Epoch 5/25, Binary loss: 9.163\n",
      "Epoch 6/25, Binary loss: 9.131\n",
      "Epoch 7/25, Binary loss: 9.092\n",
      "Epoch 8/25, Binary loss: 9.047\n",
      "Epoch 9/25, Binary loss: 8.995\n",
      "Epoch 10/25, Binary loss: 8.929\n",
      "Epoch 11/25, Binary loss: 8.851\n",
      "Epoch 12/25, Binary loss: 8.753\n",
      "Epoch 13/25, Binary loss: 8.621\n",
      "Epoch 14/25, Binary loss: 8.498\n",
      "Epoch 15/25, Binary loss: 8.331\n",
      "Epoch 16/25, Binary loss: 8.180\n",
      "Epoch 17/25, Binary loss: 8.048\n",
      "Epoch 18/25, Binary loss: 7.879\n",
      "Epoch 19/25, Binary loss: 7.727\n",
      "Epoch 20/25, Binary loss: 7.579\n",
      "Epoch 21/25, Binary loss: 7.441\n",
      "Epoch 22/25, Binary loss: 7.330\n",
      "Epoch 23/25, Binary loss: 7.216\n",
      "Epoch 24/25, Binary loss: 7.125\n",
      "Epoch 25/25, Binary loss: 7.034\n",
      "Epoch 1/25, Binary loss: 9.299\n",
      "Epoch 2/25, Binary loss: 9.251\n",
      "Epoch 3/25, Binary loss: 9.206\n",
      "Epoch 4/25, Binary loss: 9.185\n",
      "Epoch 5/25, Binary loss: 9.158\n",
      "Epoch 6/25, Binary loss: 9.137\n",
      "Epoch 7/25, Binary loss: 9.113\n",
      "Epoch 8/25, Binary loss: 9.082\n",
      "Epoch 9/25, Binary loss: 9.045\n",
      "Epoch 10/25, Binary loss: 9.003\n",
      "Epoch 11/25, Binary loss: 8.946\n",
      "Epoch 12/25, Binary loss: 8.877\n",
      "Epoch 13/25, Binary loss: 8.798\n",
      "Epoch 14/25, Binary loss: 8.720\n",
      "Epoch 15/25, Binary loss: 8.611\n",
      "Epoch 16/25, Binary loss: 8.506\n",
      "Epoch 17/25, Binary loss: 8.388\n",
      "Epoch 18/25, Binary loss: 8.262\n",
      "Epoch 19/25, Binary loss: 8.115\n",
      "Epoch 20/25, Binary loss: 7.975\n",
      "Epoch 21/25, Binary loss: 7.849\n",
      "Epoch 22/25, Binary loss: 7.756\n",
      "Epoch 23/25, Binary loss: 7.627\n",
      "Epoch 24/25, Binary loss: 7.530\n",
      "Epoch 25/25, Binary loss: 7.448\n",
      "Epoch 1/25, Binary loss: 9.237\n",
      "Epoch 2/25, Binary loss: 9.197\n",
      "Epoch 3/25, Binary loss: 9.163\n",
      "Epoch 4/25, Binary loss: 9.135\n",
      "Epoch 5/25, Binary loss: 9.115\n",
      "Epoch 6/25, Binary loss: 9.095\n",
      "Epoch 7/25, Binary loss: 9.070\n",
      "Epoch 8/25, Binary loss: 9.039\n",
      "Epoch 9/25, Binary loss: 9.005\n",
      "Epoch 10/25, Binary loss: 8.957\n",
      "Epoch 11/25, Binary loss: 8.901\n",
      "Epoch 12/25, Binary loss: 8.835\n",
      "Epoch 13/25, Binary loss: 8.754\n",
      "Epoch 14/25, Binary loss: 8.663\n",
      "Epoch 15/25, Binary loss: 8.559\n",
      "Epoch 16/25, Binary loss: 8.449\n",
      "Epoch 17/25, Binary loss: 8.322\n",
      "Epoch 18/25, Binary loss: 8.194\n",
      "Epoch 19/25, Binary loss: 8.091\n",
      "Epoch 20/25, Binary loss: 7.948\n",
      "Epoch 21/25, Binary loss: 7.831\n",
      "Epoch 22/25, Binary loss: 7.713\n",
      "Epoch 23/25, Binary loss: 7.591\n",
      "Epoch 24/25, Binary loss: 7.482\n",
      "Epoch 25/25, Binary loss: 7.379\n",
      "Epoch 1/25, Binary loss: 9.302\n",
      "Epoch 2/25, Binary loss: 9.264\n",
      "Epoch 3/25, Binary loss: 9.230\n",
      "Epoch 4/25, Binary loss: 9.191\n",
      "Epoch 5/25, Binary loss: 9.145\n",
      "Epoch 6/25, Binary loss: 9.094\n",
      "Epoch 7/25, Binary loss: 9.042\n",
      "Epoch 8/25, Binary loss: 8.973\n",
      "Epoch 9/25, Binary loss: 8.901\n",
      "Epoch 10/25, Binary loss: 8.815\n",
      "Epoch 11/25, Binary loss: 8.713\n",
      "Epoch 12/25, Binary loss: 8.592\n",
      "Epoch 13/25, Binary loss: 8.461\n",
      "Epoch 14/25, Binary loss: 8.306\n",
      "Epoch 15/25, Binary loss: 8.165\n",
      "Epoch 16/25, Binary loss: 8.007\n",
      "Epoch 17/25, Binary loss: 7.854\n",
      "Epoch 18/25, Binary loss: 7.715\n",
      "Epoch 19/25, Binary loss: 7.571\n",
      "Epoch 20/25, Binary loss: 7.452\n",
      "Epoch 21/25, Binary loss: 7.309\n",
      "Epoch 22/25, Binary loss: 7.203\n",
      "Epoch 23/25, Binary loss: 7.116\n",
      "Epoch 24/25, Binary loss: 7.005\n",
      "Epoch 25/25, Binary loss: 6.896\n",
      "Epoch 1/25, Binary loss: 9.242\n",
      "Epoch 2/25, Binary loss: 9.214\n",
      "Epoch 3/25, Binary loss: 9.191\n",
      "Epoch 4/25, Binary loss: 9.170\n",
      "Epoch 5/25, Binary loss: 9.152\n",
      "Epoch 6/25, Binary loss: 9.138\n",
      "Epoch 7/25, Binary loss: 9.114\n",
      "Epoch 8/25, Binary loss: 9.083\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-01ebcd80b9a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msiamese\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maux\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m             \u001b[0mtr_accuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mte_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msiamese\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maux\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0maccurate_aux\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mte_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-e01b1650c791>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, eta, decay, n_epochs, verbose, siamese, aux, alpha)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_classes\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m             \u001b[1;31m# Forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_64\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_64\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_64\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3_64\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_64\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_64\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'numpy'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'string_'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "accuracy_aux = []\n",
    "std_aux = []\n",
    "model_number_aux = 0\n",
    "for models in [Train_auxmodel[3]]:\n",
    "    print(models)\n",
    "    test_accuracies_aux = torch.empty((1, 11))\n",
    "    test_stds_aux = torch.empty((1,11))    \n",
    "    for j in range(11):\n",
    "        accurate_aux = []\n",
    "        for i in range(10):\n",
    "            model = models[0]()\n",
    "            model.to(device)\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size = models[4], seed=i)\n",
    "            train(model, train_loader, models[3], 0, 25, verbose=True, siamese=models[1], aux=models[2], alpha = j/10)\n",
    "            tr_accuracy, te_accuracy = accu(model, train_loader, test_loader, siamese=models[1], aux=models[2])\n",
    "            accurate_aux.append(te_accuracy)\n",
    "        test_accuracies_aux[0,j] =  torch.FloatTensor(accurate_aux).mean()\n",
    "        test_stds_aux[0,j] =  torch.FloatTensor(accurate_aux).std()\n",
    "    accuracy_aux.append(test_accuracies_aux)\n",
    "    std_aux.append(test_stds_aux)\n",
    "    max_index = test_accuracies_aux.argmax() \n",
    "    model_number_aux += 1\n",
    "    print('The optimal alpha for aux_model_%d is %.2f' %(model_number_aux, max_index/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal alpha for aux_model_1 is 0.80\n",
      "The optimal alpha for aux_model_2 is 1.00\n",
      "The optimal alpha for aux_model_3 is 0.80\n",
      "The optimal alpha for aux_model_4 is 0.00\n",
      "The optimal alpha for aux_model_5 is 0.90\n",
      "The optimal alpha for aux_model_6 is 0.80\n"
     ]
    }
   ],
   "source": [
    "accuracy_aux = []\n",
    "std_aux = []\n",
    "model_number_aux = 0\n",
    "for models in Train_auxmodel:\n",
    "    test_accuracies_aux = torch.empty((1, 11))\n",
    "    test_stds_aux = torch.empty((1,11))    \n",
    "    for j in range(11):\n",
    "        accurate_aux = []\n",
    "        for i in range(10):\n",
    "            model = models[0]()\n",
    "            model.to(device)\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size = models[4], seed=i)\n",
    "            train(model, train_loader, models[3], 0, 25, verbose=False, siamese=models[1], aux=models[2], alpha = j/10)\n",
    "            tr_accuracy, te_accuracy = accu(model, train_loader, test_loader, siamese=models[1], aux=models[2])\n",
    "            accurate_aux.append(te_accuracy)\n",
    "        test_accuracies_aux[0,j] =  torch.FloatTensor(accurate_aux).mean()\n",
    "        test_stds_aux[0,j] =  torch.FloatTensor(accurate_aux).std()\n",
    "    accuracy_aux.append(test_accuracies_aux)\n",
    "    std_aux.append(test_stds_aux)\n",
    "    max_index = test_accuracies_aux.argmax() \n",
    "    model_number_aux += 1\n",
    "    print('The optimal alpha for aux_model_%d is %.2f' %(model_number_aux, max_index/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.8170, 0.8248, 0.8303, 0.8349, 0.8381, 0.8411, 0.8430, 0.8432, 0.8488,\n",
       "          0.8440, 0.8472]]),\n",
       " tensor([[0.8447, 0.8457, 0.8537, 0.8534, 0.8562, 0.8666, 0.8639, 0.8682, 0.8687,\n",
       "          0.8722, 0.8752]]),\n",
       " tensor([[0.8365, 0.8355, 0.8364, 0.8414, 0.8436, 0.8490, 0.8479, 0.8525, 0.8616,\n",
       "          0.8584, 0.8532]]),\n",
       " tensor([[0.7969, 0.7944, 0.7895, 0.7766, 0.7578, 0.7373, 0.7253, 0.7099, 0.6919,\n",
       "          0.6775, 0.6624]]),\n",
       " tensor([[0.8493, 0.8560, 0.8571, 0.8596, 0.8652, 0.8759, 0.8703, 0.8724, 0.8765,\n",
       "          0.8817, 0.8780]]),\n",
       " tensor([[0.8693, 0.8782, 0.8795, 0.8845, 0.8826, 0.8842, 0.8854, 0.8859, 0.8907,\n",
       "          0.8897, 0.8900]])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.5550e-01, 8.2610e-01, 8.3650e-01, 8.1070e-01, 8.3090e-01],\n",
       "        [1.0561e-38, 7.3470e-39, 1.0194e-38, 9.2755e-39, 1.0653e-38],\n",
       "        [4.1327e-39, 8.9082e-39, 1.0102e-38, 7.3470e-39, 1.0194e-38],\n",
       "        [9.2755e-39, 1.0653e-38, 4.1327e-39, 8.9082e-39, 1.0102e-38]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.empty(4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "13//7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.) tensor(0.8110)\n",
      "tensor(1.) tensor(0.8030)\n",
      "tensor(1.) tensor(0.8140)\n",
      "tensor(1.) tensor(0.8200)\n",
      "tensor(1.) tensor(0.8110)\n",
      "tensor(1.) tensor(0.8140)\n",
      "tensor(1.) tensor(0.8180)\n",
      "tensor(1.) tensor(0.8200)\n",
      "tensor(1.) tensor(0.8110)\n",
      "tensor(1.) tensor(0.8110)\n",
      "Mean: 0.813, Std: 0.005\n"
     ]
    }
   ],
   "source": [
    "accuracies8 = []\n",
    "times8 = []\n",
    "losses8 = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=8, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    #model = SiameseBaseNet()\n",
    "    model = MLP()\n",
    "    model.to(device)\n",
    "    losses8[i-10, :] = torch.tensor(train(model, train_loader, 5e-4, 0, 25, verbose=False, siamese=False))\n",
    "    time2 = time.perf_counter()\n",
    "    times8.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies8.append(te_accuracy)\n",
    "\n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies8).mean(), torch.tensor(accuracies8).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times8).mean(), torch.tensor(times8).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies9 = []\n",
    "times9 = []\n",
    "losses9 = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=8, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    \n",
    "    model = SiameseMLP()\n",
    "    model.to(device)\n",
    "\n",
    "    losses9[i-10,:] = torch.tensor(train(model, train_loader, 5e-3, 0, 25, verbose=False, siamese=True))\n",
    "    time2 = time.perf_counter()\n",
    "    times9.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_siamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies9.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies9).mean(), torch.tensor(accuracies9).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times9).mean(), torch.tensor(times9).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies10 = []\n",
    "times10 = []\n",
    "losses10 = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=16, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "\n",
    "    model = AuxMLP()\n",
    "    model.to(device)\n",
    "    losses10[i-10, :] = torch.tensor(train(model, train_loader, 5e-3, 0, 25, verbose=False, siamese=False, aux=True, alpha = 0.9))\n",
    "    time2 = time.perf_counter()\n",
    "    times10.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies10.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies10).mean(), torch.tensor(accuracies10).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times10).mean(), torch.tensor(times10).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies11 = []\n",
    "times11 = []\n",
    "losses11 = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=8, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "\n",
    "    model = AuxsiameseMLP()\n",
    "    model.to(device)\n",
    "    losses11[i-10, :] = torch.tensor(train(model, train_loader, 5e-3, 0, 25, verbose=False, siamese=True, aux=True, alpha = 0.7))\n",
    "    time2 = time.perf_counter()\n",
    "    times11.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies11.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies11).mean(), torch.tensor(accuracies11).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times11).mean(), torch.tensor(times11).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "times = []\n",
    "losses = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=16, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "\n",
    "    model = BaseNet()\n",
    "    model.to(device)\n",
    "    losses[i-10, :] = torch.tensor(train(model, train_loader, 5e-4, 0, 25, verbose=False, siamese=False))\n",
    "    time2 = time.perf_counter()\n",
    "    times.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies.append(te_accuracy)\n",
    "\n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies).mean(), torch.tensor(accuracies).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times).mean(), torch.tensor(times).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies1 = []\n",
    "times1 = []\n",
    "losses1 = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=16, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    model = SiameseBaseNet()\n",
    "    model.to(device)\n",
    "\n",
    "    losses1[i-10, :] = torch.tensor(train(model, train_loader, 1e-3, 0, 25, verbose=False, siamese=True))\n",
    "    time2 = time.perf_counter()\n",
    "    times1.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_siamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies1.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies1).mean(), torch.tensor(accuracies1).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times1).mean(), torch.tensor(times1).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies2 = []\n",
    "times2 = []\n",
    "losses2 = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=8, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "\n",
    "    model = AuxBaseNet()\n",
    "    model.to(device)\n",
    "    losses2[i-10, :] = torch.tensor(train(model, train_loader, 5e-4, 0, 25, verbose=False, siamese=False, aux=True, alpha = 1.0))\n",
    "    time2 = time.perf_counter()\n",
    "    times2.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies2.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies2).mean(), torch.tensor(accuracies2).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times2).mean(), torch.tensor(times2).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies3 = []\n",
    "times3 = []\n",
    "losses3 = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=32, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    \n",
    "    model = AuxsiameseBaseNet()\n",
    "    model.to(device)\n",
    "    losses3[i-10, :] = torch.tensor(train(model, train_loader, 5e-3, 0, 25, verbose=False, siamese=True, aux=True, alpha = 0.6))\n",
    "    time2 = time.perf_counter()\n",
    "    times3.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies3.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies3).mean(), torch.tensor(accuracies3).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times3).mean(), torch.tensor(times3).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Binary loss: 19.887\n",
      "Epoch 2/25, Binary loss: 15.464\n",
      "Epoch 3/25, Binary loss: 12.644\n",
      "Epoch 4/25, Binary loss: 9.707\n",
      "Epoch 5/25, Binary loss: 7.833\n",
      "Epoch 6/25, Binary loss: 5.845\n",
      "Epoch 7/25, Binary loss: 4.467\n",
      "Epoch 8/25, Binary loss: 2.233\n",
      "Epoch 9/25, Binary loss: 1.382\n",
      "Epoch 10/25, Binary loss: 0.663\n",
      "Epoch 11/25, Binary loss: 0.480\n",
      "Epoch 12/25, Binary loss: 0.254\n",
      "Epoch 13/25, Binary loss: 0.166\n",
      "Epoch 14/25, Binary loss: 0.127\n",
      "Epoch 15/25, Binary loss: 0.094\n",
      "Epoch 16/25, Binary loss: 0.076\n",
      "Epoch 17/25, Binary loss: 0.062\n",
      "Epoch 18/25, Binary loss: 0.052\n",
      "Epoch 19/25, Binary loss: 0.047\n",
      "Epoch 20/25, Binary loss: 0.040\n",
      "Epoch 21/25, Binary loss: 0.037\n",
      "Epoch 22/25, Binary loss: 0.034\n",
      "Epoch 23/25, Binary loss: 0.031\n",
      "Epoch 24/25, Binary loss: 0.027\n",
      "Epoch 25/25, Binary loss: 0.025\n",
      "tensor(1., device='cuda:0') tensor(0.8400, device='cuda:0')\n",
      "Epoch 1/25, Binary loss: 20.017\n",
      "Epoch 2/25, Binary loss: 14.624\n",
      "Epoch 3/25, Binary loss: 11.656\n",
      "Epoch 4/25, Binary loss: 9.744\n",
      "Epoch 5/25, Binary loss: 8.570\n",
      "Epoch 6/25, Binary loss: 6.193\n",
      "Epoch 7/25, Binary loss: 3.675\n",
      "Epoch 8/25, Binary loss: 2.972\n",
      "Epoch 9/25, Binary loss: 2.093\n",
      "Epoch 10/25, Binary loss: 1.180\n",
      "Epoch 11/25, Binary loss: 0.499\n",
      "Epoch 12/25, Binary loss: 0.248\n",
      "Epoch 13/25, Binary loss: 0.170\n",
      "Epoch 14/25, Binary loss: 0.127\n",
      "Epoch 15/25, Binary loss: 0.105\n",
      "Epoch 16/25, Binary loss: 0.089\n",
      "Epoch 17/25, Binary loss: 0.063\n",
      "Epoch 18/25, Binary loss: 0.053\n",
      "Epoch 19/25, Binary loss: 0.046\n",
      "Epoch 20/25, Binary loss: 0.040\n",
      "Epoch 21/25, Binary loss: 0.034\n",
      "Epoch 22/25, Binary loss: 0.034\n",
      "Epoch 23/25, Binary loss: 0.029\n",
      "Epoch 24/25, Binary loss: 0.026\n",
      "Epoch 25/25, Binary loss: 0.023\n",
      "tensor(1., device='cuda:0') tensor(0.8200, device='cuda:0')\n",
      "Epoch 1/25, Binary loss: 18.839\n",
      "Epoch 2/25, Binary loss: 14.540\n",
      "Epoch 3/25, Binary loss: 11.242\n",
      "Epoch 4/25, Binary loss: 9.005\n",
      "Epoch 5/25, Binary loss: 6.763\n",
      "Epoch 6/25, Binary loss: 5.428\n",
      "Epoch 7/25, Binary loss: 4.102\n",
      "Epoch 8/25, Binary loss: 2.529\n",
      "Epoch 9/25, Binary loss: 1.439\n",
      "Epoch 10/25, Binary loss: 0.585\n",
      "Epoch 11/25, Binary loss: 0.386\n",
      "Epoch 12/25, Binary loss: 0.251\n",
      "Epoch 13/25, Binary loss: 0.143\n",
      "Epoch 14/25, Binary loss: 0.111\n",
      "Epoch 15/25, Binary loss: 0.081\n",
      "Epoch 16/25, Binary loss: 0.073\n",
      "Epoch 17/25, Binary loss: 0.061\n",
      "Epoch 18/25, Binary loss: 0.055\n",
      "Epoch 19/25, Binary loss: 0.052\n",
      "Epoch 20/25, Binary loss: 0.047\n",
      "Epoch 21/25, Binary loss: 0.051\n",
      "Epoch 22/25, Binary loss: 0.032\n",
      "Epoch 23/25, Binary loss: 0.033\n",
      "Epoch 24/25, Binary loss: 0.029\n",
      "Epoch 25/25, Binary loss: 0.025\n",
      "tensor(1., device='cuda:0') tensor(0.8320, device='cuda:0')\n",
      "Epoch 1/25, Binary loss: 18.897\n",
      "Epoch 2/25, Binary loss: 14.480\n",
      "Epoch 3/25, Binary loss: 11.141\n",
      "Epoch 4/25, Binary loss: 8.624\n",
      "Epoch 5/25, Binary loss: 6.419\n",
      "Epoch 6/25, Binary loss: 4.239\n",
      "Epoch 7/25, Binary loss: 3.671\n",
      "Epoch 8/25, Binary loss: 2.613\n",
      "Epoch 9/25, Binary loss: 1.975\n",
      "Epoch 10/25, Binary loss: 1.086\n",
      "Epoch 11/25, Binary loss: 0.564\n",
      "Epoch 12/25, Binary loss: 0.239\n",
      "Epoch 13/25, Binary loss: 0.144\n",
      "Epoch 14/25, Binary loss: 0.108\n",
      "Epoch 15/25, Binary loss: 0.094\n",
      "Epoch 16/25, Binary loss: 0.074\n",
      "Epoch 17/25, Binary loss: 0.060\n",
      "Epoch 18/25, Binary loss: 0.054\n",
      "Epoch 19/25, Binary loss: 0.048\n",
      "Epoch 20/25, Binary loss: 0.041\n",
      "Epoch 21/25, Binary loss: 0.038\n",
      "Epoch 22/25, Binary loss: 0.035\n",
      "Epoch 23/25, Binary loss: 0.030\n",
      "Epoch 24/25, Binary loss: 0.028\n",
      "Epoch 25/25, Binary loss: 0.024\n",
      "tensor(1., device='cuda:0') tensor(0.8230, device='cuda:0')\n",
      "Epoch 1/25, Binary loss: 19.497\n",
      "Epoch 2/25, Binary loss: 15.516\n",
      "Epoch 3/25, Binary loss: 12.856\n",
      "Epoch 4/25, Binary loss: 9.851\n",
      "Epoch 5/25, Binary loss: 7.664\n",
      "Epoch 6/25, Binary loss: 5.273\n",
      "Epoch 7/25, Binary loss: 4.548\n",
      "Epoch 8/25, Binary loss: 2.507\n",
      "Epoch 9/25, Binary loss: 1.769\n",
      "Epoch 10/25, Binary loss: 0.981\n",
      "Epoch 11/25, Binary loss: 0.554\n",
      "Epoch 12/25, Binary loss: 0.481\n",
      "Epoch 13/25, Binary loss: 0.253\n",
      "Epoch 14/25, Binary loss: 0.150\n",
      "Epoch 15/25, Binary loss: 0.117\n",
      "Epoch 16/25, Binary loss: 0.086\n",
      "Epoch 17/25, Binary loss: 0.075\n",
      "Epoch 18/25, Binary loss: 0.065\n",
      "Epoch 19/25, Binary loss: 0.053\n",
      "Epoch 20/25, Binary loss: 0.050\n",
      "Epoch 21/25, Binary loss: 0.043\n",
      "Epoch 22/25, Binary loss: 0.042\n",
      "Epoch 23/25, Binary loss: 0.035\n",
      "Epoch 24/25, Binary loss: 0.030\n",
      "Epoch 25/25, Binary loss: 0.027\n",
      "tensor(1., device='cuda:0') tensor(0.8190, device='cuda:0')\n",
      "Epoch 1/25, Binary loss: 20.623\n",
      "Epoch 2/25, Binary loss: 15.323\n",
      "Epoch 3/25, Binary loss: 12.275\n",
      "Epoch 4/25, Binary loss: 10.132\n",
      "Epoch 5/25, Binary loss: 7.587\n",
      "Epoch 6/25, Binary loss: 6.674\n",
      "Epoch 7/25, Binary loss: 4.498\n",
      "Epoch 8/25, Binary loss: 3.259\n",
      "Epoch 9/25, Binary loss: 2.253\n",
      "Epoch 10/25, Binary loss: 1.422\n",
      "Epoch 11/25, Binary loss: 0.590\n",
      "Epoch 12/25, Binary loss: 0.259\n",
      "Epoch 13/25, Binary loss: 0.165\n",
      "Epoch 14/25, Binary loss: 0.120\n",
      "Epoch 15/25, Binary loss: 0.097\n",
      "Epoch 16/25, Binary loss: 0.082\n",
      "Epoch 17/25, Binary loss: 0.069\n",
      "Epoch 18/25, Binary loss: 0.061\n",
      "Epoch 19/25, Binary loss: 0.051\n",
      "Epoch 20/25, Binary loss: 0.046\n",
      "Epoch 21/25, Binary loss: 0.039\n",
      "Epoch 22/25, Binary loss: 0.037\n",
      "Epoch 23/25, Binary loss: 0.034\n",
      "Epoch 24/25, Binary loss: 0.029\n",
      "Epoch 25/25, Binary loss: 0.029\n",
      "tensor(1., device='cuda:0') tensor(0.8150, device='cuda:0')\n",
      "Epoch 1/25, Binary loss: 21.660\n",
      "Epoch 2/25, Binary loss: 15.189\n",
      "Epoch 3/25, Binary loss: 12.019\n",
      "Epoch 4/25, Binary loss: 9.314\n",
      "Epoch 5/25, Binary loss: 8.156\n",
      "Epoch 6/25, Binary loss: 5.377\n",
      "Epoch 7/25, Binary loss: 3.262\n",
      "Epoch 8/25, Binary loss: 2.217\n",
      "Epoch 9/25, Binary loss: 2.696\n",
      "Epoch 10/25, Binary loss: 2.146\n",
      "Epoch 11/25, Binary loss: 2.306\n",
      "Epoch 12/25, Binary loss: 1.223\n",
      "Epoch 13/25, Binary loss: 0.471\n",
      "Epoch 14/25, Binary loss: 0.212\n",
      "Epoch 15/25, Binary loss: 0.126\n",
      "Epoch 16/25, Binary loss: 0.093\n",
      "Epoch 17/25, Binary loss: 0.077\n",
      "Epoch 18/25, Binary loss: 0.065\n",
      "Epoch 19/25, Binary loss: 0.056\n",
      "Epoch 20/25, Binary loss: 0.050\n",
      "Epoch 21/25, Binary loss: 0.045\n",
      "Epoch 22/25, Binary loss: 0.040\n",
      "Epoch 23/25, Binary loss: 0.035\n",
      "Epoch 24/25, Binary loss: 0.033\n",
      "Epoch 25/25, Binary loss: 0.031\n",
      "tensor(1., device='cuda:0') tensor(0.8410, device='cuda:0')\n",
      "Epoch 1/25, Binary loss: 19.237\n",
      "Epoch 2/25, Binary loss: 14.630\n",
      "Epoch 3/25, Binary loss: 12.450\n",
      "Epoch 4/25, Binary loss: 9.829\n",
      "Epoch 5/25, Binary loss: 7.195\n",
      "Epoch 6/25, Binary loss: 4.790\n",
      "Epoch 7/25, Binary loss: 3.216\n",
      "Epoch 8/25, Binary loss: 3.265\n",
      "Epoch 9/25, Binary loss: 2.687\n",
      "Epoch 10/25, Binary loss: 1.038\n",
      "Epoch 11/25, Binary loss: 0.461\n",
      "Epoch 12/25, Binary loss: 0.229\n",
      "Epoch 13/25, Binary loss: 0.183\n",
      "Epoch 14/25, Binary loss: 0.124\n",
      "Epoch 15/25, Binary loss: 0.089\n",
      "Epoch 16/25, Binary loss: 0.072\n",
      "Epoch 17/25, Binary loss: 0.060\n",
      "Epoch 18/25, Binary loss: 0.051\n",
      "Epoch 19/25, Binary loss: 0.045\n",
      "Epoch 20/25, Binary loss: 0.041\n",
      "Epoch 21/25, Binary loss: 0.036\n",
      "Epoch 22/25, Binary loss: 0.033\n",
      "Epoch 23/25, Binary loss: 0.028\n",
      "Epoch 24/25, Binary loss: 0.026\n",
      "Epoch 25/25, Binary loss: 0.023\n",
      "tensor(1., device='cuda:0') tensor(0.8320, device='cuda:0')\n",
      "Epoch 1/25, Binary loss: 19.490\n",
      "Epoch 2/25, Binary loss: 15.145\n",
      "Epoch 3/25, Binary loss: 12.268\n",
      "Epoch 4/25, Binary loss: 9.914\n",
      "Epoch 5/25, Binary loss: 7.054\n",
      "Epoch 6/25, Binary loss: 6.090\n",
      "Epoch 7/25, Binary loss: 5.373\n",
      "Epoch 8/25, Binary loss: 5.327\n",
      "Epoch 9/25, Binary loss: 2.908\n",
      "Epoch 10/25, Binary loss: 1.219\n",
      "Epoch 11/25, Binary loss: 0.566\n",
      "Epoch 12/25, Binary loss: 0.369\n",
      "Epoch 13/25, Binary loss: 0.236\n",
      "Epoch 14/25, Binary loss: 0.172\n",
      "Epoch 15/25, Binary loss: 0.127\n",
      "Epoch 16/25, Binary loss: 0.105\n",
      "Epoch 17/25, Binary loss: 0.087\n",
      "Epoch 18/25, Binary loss: 0.074\n",
      "Epoch 19/25, Binary loss: 0.066\n",
      "Epoch 20/25, Binary loss: 0.057\n",
      "Epoch 21/25, Binary loss: 0.050\n",
      "Epoch 22/25, Binary loss: 0.051\n",
      "Epoch 23/25, Binary loss: 0.046\n",
      "Epoch 24/25, Binary loss: 0.036\n",
      "Epoch 25/25, Binary loss: 0.030\n",
      "tensor(1., device='cuda:0') tensor(0.8190, device='cuda:0')\n",
      "Epoch 1/25, Binary loss: 19.773\n",
      "Epoch 2/25, Binary loss: 14.370\n",
      "Epoch 3/25, Binary loss: 12.184\n",
      "Epoch 4/25, Binary loss: 9.719\n",
      "Epoch 5/25, Binary loss: 6.906\n",
      "Epoch 6/25, Binary loss: 5.278\n",
      "Epoch 7/25, Binary loss: 2.867\n",
      "Epoch 8/25, Binary loss: 1.589\n",
      "Epoch 9/25, Binary loss: 0.888\n",
      "Epoch 10/25, Binary loss: 0.542\n",
      "Epoch 11/25, Binary loss: 0.370\n",
      "Epoch 12/25, Binary loss: 0.193\n",
      "Epoch 13/25, Binary loss: 0.151\n",
      "Epoch 14/25, Binary loss: 0.108\n",
      "Epoch 15/25, Binary loss: 0.091\n",
      "Epoch 16/25, Binary loss: 0.072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/25, Binary loss: 0.063\n",
      "Epoch 18/25, Binary loss: 0.055\n",
      "Epoch 19/25, Binary loss: 0.049\n",
      "Epoch 20/25, Binary loss: 0.042\n",
      "Epoch 21/25, Binary loss: 0.038\n",
      "Epoch 22/25, Binary loss: 0.034\n",
      "Epoch 23/25, Binary loss: 0.030\n",
      "Epoch 24/25, Binary loss: 0.025\n",
      "Epoch 25/25, Binary loss: 0.024\n",
      "tensor(1., device='cuda:0') tensor(0.8060, device='cuda:0')\n",
      "Mean: 0.825, Std: 0.011\n"
     ]
    }
   ],
   "source": [
    "accuracies4 = []\n",
    "times4 = []\n",
    "losses4 = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=32, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "\n",
    "    model = ResNet(nb_residual_blocks = 4, input_channels = 2, nb_channels = 16, kernel_size = 3, nb_classes = 2)\n",
    "    model.to(device)\n",
    "    losses4[i-10, :] = torch.tensor(train(model, train_loader, 1e-3, 0, 25, verbose=True, siamese=False))\n",
    "    time2 = time.perf_counter()\n",
    "    times4.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy, te_accuracy = accu(model, train_loader, test_loader, siamese=False, aux=False)\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies4.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies4).mean(), torch.tensor(accuracies4).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times4).mean(), torch.tensor(times4).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies5 = []\n",
    "times5 = []\n",
    "losses5 = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=32, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    \n",
    "    model = SiameseResNet(nb_residual_blocks = 4, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "    model.to(device)\n",
    "    losses5[i-10, :] = torch.tensor(train(model, train_loader, 5e-3, 0, 25, verbose=False, siamese=True))\n",
    "    time2 = time.perf_counter()\n",
    "    times5.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_siamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies5.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies5).mean(), torch.tensor(accuracies5).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times5).mean(), torch.tensor(times5).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies6 = []\n",
    "times6 = []\n",
    "losses6 = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=16, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    \n",
    "    model = AuxResNet(nb_residual_blocks = 10, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "    model.to(device)\n",
    "    losses6[i-10, :] = torch.tensor(train(model, train_loader, 1e-3, 0, 25, verbose=False, siamese=False, aux=True, alpha = 0.6))\n",
    "    time2 = time.perf_counter()\n",
    "    times6.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies6.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies6).mean(), torch.tensor(accuracies6).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times6).mean(), torch.tensor(times6).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies7 = []\n",
    "times7 = []\n",
    "losses7 = torch.empty((10,25))\n",
    "\n",
    "for i in range(10,20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=32, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    #model = SiameseBaseNet()\n",
    "    #model = BaseNet()\n",
    "    model = AuxsiameseResNet(nb_residual_blocks = 4, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "    model.to(device)\n",
    "    losses7[i-10, :] = torch.tensor(train(model, train_loader, 5e-3, 0, 25, verbose=False, siamese=True, aux=True, alpha = 0.6))\n",
    "    time2 = time.perf_counter()\n",
    "    times7.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies7.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies7).mean(), torch.tensor(accuracies7).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times7).mean(), torch.tensor(times7).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize learning rate and batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8057, 0.7989, 0.8030, 0.8060, 0.8008],\n",
      "        [0.8062, 0.8070, 0.8065, 0.8033, 0.8010],\n",
      "        [0.8071, 0.8070, 0.8051, 0.8005, 0.7952],\n",
      "        [0.7974, 0.7917, 0.7891, 0.7792, 0.7682]])\n"
     ]
    }
   ],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies1 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = MLP()\n",
    "            model.to(device)\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=False)\n",
    "            te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies1[j,k] =  torch.cuda.FloatTensor(accurate).mean()\n",
    "print(test_accuracies1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8033, device='cuda:0')\n",
      "tensor(0.7989, device='cuda:0')\n",
      "tensor(0.8030, device='cuda:0')\n",
      "tensor(0.8060, device='cuda:0')\n",
      "tensor(0.8008, device='cuda:0')\n",
      "tensor(0.8062, device='cuda:0')\n",
      "tensor(0.8070, device='cuda:0')\n",
      "tensor(0.8065, device='cuda:0')\n",
      "tensor(0.8033, device='cuda:0')\n",
      "tensor(0.8010, device='cuda:0')\n",
      "tensor(0.8071, device='cuda:0')\n",
      "tensor(0.8070, device='cuda:0')\n",
      "tensor(0.8051, device='cuda:0')\n",
      "tensor(0.8005, device='cuda:0')\n",
      "tensor(0.7952, device='cuda:0')\n",
      "tensor(0.7974, device='cuda:0')\n",
      "tensor(0.7917, device='cuda:0')\n",
      "tensor(0.7891, device='cuda:0')\n",
      "tensor(0.7792, device='cuda:0')\n",
      "tensor(0.7682, device='cuda:0')\n",
      "tensor([[0.8033, 0.7989, 0.8030, 0.8060, 0.8008],\n",
      "        [0.8062, 0.8070, 0.8065, 0.8033, 0.8010],\n",
      "        [0.8071, 0.8070, 0.8051, 0.8005, 0.7952],\n",
      "        [0.7974, 0.7917, 0.7891, 0.7792, 0.7682]])\n"
     ]
    }
   ],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies1 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = MLP()\n",
    "            model.to(device)\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=False, aux=False)\n",
    "            te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        print(torch.cuda.FloatTensor(accurate).mean())\n",
    "        test_accuracies1[j,k] =  torch.cuda.FloatTensor(accurate).mean()\n",
    "print(test_accuracies1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8509, 0.8454, 0.8457, 0.8422, 0.8374],\n",
      "        [0.8414, 0.8386, 0.8371, 0.8364, 0.8293],\n",
      "        [0.8403, 0.8347, 0.8343, 0.8299, 0.8143],\n",
      "        [0.8195, 0.8097, 0.7901, 0.7710, 0.7423]])\n"
     ]
    }
   ],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies2 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = SiameseMLP()\n",
    "            model.to(device)\n",
    "            #model = BaseNet()\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=True)\n",
    "            te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies2[j,k] =  torch.cuda.FloatTensor(accurate).mean()\n",
    "print(test_accuracies2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8190, 0.8242, 0.8229, 0.8176, 0.8115],\n",
      "        [0.8141, 0.8120, 0.8107, 0.8063, 0.8106],\n",
      "        [0.8127, 0.8092, 0.8097, 0.8116, 0.8016],\n",
      "        [0.8058, 0.7968, 0.7874, 0.7730, 0.7571]])\n"
     ]
    }
   ],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies3 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = AuxMLP()\n",
    "            model.to(device)\n",
    "            #model = BaseNet()\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=False, aux=True, alpha = 0.0)\n",
    "            te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies3[j,k] =  torch.cuda.FloatTensor(accurate).mean()\n",
    "print(test_accuracies3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies5 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = BaseNet()\n",
    "            model.to(device)\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=False)\n",
    "            te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies5[j,k] =  torch.cuda.FloatTensor(accurate).mean()\n",
    "print(test_accuracies5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8511, 0.8231, 0.8442, 0.8428, 0.8415],\n",
      "        [0.8533, 0.8563, 0.8511, 0.8492, 0.8524],\n",
      "        [0.8526, 0.8517, 0.8529, 0.8496, 0.8533],\n",
      "        [0.8486, 0.8502, 0.8439, 0.8259, 0.7883]])\n"
     ]
    }
   ],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies6 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = SiameseCNN()\n",
    "            model.to(device)\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=True)\n",
    "            te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies6[j,k] =  torch.cuda.FloatTensor(accurate).mean()\n",
    "print(test_accuracies6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies7 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = AuxBaseNet()\n",
    "            model.to(device)\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=False, aux=True, alpha = 0.0)\n",
    "            te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies7[j,k] =  torch.cuda.FloatTensor(accurate).mean()\n",
    "print(test_accuracies7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8227, 0.8301, 0.8310, 0.8322, 0.8278],\n",
      "        [0.8387, 0.8398, 0.8384, 0.8336, 0.8317],\n",
      "        [0.8332, 0.8305, 0.8267, 0.8279, 0.8229],\n",
      "        [0.8151, 0.8161, 0.8175, 0.8149, 0.8153]])\n"
     ]
    }
   ],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies9 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = ResNet(nb_residual_blocks = 4, input_channels = 2, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "            model.to(device)\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=False)\n",
    "            te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies9[j,k] =  torch.cuda.FloatTensor(accurate).mean()\n",
    "print(test_accuracies9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-cba3bf4e69c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_sizes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgammas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msiamese\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0mte_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcompute_nb_errors_siamese\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0maccurate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mte_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-643deba1e55f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, eta, decay, n_epochs, verbose, siamese, aux, alpha)\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[1;31m# Backward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m             \u001b[0mtotal_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_64\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_64\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies10 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = SiameseResNet(nb_residual_blocks = 4, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "            model.to(device)\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            loss = train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=True)\n",
    "            te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies10[j,k] =  torch.FloatTensor(accurate).mean()\n",
    "print(test_accuracies10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8156, 0.8483, 0.8466, 0.8430, 0.8749],\n",
       "        [0.8823, 0.8833, 0.8772, 0.8711, 0.8317],\n",
       "        [0.8332, 0.8305, 0.8267, 0.8279, 0.8229],\n",
       "        [0.8151, 0.8161, 0.8175, 0.8149, 0.8153]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracies10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies11 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = AuxResNet(nb_residual_blocks = 4, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "            model.to(device)\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            loss = train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=False, aux=True, alpha = 0.0)\n",
    "            te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies11[j,k] =  torch.FloatTensor(accurate).mean()\n",
    "print(test_accuracies11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize alpha for auxiliary loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(11):\n",
    "    accuracies100 = []\n",
    "\n",
    "    for i in range(10):\n",
    "        train_loader, test_loader = load_data(N=1000, batch_size=16, seed=i)\n",
    "        \n",
    "        model = AuxMLP()\n",
    "        model.to(device)\n",
    "        loss = train(model, train_loader, 5e-3, 0, 25, verbose=False, siamese=False, aux=True, alpha = j/10)\n",
    "\n",
    "        te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "        accuracies100.append(te_accuracy)\n",
    "    print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies100).mean(), torch.tensor(accuracies100).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(11):\n",
    "    accuracies101 = []\n",
    "\n",
    "    for i in range(10):\n",
    "        train_loader, test_loader = load_data(N=1000, batch_size=8, seed=i)\n",
    "\n",
    "        model = AuxsiameseMLP()\n",
    "        model.to(device)\n",
    "        loss = train(model, train_loader, 5e-3, 0, 25, verbose=False, siamese=True, aux=True, alpha = j/10)\n",
    "\n",
    "        te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "        accuracies101.append(te_accuracy)\n",
    "    print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies101).mean(), torch.tensor(accuracies101).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(11):\n",
    "    accuracies102 = []\n",
    "\n",
    "    for i in range(10):\n",
    "        train_loader, test_loader = load_data(N=1000, batch_size=32, seed=i)\n",
    "\n",
    "        model = AuxsiameseBaseNet()\n",
    "        model.to(device)\n",
    "        loss = train(model, train_loader, 5e-3, 0, 25, verbose=False, siamese=True, aux=True, alpha = j/10)\n",
    "\n",
    "        te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "        accuracies102.append(te_accuracy)\n",
    "    print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies102).mean(), torch.tensor(accuracies102).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(11):\n",
    "    accuracies103 = []\n",
    "\n",
    "    for i in range(10):\n",
    "        train_loader, test_loader = load_data(N=1000, batch_size=8, seed=i)\n",
    "\n",
    "        model = AuxBaseNet()\n",
    "        model.to(device)\n",
    "        train(model, train_loader, 5e-4, 0, 25, verbose=False, siamese=False, aux=True, alpha = j/10)\n",
    "\n",
    "        te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "        accuracies103.append(te_accuracy)\n",
    "    print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies103).mean(), torch.tensor(accuracies103).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(11):\n",
    "    accuracies104 = []\n",
    "\n",
    "    for i in range(10):\n",
    "        train_loader, test_loader = load_data(N=1000, batch_size=16, seed=i)\n",
    "\n",
    "        model = AuxsiameseResNet(nb_residual_blocks = 4, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "        model.to(device)\n",
    "        train(model, train_loader, 1e-3, 0, 25, verbose=False, siamese=True, aux=True, alpha = j/10)\n",
    "\n",
    "        te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "        accuracies104.append(te_accuracy)\n",
    "    print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies104).mean(), torch.tensor(accuracies104).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(11):\n",
    "    accuracies105 = []\n",
    "\n",
    "    for i in range(10):\n",
    "        train_loader, test_loader = load_data(N=1000, batch_size=64, seed=i)\n",
    "\n",
    "        model = AuxResNet(nb_residual_blocks = 4, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "        model.to(device)\n",
    "        train(model, train_loader, 1e-3, 0, 25, verbose=False, siamese=False, aux=True, alpha = j/10)\n",
    "\n",
    "        te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "        accuracies105.append(te_accuracy)\n",
    "    print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies105).mean(), torch.tensor(accuracies105).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.2021, -0.2152,  0.0717],\n",
       "          [-0.0338,  0.1757,  0.1063],\n",
       "          [-0.2043, -0.0632, -0.1443]],\n",
       "\n",
       "         [[ 0.1711, -0.2197, -0.0794],\n",
       "          [-0.0367, -0.1827,  0.1783],\n",
       "          [-0.0457, -0.1880, -0.1476]]],\n",
       "\n",
       "\n",
       "        [[[-0.1065,  0.2347,  0.0664],\n",
       "          [ 0.0989,  0.2181, -0.2000],\n",
       "          [-0.2090,  0.1658, -0.0477]],\n",
       "\n",
       "         [[ 0.1767, -0.0043,  0.0595],\n",
       "          [ 0.0525,  0.2171,  0.1257],\n",
       "          [ 0.0687, -0.2002,  0.1470]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0411, -0.1230,  0.2138],\n",
       "          [ 0.0970,  0.0338,  0.1252],\n",
       "          [ 0.0936, -0.1127, -0.1848]],\n",
       "\n",
       "         [[ 0.1284, -0.2110,  0.1211],\n",
       "          [-0.1395,  0.1456, -0.2234],\n",
       "          [-0.1436, -0.2100,  0.0385]]]], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> import torch\n",
    ">>> from torch import nn\n",
    ">>> layer = nn.Conv2d(2, 3, kernel_size=3, stride=1, padding=0)\n",
    ">>> layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
