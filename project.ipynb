{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import data_loader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import dlc_practical_prologue as prologue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def load_data(N=1000, batch_size=50, seed=42):\n",
    "    # Load data\n",
    "    train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(N)\n",
    "    train_input = train_input.to(device)\n",
    "    train_target = train_target.to(device)\n",
    "    train_classes = train_classes.to(device)\n",
    "    test_input = test_input.to(device)\n",
    "    test_target = test_target.to(device)\n",
    "    test_classes = test_classes.to(device)\n",
    "    # Normalize data\n",
    "    mean, std = train_input.mean(), train_input.std()\n",
    "    train_input.sub_(mean).div_(std)\n",
    "    test_input.sub_(mean).div_(std)\n",
    "    \n",
    "    # Generate dataset\n",
    "    train_data = TensorDataset(train_input, train_target, train_classes)\n",
    "    test_data = TensorDataset(test_input, test_target, test_classes)\n",
    "    \n",
    "    # For reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Generate data loader\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper.py\n",
    "\n",
    "# Count the number of parameters\n",
    "def count_param(model):\n",
    "    return sum([torch.numel(param) for param in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = load_data(N=1000, batch_size=50, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, data_loader):\n",
    "\n",
    "    nb_data_errors = 0\n",
    "\n",
    "    for data_input, data_target, data_classes in data_loader:\n",
    "        data_target = torch.nn.functional.one_hot(data_target)\n",
    "        output = model(data_input)\n",
    "        nb_error = torch.sum(torch.argmax(output, dim=1, keepdim=True) != torch.argmax(data_target, dim=1, keepdim=True))\n",
    "        nb_data_errors += nb_error\n",
    "        \n",
    "    return nb_data_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors_siamese(model, data_loader):\n",
    "\n",
    "    nb_data_errors = 0\n",
    "    for data_input, data_target, data_classes in data_loader:\n",
    "        data_1, data_2 = data_input.unbind(1)               \n",
    "        output = model(data_1.unsqueeze(1), data_2.unsqueeze(1))\n",
    "        data_target = torch.nn.functional.one_hot(data_target)\n",
    "        nb_error = torch.sum(torch.argmax(output, dim=1, keepdim=True) != torch.argmax(data_target, dim=1, keepdim=True))\n",
    "        nb_data_errors += nb_error\n",
    "        \n",
    "    return nb_data_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors_auxsiamese(model, data_loader):\n",
    "\n",
    "    nb_data_errors = 0\n",
    "    for data_input, data_target, data_classes in data_loader:\n",
    "        data_1, data_2 = data_input.unbind(1)               \n",
    "        output, aux1, aux2 = model(data_1.unsqueeze(1), data_2.unsqueeze(1))\n",
    "        data_target = torch.nn.functional.one_hot(data_target)\n",
    "        nb_error = torch.sum(torch.argmax(output, dim=1, keepdim=True) != torch.argmax(data_target, dim=1, keepdim=True))\n",
    "        nb_data_errors += nb_error\n",
    "        \n",
    "    return nb_data_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 32, kernel_size=3)    # size [nb, 32, 12, 12]\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)   # size [nb, 64, 4, 4]\n",
    "        self.fc1 = nn.Linear(256, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "        self.fc3 = nn.Linear(10, 2)\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2)) # size [nb, 32, 6, 6]      \n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2)) # size [nb, 64, 2, 2]\n",
    "        x = x.view(-1, 256) # size [nb, 256]\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseBaseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseBaseNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)    # size [nb, 32, 10, 10]\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)   # size [nb, 64, 4, 4]\n",
    "        self.fc1 = nn.Linear(256, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "        self.fc3 = nn.Linear(20, 2)\n",
    "        \n",
    "    def convs(self, x):        \n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2)) # size [nb, 32, 5, 5]      \n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2)) # size [nb, 64, 2, 2]\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.convs(x1)\n",
    "        x1 = x1.view(-1, 256)\n",
    "        x1 = F.relu((self.fc1(x1)))\n",
    "        x1 = F.relu(self.fc2(x1))\n",
    "        \n",
    "        x2 = self.convs(x2)\n",
    "        x2 = x2.view(-1, 256)\n",
    "        x2 = F.relu(self.fc1(x2))\n",
    "        x2 = F.relu(self.fc2(x2))\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        #x = torch.abs(x1 - x2)\n",
    "        #x = F.relu(self.fc1(x.flatten(start_dim=1)))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        \n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxsiameseBaseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AuxsiameseBaseNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)    # size [nb, 32, 10, 10]\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)   # size [nb, 64, 4, 4]\n",
    "        self.fc1 = nn.Linear(256, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "        self.fc3 = nn.Linear(20, 2)\n",
    "        \n",
    "    def convs(self, x):        \n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2)) # size [nb, 32, 5, 5]      \n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2)) # size [nb, 64, 2, 2]\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.convs(x1)\n",
    "        x1 = x1.view(-1, 256)\n",
    "        x1 = F.relu((self.fc1(x1)))\n",
    "        x1 = F.relu(self.fc2(x1))\n",
    "        \n",
    "        x2 = self.convs(x2)\n",
    "        x2 = x2.view(-1, 256)\n",
    "        x2 = F.relu(self.fc1(x2))\n",
    "        x2 = F.relu(self.fc2(x2))\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        aux1 = F.softmax(x1)\n",
    "        aux2 = F.softmax(x2)\n",
    "        #x = torch.abs(x1 - x2)\n",
    "        #x = F.relu(self.fc1(x.flatten(start_dim=1)))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        \n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        \n",
    "        return x, aux1, aux2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxBaseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AuxBaseNet, self).__init__()\n",
    "        self.conv11 = nn.Conv2d(1, 32, kernel_size=3)    # size [nb, 32, 10, 10]\n",
    "        self.conv21 = nn.Conv2d(32, 64, kernel_size=3)   # size [nb, 64, 4, 4]\n",
    "        self.fc11 = nn.Linear(256, 200)\n",
    "        self.fc21 = nn.Linear(200, 10)\n",
    "        self.conv12 = nn.Conv2d(1, 32, kernel_size=5)    # size [nb, 32, 10, 10]\n",
    "        self.conv22 = nn.Conv2d(32, 64, kernel_size=2)   # size [nb, 64, 4, 4]\n",
    "        self.fc12 = nn.Linear(256, 200)\n",
    "        self.fc22 = nn.Linear(200, 10)\n",
    "        self.fc3 = nn.Linear(20, 2)\n",
    "        \n",
    "    def convs(self, x):        \n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2)) # size [nb, 32, 5, 5]      \n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2)) # size [nb, 64, 2, 2]\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = F.relu(F.max_pool2d(self.conv11(x1), kernel_size=2)) # size [nb, 32, 5, 5]  \n",
    "        x1 = F.relu(F.max_pool2d(self.conv21(x1), kernel_size=2)) # size [nb, 64, 2, 2]\n",
    "        x1 = x1.view(-1, 256)\n",
    "        x1 = F.relu((self.fc11(x1)))\n",
    "        x1 = F.relu(self.fc21(x1))\n",
    "        \n",
    "        x2 = F.relu(F.max_pool2d(self.conv12(x2), kernel_size=2)) # size [nb, 32, 5, 5]  \n",
    "        x2 = F.relu(F.max_pool2d(self.conv22(x2), kernel_size=2)) # size [nb, 64, 2, 2]\n",
    "        x2 = x2.view(-1, 256)\n",
    "        x2 = F.relu((self.fc12(x2)))\n",
    "        x2 = F.relu(self.fc22(x2))\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        aux1 = F.softmax(x1)\n",
    "        aux2 = F.softmax(x2)\n",
    "        #x = torch.abs(x1 - x2)\n",
    "        #x = F.relu(self.fc1(x.flatten(start_dim=1)))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        \n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        \n",
    "        return x, aux1, aux2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNetBlock with skip-connection and batch normalization\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, nb_channels, kernel_size, dropout = 0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(nb_channels, nb_channels,\n",
    "                               kernel_size = kernel_size,\n",
    "                               padding = (kernel_size - 1) // 2)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(nb_channels)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(nb_channels, nb_channels,\n",
    "                               kernel_size = kernel_size,\n",
    "                               padding = (kernel_size - 1) // 2)\n",
    "\n",
    "        self.bn2 = nn.BatchNorm2d(nb_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        y = self.bn1(y)\n",
    "        y = self.dropout(y)\n",
    "        y = F.relu(y)\n",
    "        y = self.conv2(y)\n",
    "        y = self.bn2(y)\n",
    "        y = self.dropout(y)\n",
    "        y = y + x\n",
    "        y = F.relu(y)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, nb_residual_blocks, input_channels, nb_channels,\n",
    "                 kernel_size = 3, nb_classes = 10, dropout = 0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(input_channels, nb_channels,\n",
    "                              kernel_size = kernel_size,\n",
    "                              padding = (kernel_size - 1) // 2)\n",
    "        self.bn = nn.BatchNorm2d(nb_channels)\n",
    "\n",
    "        self.resnet_blocks = nn.Sequential(\n",
    "            *(ResNetBlock(nb_channels, kernel_size, dropout)\n",
    "              for _ in range(nb_residual_blocks))\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(288, nb_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn(self.conv(x)))\n",
    "        #x = F.relu(self.dropout(self.bn(self.conv(x))))\n",
    "        x = self.resnet_blocks(x)\n",
    "        x = F.avg_pool2d(x, 4).view(x.size(0), -1)\n",
    "        x = torch.sigmoid(self.fc(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, nb_residual_blocks, input_channels, nb_channels,\n",
    "                 kernel_size = 3, nb_classes = 10, dropout = 0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(input_channels, nb_channels,\n",
    "                              kernel_size = kernel_size,\n",
    "                              padding = (kernel_size - 1) // 2)\n",
    "        self.bn = nn.BatchNorm2d(nb_channels)\n",
    "\n",
    "        self.resnet_blocks = nn.Sequential(\n",
    "            *(ResNetBlock(nb_channels, kernel_size, dropout)\n",
    "              for _ in range(nb_residual_blocks))\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(576, nb_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = F.relu(self.bn(self.conv(x1)))\n",
    "        #x = F.relu(self.dropout(self.bn(self.conv(x))))\n",
    "        x1 = self.resnet_blocks(x1)\n",
    "        x1 = F.avg_pool2d(x1, 4).view(x1.size(0), -1)\n",
    "        \n",
    "        x2 = F.relu(self.bn(self.conv(x2)))\n",
    "        #x = F.relu(self.dropout(self.bn(self.conv(x))))\n",
    "        x2 = self.resnet_blocks(x2)\n",
    "        x2 = F.avg_pool2d(x2, 4).view(x2.size(0), -1)\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        #aux1 = F.softmax(self.fc1(x1)\n",
    "        #aux2 = F.softmax(self.fc1(x2)\n",
    "        #x = torch.abs(x1 - x2)\n",
    "        #x = F.relu(self.fc1(x.flatten(start_dim=1)))\n",
    "        #x = F.relu(self.fc2(x))      \n",
    "        x = torch.sigmoid(self.fc(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, nb_residual_blocks, input_channels, nb_channels,\n",
    "                 kernel_size = 3, nb_classes = 10, dropout = 0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(input_channels, nb_channels,\n",
    "                              kernel_size = kernel_size,\n",
    "                              padding = (kernel_size - 1) // 2)\n",
    "        self.conv2 = nn.Conv2d(input_channels, nb_channels,\n",
    "                              kernel_size = kernel_size,\n",
    "                              padding = (kernel_size - 1) // 2)\n",
    "        self.bn1 = nn.BatchNorm2d(nb_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(nb_channels)\n",
    "\n",
    "        self.resnet_blocks1 = nn.Sequential(\n",
    "            *(ResNetBlock(nb_channels, kernel_size, dropout)\n",
    "              for _ in range(nb_residual_blocks))\n",
    "        )\n",
    "        self.resnet_blocks2 = nn.Sequential(\n",
    "            *(ResNetBlock(nb_channels, kernel_size, dropout)\n",
    "              for _ in range(nb_residual_blocks))\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(576, nb_classes)\n",
    "        self.fc1 = nn.Linear(288, 10)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = F.relu(self.bn1(self.conv1(x1)))\n",
    "        #x = F.relu(self.dropout(self.bn(self.conv(x))))\n",
    "        x1 = self.resnet_blocks1(x1)\n",
    "        x1 = F.avg_pool2d(x1, 4).view(x1.size(0), -1)\n",
    "        \n",
    "        x2 = F.relu(self.bn2(self.conv2(x2)))\n",
    "        #x = F.relu(self.dropout(self.bn(self.conv(x))))\n",
    "        x2 = self.resnet_blocks2(x2)\n",
    "        x2 = F.avg_pool2d(x2, 4).view(x2.size(0), -1)\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        x = torch.sigmoid(self.fc(x))\n",
    "        aux1 = F.softmax(self.fc1(x1))\n",
    "        aux2 = F.softmax(self.fc1(x2))\n",
    "        #aux1 = F.softmax(x1)\n",
    "        #aux2 = F.softmax(x2)\n",
    "        #x = torch.abs(x1 - x2)\n",
    "        #x = F.relu(self.fc1(x.flatten(start_dim=1)))\n",
    "        #x = F.relu(self.fc2(x))      \n",
    "        \n",
    "        return x, aux1, aux2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxsiameseResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, nb_residual_blocks, input_channels, nb_channels,\n",
    "                 kernel_size = 3, nb_classes = 10, dropout = 0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(input_channels, nb_channels,\n",
    "                              kernel_size = kernel_size,\n",
    "                              padding = (kernel_size - 1) // 2)\n",
    "        self.bn = nn.BatchNorm2d(nb_channels)\n",
    "\n",
    "        self.resnet_blocks = nn.Sequential(\n",
    "            *(ResNetBlock(nb_channels, kernel_size, dropout)\n",
    "              for _ in range(nb_residual_blocks))\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(576, nb_classes)\n",
    "        self.fc1 = nn.Linear(288, 10)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = F.relu(self.bn(self.conv(x1)))\n",
    "        #x = F.relu(self.dropout(self.bn(self.conv(x))))\n",
    "        x1 = self.resnet_blocks(x1)\n",
    "        x1 = F.avg_pool2d(x1, 4).view(x1.size(0), -1)\n",
    "        \n",
    "        x2 = F.relu(self.bn(self.conv(x2)))\n",
    "        #x = F.relu(self.dropout(self.bn(self.conv(x))))\n",
    "        x2 = self.resnet_blocks(x2)\n",
    "        x2 = F.avg_pool2d(x2, 4).view(x2.size(0), -1)\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        x = torch.sigmoid(self.fc(x))\n",
    "        aux1 = F.softmax(self.fc1(x1))\n",
    "        aux2 = F.softmax(self.fc1(x2))\n",
    "        #aux1 = F.softmax(x1)\n",
    "        #aux2 = F.softmax(x2)\n",
    "        #x = torch.abs(x1 - x2)\n",
    "        #x = F.relu(self.fc1(x.flatten(start_dim=1)))\n",
    "        #x = F.relu(self.fc2(x))      \n",
    "        \n",
    "        return x, aux1, aux2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190668"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AuxsiameseResNet(nb_residual_blocks = 10, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "\n",
    "# Calculate the number of parameters in the model\n",
    "count_param(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187778"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SiameseResNet(nb_residual_blocks = 10, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "\n",
    "# Calculate the number of parameters in the model\n",
    "count_param(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "377292"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AuxResNet(nb_residual_blocks = 10, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "\n",
    "# Calculate the number of parameters in the model\n",
    "count_param(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187490"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet(nb_residual_blocks = 10, input_channels = 2, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "# Calculate the number of parameters in the model\n",
    "count_param(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72536"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BaseNet()\n",
    "\n",
    "# Calculate the number of parameters in the model\n",
    "count_param(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72268"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SiameseBaseNet()\n",
    "\n",
    "# Calculate the number of parameters in the model\n",
    "count_param(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72268"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AuxsiameseBaseNet()\n",
    "\n",
    "# Calculate the number of parameters in the model\n",
    "count_param(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134766"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AuxBaseNet()\n",
    "\n",
    "# Calculate the number of parameters in the model\n",
    "count_param(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, eta, decay, n_epochs=25, verbose=False, siamese=False, aux=False, alpha = 0):\n",
    "\n",
    "    #binary_crit = nn.CrossEntropyLoss()\n",
    "    binary_crit = torch.nn.BCELoss()\n",
    "    aux_crit = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=eta, weight_decay=decay)\n",
    "    \n",
    "    tr_losses = []\n",
    "    tr_accuracies = []\n",
    "\n",
    "    for e in range(n_epochs):\n",
    "        # Reset training/validation loss\n",
    "        tr_loss = 0\n",
    "\n",
    "        # Training model\n",
    "        model.train()\n",
    "\n",
    "        for train_input, train_target, train_classes in iter(train_loader):\n",
    "            train_target = torch.nn.functional.one_hot(train_target)\n",
    "            # Forward pass\n",
    "            \n",
    "            if siamese == True:\n",
    "                train_1, train_2 = train_input.unbind(1)\n",
    "                if aux == True:\n",
    "                    output, aux1, aux2 = model(train_1.unsqueeze(1), train_2.unsqueeze(1))\n",
    "                else:\n",
    "                    output = model(train_1.unsqueeze(1), train_2.unsqueeze(1))\n",
    "            elif aux == True:\n",
    "                train_1, train_2 = train_input.unbind(1)\n",
    "                output, aux1, aux2 = model(train_1.unsqueeze(1), train_2.unsqueeze(1))\n",
    "            else:\n",
    "                output = model(train_input)\n",
    "                \n",
    "            # Binary classification loss\n",
    "            binary_loss = binary_crit(output, train_target.float())\n",
    "            total_loss = binary_loss\n",
    "            \n",
    "            # Auxiliary loss\n",
    "            if aux == True:\n",
    "\n",
    "                aux_loss1 = aux_crit(aux1, train_classes[:,0])\n",
    "                aux_loss2 = aux_crit(aux2, train_classes[:,1])\n",
    "                aux_loss = aux_loss1 + aux_loss2\n",
    "                total_loss = binary_loss + aux_loss * alpha\n",
    "        \n",
    "            # Total loss = Binary loss + aux loss * alpha\n",
    "            \n",
    "            tr_loss += total_loss\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Collect accuracy data\n",
    "        # tr_accuracies.append(compute_nb_errors_siamese(model, train_loader)/1000)\n",
    "\n",
    "        # Collect loss data\n",
    "        tr_losses.append(tr_loss)\n",
    "\n",
    "        if verbose:\n",
    "            print('Epoch %d/%d, Binary loss: %.3f' %\n",
    "                  (e+1, n_epochs, tr_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Binary loss: 22.082\n",
      "Epoch 2/25, Binary loss: 16.807\n",
      "Epoch 3/25, Binary loss: 14.288\n",
      "Epoch 4/25, Binary loss: 12.621\n",
      "Epoch 5/25, Binary loss: 11.836\n",
      "Epoch 6/25, Binary loss: 10.218\n",
      "Epoch 7/25, Binary loss: 9.515\n",
      "Epoch 8/25, Binary loss: 8.744\n",
      "Epoch 9/25, Binary loss: 8.608\n",
      "Epoch 10/25, Binary loss: 8.244\n",
      "Epoch 11/25, Binary loss: 8.606\n",
      "Epoch 12/25, Binary loss: 8.923\n",
      "Epoch 13/25, Binary loss: 8.164\n",
      "Epoch 14/25, Binary loss: 7.558\n",
      "Epoch 15/25, Binary loss: 7.285\n",
      "Epoch 16/25, Binary loss: 7.127\n",
      "Epoch 17/25, Binary loss: 7.014\n",
      "Epoch 18/25, Binary loss: 6.707\n",
      "Epoch 19/25, Binary loss: 6.664\n",
      "Epoch 20/25, Binary loss: 6.642\n",
      "Epoch 21/25, Binary loss: 6.631\n",
      "Epoch 22/25, Binary loss: 6.623\n",
      "Epoch 23/25, Binary loss: 6.615\n",
      "Epoch 24/25, Binary loss: 6.610\n",
      "Epoch 25/25, Binary loss: 6.608\n",
      "Spend 1.216382e+02 s\n",
      "tensor(1., device='cuda:0') tensor(0.8770, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "number = torch.randint(1,50,(1,))\n",
    "train_loader, test_loader = load_data(N=1000, batch_size=50, seed=number)\n",
    "time1 = time.perf_counter()\n",
    "model = AuxsiameseResNet(nb_residual_blocks = 10, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "model.to(device)\n",
    "#model = BaseNet()\n",
    "train(model, train_loader, 0.001, 0, 25, verbose=True, siamese=True, aux=True, alpha = 0.1)\n",
    "time2 = time.perf_counter()\n",
    "print('Spend {:e} s'.format(time2 - time1))\n",
    "\n",
    "tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "print(tr_accuracy, te_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Binary loss: 40.103\n",
      "Epoch 2/25, Binary loss: 31.625\n",
      "Epoch 3/25, Binary loss: 28.229\n",
      "Epoch 4/25, Binary loss: 26.540\n",
      "Epoch 5/25, Binary loss: 24.686\n",
      "Epoch 6/25, Binary loss: 23.148\n",
      "Epoch 7/25, Binary loss: 22.162\n",
      "Epoch 8/25, Binary loss: 21.673\n",
      "Epoch 9/25, Binary loss: 20.688\n",
      "Epoch 10/25, Binary loss: 19.981\n",
      "Epoch 11/25, Binary loss: 19.738\n",
      "Epoch 12/25, Binary loss: 19.896\n",
      "Epoch 13/25, Binary loss: 19.436\n",
      "Epoch 14/25, Binary loss: 19.325\n",
      "Epoch 15/25, Binary loss: 19.050\n",
      "Epoch 16/25, Binary loss: 18.952\n",
      "Epoch 17/25, Binary loss: 18.856\n",
      "Epoch 18/25, Binary loss: 18.774\n",
      "Epoch 19/25, Binary loss: 18.752\n",
      "Epoch 20/25, Binary loss: 17.955\n",
      "Epoch 21/25, Binary loss: 17.716\n",
      "Epoch 22/25, Binary loss: 17.694\n",
      "Epoch 23/25, Binary loss: 17.623\n",
      "Epoch 24/25, Binary loss: 17.593\n",
      "Epoch 25/25, Binary loss: 17.584\n",
      "Spend 1.226981e+02 s\n",
      "tensor(1., device='cuda:0') tensor(0.8610, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "number = torch.randint(1,50,(1,))\n",
    "train_loader, test_loader = load_data(N=1000, batch_size=50, seed=number)\n",
    "time1 = time.perf_counter()\n",
    "#model = SiameseBaseNet()\n",
    "#model = BaseNet()\n",
    "model = AuxsiameseResNet(nb_residual_blocks = 10, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "model.to(device\n",
    "train(model, train_loader, 0.001, 0, 25, verbose=True, siamese=False, aux=True, alpha = 0.3)\n",
    "time2 = time.perf_counter()\n",
    "print('Spend {:e} s'.format(time2 - time1))\n",
    "\n",
    "tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "print(tr_accuracy, te_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1081,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Binary loss: 11.641\n",
      "Epoch 2/25, Binary loss: 8.032\n",
      "Epoch 3/25, Binary loss: 5.910\n",
      "Epoch 4/25, Binary loss: 5.168\n",
      "Epoch 5/25, Binary loss: 4.474\n",
      "Epoch 6/25, Binary loss: 3.215\n",
      "Epoch 7/25, Binary loss: 3.069\n",
      "Epoch 8/25, Binary loss: 3.053\n",
      "Epoch 9/25, Binary loss: 2.638\n",
      "Epoch 10/25, Binary loss: 2.044\n",
      "Epoch 11/25, Binary loss: 2.564\n",
      "Epoch 12/25, Binary loss: 1.829\n",
      "Epoch 13/25, Binary loss: 1.344\n",
      "Epoch 14/25, Binary loss: 0.564\n",
      "Epoch 15/25, Binary loss: 0.404\n",
      "Epoch 16/25, Binary loss: 0.158\n",
      "Epoch 17/25, Binary loss: 0.359\n",
      "Epoch 18/25, Binary loss: 0.458\n",
      "Epoch 19/25, Binary loss: 0.738\n",
      "Epoch 20/25, Binary loss: 0.517\n",
      "Epoch 21/25, Binary loss: 0.761\n",
      "Epoch 22/25, Binary loss: 0.393\n",
      "Epoch 23/25, Binary loss: 0.200\n",
      "Epoch 24/25, Binary loss: 0.130\n",
      "Epoch 25/25, Binary loss: 0.067\n",
      "Spend 4.333027e+02 s\n",
      "tensor(0.9990) tensor(0.8690)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "number = torch.randint(1,50,(1,))\n",
    "train_loader, test_loader = load_data(N=1000, batch_size=50, seed=number)\n",
    "time1 = time.perf_counter()\n",
    "model = SiameseResNet(nb_residual_blocks = 10, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "#model = BaseNet()\n",
    "train(model, train_loader, 0.001, 0, 25, verbose=True, siamese=True)\n",
    "time2 = time.perf_counter()\n",
    "print('Spend {:e} s'.format(time2 - time1))\n",
    "\n",
    "tr_accuracy = 1 - compute_nb_errors_siamese(model, train_loader)/1000\n",
    "te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "print(tr_accuracy, te_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1055,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Binary loss: 12.579\n",
      "Epoch 2/25, Binary loss: 10.303\n",
      "Epoch 3/25, Binary loss: 9.214\n",
      "Epoch 4/25, Binary loss: 7.878\n",
      "Epoch 5/25, Binary loss: 7.113\n",
      "Epoch 6/25, Binary loss: 6.340\n",
      "Epoch 7/25, Binary loss: 5.647\n",
      "Epoch 8/25, Binary loss: 5.246\n",
      "Epoch 9/25, Binary loss: 4.497\n",
      "Epoch 10/25, Binary loss: 4.413\n",
      "Epoch 11/25, Binary loss: 4.452\n",
      "Epoch 12/25, Binary loss: 3.421\n",
      "Epoch 13/25, Binary loss: 2.968\n",
      "Epoch 14/25, Binary loss: 3.099\n",
      "Epoch 15/25, Binary loss: 2.219\n",
      "Epoch 16/25, Binary loss: 1.569\n",
      "Epoch 17/25, Binary loss: 1.443\n",
      "Epoch 18/25, Binary loss: 1.220\n",
      "Epoch 19/25, Binary loss: 0.891\n",
      "Epoch 20/25, Binary loss: 1.047\n",
      "Epoch 21/25, Binary loss: 0.979\n",
      "Epoch 22/25, Binary loss: 0.917\n",
      "Epoch 23/25, Binary loss: 1.077\n",
      "Epoch 24/25, Binary loss: 0.904\n",
      "Epoch 25/25, Binary loss: 0.614\n",
      "Spend 6.177664e+01 s\n",
      "tensor(0.9980) tensor(0.8360)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "number = torch.randint(1,50,(1,))\n",
    "train_loader, test_loader = load_data(N=1000, batch_size=50, seed=number)\n",
    "time1 = time.perf_counter()\n",
    "#model = SiameseBaseNet()\n",
    "model = ResNet(nb_residual_blocks = 3, input_channels = 2, nb_channels = 32, kernel_size = 3, nb_classes = 2, dropout = 0.1)\n",
    "train(model, train_loader, 0.001, 0, 25, verbose=True, siamese=False)\n",
    "time2 = time.perf_counter()\n",
    "print('Spend {:e} s'.format(time2 - time1))\n",
    "\n",
    "tr_accuracy = 1 - compute_nb_errors(model, train_loader)/1000\n",
    "te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "print(tr_accuracy, te_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Binary loss: 2.743\n",
      "Epoch 2/25, Binary loss: 2.704\n",
      "Epoch 3/25, Binary loss: 2.636\n",
      "Epoch 4/25, Binary loss: 2.529\n",
      "Epoch 5/25, Binary loss: 2.372\n",
      "Epoch 6/25, Binary loss: 2.187\n",
      "Epoch 7/25, Binary loss: 1.991\n",
      "Epoch 8/25, Binary loss: 1.832\n",
      "Epoch 9/25, Binary loss: 1.687\n",
      "Epoch 10/25, Binary loss: 1.552\n",
      "Epoch 11/25, Binary loss: 1.421\n",
      "Epoch 12/25, Binary loss: 1.303\n",
      "Epoch 13/25, Binary loss: 1.189\n",
      "Epoch 14/25, Binary loss: 1.096\n",
      "Epoch 15/25, Binary loss: 1.001\n",
      "Epoch 16/25, Binary loss: 0.925\n",
      "Epoch 17/25, Binary loss: 0.836\n",
      "Epoch 18/25, Binary loss: 0.760\n",
      "Epoch 19/25, Binary loss: 0.696\n",
      "Epoch 20/25, Binary loss: 0.614\n",
      "Epoch 21/25, Binary loss: 0.548\n",
      "Epoch 22/25, Binary loss: 0.487\n",
      "Epoch 23/25, Binary loss: 0.422\n",
      "Epoch 24/25, Binary loss: 0.361\n",
      "Epoch 25/25, Binary loss: 0.307\n",
      "Spend 9.664215e+00 s\n",
      "tensor(0.9890) tensor(0.8580)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "number = torch.randint(1,50,(1,))\n",
    "train_loader, test_loader = load_data(N=1000, batch_size=50, seed=number)\n",
    "time1 = time.perf_counter()\n",
    "model = SiameseBaseNet()\n",
    "#model = BaseNet()\n",
    "train(model, train_loader, 0.001, 0, 25, verbose=True, siamese=True)\n",
    "time2 = time.perf_counter()\n",
    "print('Spend {:e} s'.format(time2 - time1))\n",
    "\n",
    "tr_accuracy = 1 - compute_nb_errors_siamese(model, train_loader)/1000\n",
    "te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "print(tr_accuracy, te_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Binary loss: 2.775\n",
      "Epoch 2/25, Binary loss: 2.737\n",
      "Epoch 3/25, Binary loss: 2.680\n",
      "Epoch 4/25, Binary loss: 2.574\n",
      "Epoch 5/25, Binary loss: 2.400\n",
      "Epoch 6/25, Binary loss: 2.175\n",
      "Epoch 7/25, Binary loss: 1.978\n",
      "Epoch 8/25, Binary loss: 1.849\n",
      "Epoch 9/25, Binary loss: 1.777\n",
      "Epoch 10/25, Binary loss: 1.617\n",
      "Epoch 11/25, Binary loss: 1.531\n",
      "Epoch 12/25, Binary loss: 1.432\n",
      "Epoch 13/25, Binary loss: 1.335\n",
      "Epoch 14/25, Binary loss: 1.232\n",
      "Epoch 15/25, Binary loss: 1.146\n",
      "Epoch 16/25, Binary loss: 1.058\n",
      "Epoch 17/25, Binary loss: 0.955\n",
      "Epoch 18/25, Binary loss: 0.865\n",
      "Epoch 19/25, Binary loss: 0.781\n",
      "Epoch 20/25, Binary loss: 0.770\n",
      "Epoch 21/25, Binary loss: 0.651\n",
      "Epoch 22/25, Binary loss: 0.549\n",
      "Epoch 23/25, Binary loss: 0.616\n",
      "Epoch 24/25, Binary loss: 0.616\n",
      "Epoch 25/25, Binary loss: 0.484\n",
      "Spend 4.811039e+00 s\n",
      "tensor(0.9780) tensor(0.8450)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "number = torch.randint(1,50,(1,))\n",
    "train_loader, test_loader = load_data(N=1000, batch_size=50, seed=number)\n",
    "time1 = time.perf_counter()\n",
    "#modeF.avg_pool2dl = SiameseBaseNet()\n",
    "model = BaseNet()\n",
    "train(model, train_loader, 0.001, 0, 25, verbose=True, siamese=False)\n",
    "time2 = time.perf_counter()\n",
    "print('Spend {:e} s'.format(time2 - time1))\n",
    "\n",
    "tr_accuracy = 1 - compute_nb_errors(model, train_loader)/1000\n",
    "te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "print(tr_accuracy, te_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-673-21efc383ef87>:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  aux1 = F.softmax(x1)\n",
      "<ipython-input-673-21efc383ef87>:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  aux2 = F.softmax(x2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Binary loss: 41.301\n",
      "Epoch 2/25, Binary loss: 40.527\n",
      "Epoch 3/25, Binary loss: 39.194\n",
      "Epoch 4/25, Binary loss: 37.589\n",
      "Epoch 5/25, Binary loss: 35.413\n",
      "Epoch 6/25, Binary loss: 33.461\n",
      "Epoch 7/25, Binary loss: 32.224\n",
      "Epoch 8/25, Binary loss: 31.236\n",
      "Epoch 9/25, Binary loss: 30.435\n",
      "Epoch 10/25, Binary loss: 29.611\n",
      "Epoch 11/25, Binary loss: 29.025\n",
      "Epoch 12/25, Binary loss: 28.527\n",
      "Epoch 13/25, Binary loss: 28.140\n",
      "Epoch 14/25, Binary loss: 27.689\n",
      "Epoch 15/25, Binary loss: 27.367\n",
      "Epoch 16/25, Binary loss: 27.219\n",
      "Epoch 17/25, Binary loss: 27.102\n",
      "Epoch 18/25, Binary loss: 26.959\n",
      "Epoch 19/25, Binary loss: 26.890\n",
      "Epoch 20/25, Binary loss: 26.829\n",
      "Epoch 21/25, Binary loss: 26.785\n",
      "Epoch 22/25, Binary loss: 26.759\n",
      "Epoch 23/25, Binary loss: 26.788\n",
      "Epoch 24/25, Binary loss: 26.739\n",
      "Epoch 25/25, Binary loss: 26.726\n",
      "Spend 1.036312e+01 s\n",
      "tensor(1.) tensor(0.8700)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "number = torch.randint(1,50,(1,))\n",
    "train_loader, test_loader = load_data(N=1000, batch_size=50, seed=number)\n",
    "time1 = time.perf_counter()\n",
    "#model = SiameseBaseNet()\n",
    "#model = BaseNet()\n",
    "model = AuxsiameseBaseNet()\n",
    "train(model, train_loader, 0.001, 0, 25, verbose=True, siamese=True, aux=True, alpha = 0.3)\n",
    "time2 = time.perf_counter()\n",
    "print('Spend {:e} s'.format(time2 - time1))\n",
    "\n",
    "tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "print(tr_accuracy, te_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-699-f890ddbc2f26>:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  aux1 = F.softmax(x1)\n",
      "<ipython-input-699-f890ddbc2f26>:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  aux2 = F.softmax(x2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Binary loss: 41.354\n",
      "Epoch 2/25, Binary loss: 40.233\n",
      "Epoch 3/25, Binary loss: 38.545\n",
      "Epoch 4/25, Binary loss: 36.119\n",
      "Epoch 5/25, Binary loss: 34.638\n",
      "Epoch 6/25, Binary loss: 32.981\n",
      "Epoch 7/25, Binary loss: 32.049\n",
      "Epoch 8/25, Binary loss: 31.040\n",
      "Epoch 9/25, Binary loss: 30.221\n",
      "Epoch 10/25, Binary loss: 29.890\n",
      "Epoch 11/25, Binary loss: 29.816\n",
      "Epoch 12/25, Binary loss: 28.891\n",
      "Epoch 13/25, Binary loss: 28.312\n",
      "Epoch 14/25, Binary loss: 27.903\n",
      "Epoch 15/25, Binary loss: 27.736\n",
      "Epoch 16/25, Binary loss: 27.181\n",
      "Epoch 17/25, Binary loss: 26.733\n",
      "Epoch 18/25, Binary loss: 26.162\n",
      "Epoch 19/25, Binary loss: 26.032\n",
      "Epoch 20/25, Binary loss: 25.755\n",
      "Epoch 21/25, Binary loss: 25.475\n",
      "Epoch 22/25, Binary loss: 25.369\n",
      "Epoch 23/25, Binary loss: 25.333\n",
      "Epoch 24/25, Binary loss: 25.143\n",
      "Epoch 25/25, Binary loss: 25.044\n",
      "Spend 9.934680e+00 s\n",
      "tensor(1.) tensor(0.8390)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "number = torch.randint(1,50,(1,))\n",
    "train_loader, test_loader = load_data(N=1000, batch_size=50, seed=number)\n",
    "time1 = time.perf_counter()\n",
    "#model = SiameseBaseNet()\n",
    "#model = BaseNet()\n",
    "model = AuxBaseNet()\n",
    "train(model, train_loader, 0.001, 0, 25, verbose=True, siamese=False, aux=True, alpha = 0.3)\n",
    "time2 = time.perf_counter()\n",
    "print('Spend {:e} s'.format(time2 - time1))\n",
    "\n",
    "tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "print(tr_accuracy, te_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0') tensor(0.8350, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8010, device='cuda:0')\n",
      "tensor(0.9830, device='cuda:0') tensor(0.8370, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8430, device='cuda:0')\n",
      "tensor(0.9810, device='cuda:0') tensor(0.8170, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8380, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8100, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8280, device='cuda:0')\n",
      "tensor(0.9990, device='cuda:0') tensor(0.8480, device='cuda:0')\n",
      "tensor(0.9990, device='cuda:0') tensor(0.8280, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8330, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8200, device='cuda:0')\n",
      "tensor(0.9990, device='cuda:0') tensor(0.8200, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8500, device='cuda:0')\n",
      "tensor(0.9990, device='cuda:0') tensor(0.8220, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.7860, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8260, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8300, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8310, device='cuda:0')\n",
      "tensor(0.9800, device='cuda:0') tensor(0.8070, device='cuda:0')\n",
      "Mean: 0.826, Std: 0.016\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "times = []\n",
    "\n",
    "for i in range(20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=100, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    #model = SiameseBaseNet()\n",
    "    model = BaseNet()\n",
    "    model.to(device)\n",
    "    train(model, train_loader, 0.001, 0, 25, verbose=False, siamese=False)\n",
    "    time2 = time.perf_counter()\n",
    "    times.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies.append(te_accuracy)\n",
    "\n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies).mean(), torch.tensor(accuracies).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 6.692, Std: 0.067\n"
     ]
    }
   ],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times).mean(), torch.tensor(times).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0') tensor(0.8480, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8780, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8390, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8690, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8710, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8500, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8570, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8400, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8580, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8500, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8560, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8370, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8460, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8580, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8500, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8550, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8470, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8610, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8490, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8590, device='cuda:0')\n",
      "Mean: 0.854, Std: 0.011\n"
     ]
    }
   ],
   "source": [
    "accuracies1 = []\n",
    "times1 = []\n",
    "\n",
    "for i in range(20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=50, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    model = SiameseBaseNet()\n",
    "    model.to(device)\n",
    "    #model = BaseNet()\n",
    "    train(model, train_loader, 0.001, 0, 25, verbose=False, siamese=True)\n",
    "    time2 = time.perf_counter()\n",
    "    times1.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_siamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies1.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies1).mean(), torch.tensor(accuracies1).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 16.312, Std: 0.113\n"
     ]
    }
   ],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times1).mean(), torch.tensor(times1).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9970, device='cuda:0') tensor(0.8480, device='cuda:0')\n",
      "tensor(0.9880, device='cuda:0') tensor(0.8390, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8480, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8260, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8500, device='cuda:0')\n",
      "tensor(0.9980, device='cuda:0') tensor(0.8460, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8230, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8460, device='cuda:0')\n",
      "tensor(0.9990, device='cuda:0') tensor(0.8570, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8210, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8500, device='cuda:0')\n",
      "tensor(0.9980, device='cuda:0') tensor(0.8550, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8470, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8350, device='cuda:0')\n",
      "tensor(0.9910, device='cuda:0') tensor(0.7990, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8320, device='cuda:0')\n",
      "tensor(0.9950, device='cuda:0') tensor(0.8370, device='cuda:0')\n",
      "tensor(0.9970, device='cuda:0') tensor(0.8420, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8660, device='cuda:0')\n",
      "tensor(0.9980, device='cuda:0') tensor(0.8100, device='cuda:0')\n",
      "Mean: 0.839, Std: 0.016\n"
     ]
    }
   ],
   "source": [
    "accuracies2 = []\n",
    "times2 = []\n",
    "\n",
    "for i in range(20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=50, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    #model = SiameseBaseNet()\n",
    "    #model = BaseNet()\n",
    "    model = AuxBaseNet()\n",
    "    model.to(device)\n",
    "    train(model, train_loader, 0.001, 0, 25, verbose=False, siamese=False, aux=True, alpha = 0.6)\n",
    "    time2 = time.perf_counter()\n",
    "    times2.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies2.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies2).mean(), torch.tensor(accuracies2).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 22.799, Std: 0.173\n"
     ]
    }
   ],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times2).mean(), torch.tensor(times2).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0') tensor(0.8650, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8800, device='cuda:0')\n",
      "tensor(0.9950, device='cuda:0') tensor(0.8510, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8760, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8690, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8470, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8650, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8510, device='cuda:0')\n",
      "tensor(0.9970, device='cuda:0') tensor(0.8610, device='cuda:0')\n",
      "tensor(0.9930, device='cuda:0') tensor(0.8480, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8580, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8460, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8600, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8650, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8630, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8680, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8670, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8620, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8500, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8710, device='cuda:0')\n",
      "Mean: 0.861, Std: 0.010\n"
     ]
    }
   ],
   "source": [
    "accuracies3 = []\n",
    "times3 = []\n",
    "\n",
    "for i in range(20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=50, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    #model = SiameseBaseNet()\n",
    "    #model = BaseNet()\n",
    "    model = AuxsiameseBaseNet()\n",
    "    model.to(device)\n",
    "    train(model, train_loader, 0.001, 0, 25, verbose=False, siamese=True, aux=True, alpha = 0.5)\n",
    "    time2 = time.perf_counter()\n",
    "    times3.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies3.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies3).mean(), torch.tensor(accuracies3).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 18.461, Std: 0.171\n"
     ]
    }
   ],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times3).mean(), torch.tensor(times3).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0') tensor(0.8180, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8190, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8430, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8010, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8510, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8360, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8380, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8430, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8370, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8590, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8180, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8510, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8470, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8240, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8530, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8280, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8540, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8460, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8280, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8390, device='cuda:0')\n",
      "Mean: 0.837, Std: 0.015\n"
     ]
    }
   ],
   "source": [
    "accuracies4 = []\n",
    "times4 = []\n",
    "\n",
    "for i in range(20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=50, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    #model = SiameseBaseNet()\n",
    "    #model = BaseNet()\n",
    "    model = ResNet(nb_residual_blocks = 10, input_channels = 2, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "    model.to(device)\n",
    "    train(model, train_loader, 0.001, 0, 25, verbose=False, siamese=False)\n",
    "    time2 = time.perf_counter()\n",
    "    times4.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies4.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies4).mean(), torch.tensor(accuracies4).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 84.119, Std: 0.164\n"
     ]
    }
   ],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times4).mean(), torch.tensor(times4).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0') tensor(0.8670, device='cuda:0')\n",
      "tensor(0.9970, device='cuda:0') tensor(0.8650, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8830, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8730, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8820, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8740, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8700, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8720, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8770, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8610, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8680, device='cuda:0')\n",
      "tensor(0.9880, device='cuda:0') tensor(0.8370, device='cuda:0')\n",
      "tensor(0.9950, device='cuda:0') tensor(0.8470, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8640, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8520, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8800, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8770, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8770, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8860, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8880, device='cuda:0')\n",
      "Mean: 0.870, Std: 0.013\n"
     ]
    }
   ],
   "source": [
    "accuracies5 = []\n",
    "times5 = []\n",
    "\n",
    "for i in range(20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=50, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    model = SiameseResNet(nb_residual_blocks = 10, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "    #model = BaseNet()\n",
    "    model.to(device)\n",
    "    train(model, train_loader, 0.001, 0, 25, verbose=False, siamese=True)\n",
    "    time2 = time.perf_counter()\n",
    "    times5.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_siamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies5.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies5).mean(), torch.tensor(accuracies5).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 118.672, Std: 0.185\n"
     ]
    }
   ],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times5).mean(), torch.tensor(times5).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9870, device='cuda:0') tensor(0.8550, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8690, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8760, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8690, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8790, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8800, device='cuda:0')\n",
      "tensor(0.9990, device='cuda:0') tensor(0.8590, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.9010, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8720, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8780, device='cuda:0')\n",
      "tensor(0.9950, device='cuda:0') tensor(0.8660, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8760, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8860, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8740, device='cuda:0')\n",
      "tensor(0.9990, device='cuda:0') tensor(0.8920, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8590, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8710, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8830, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8830, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8850, device='cuda:0')\n",
      "Mean: 0.876, Std: 0.011\n"
     ]
    }
   ],
   "source": [
    "accuracies6 = []\n",
    "times6 = []\n",
    "\n",
    "for i in range(20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=50, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    #model = SiameseBaseNet()\n",
    "    #model = BaseNet()\n",
    "    model = AuxResNet(nb_residual_blocks = 10, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "    model.to(device)\n",
    "    train(model, train_loader, 0.001, 0, 25, verbose=False, siamese=False, aux=True, alpha = 0.9)\n",
    "    time2 = time.perf_counter()\n",
    "    times6.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies6.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies6).mean(), torch.tensor(accuracies6).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 164.991, Std: 0.467\n"
     ]
    }
   ],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times6).mean(), torch.tensor(times6).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0') tensor(0.9170, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8890, device='cuda:0')\n",
      "tensor(0.9830, device='cuda:0') tensor(0.8790, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.9060, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8930, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8900, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.9130, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8820, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8970, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.9110, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8780, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8870, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8680, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.9050, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8860, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8840, device='cuda:0')\n",
      "tensor(0.9970, device='cuda:0') tensor(0.8740, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8640, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8840, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.9030, device='cuda:0')\n",
      "Mean: 0.891, Std: 0.015\n"
     ]
    }
   ],
   "source": [
    "accuracies7 = []\n",
    "times7 = []\n",
    "\n",
    "for i in range(20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=50, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    #model = SiameseBaseNet()\n",
    "    #model = BaseNet()\n",
    "    model = AuxsiameseResNet(nb_residual_blocks = 10, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "    model.to(device)\n",
    "    train(model, train_loader, 0.001, 0, 25, verbose=False, siamese=True, aux=True, alpha = 0.7)\n",
    "    time2 = time.perf_counter()\n",
    "    times7.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies7.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies7).mean(), torch.tensor(accuracies7).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 122.711, Std: 0.480\n"
     ]
    }
   ],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times7).mean(), torch.tensor(times7).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-419-1ee55fdbefe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgammas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mte_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcompute_nb_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0maccurate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mte_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-405-c75a86d473c7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, eta, decay, n_epochs, verbose)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mbinary_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Collect accuracy data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'betas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    109\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gammas = torch.logspace(start=-4, end=-2, steps=5)\n",
    "decays = torch.logspace(start=-13, end=-8, steps=6)\n",
    "accuracies = torch.empty((len(gammas), len(decays)))\n",
    "model = BaseNet()\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(decays)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=50, seed=42)\n",
    "            train(model, train_loader, gammas[j], decays[k], 25, verbose=False)\n",
    "            te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        accuracies[j,k] = torch.Tensor(accurate).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-744-d08fd1651f1c>:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  aux1 = F.softmax(x1)\n",
      "<ipython-input-744-d08fd1651f1c>:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  aux2 = F.softmax(x2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.833, Std: 0.009\n",
      "Mean: 0.831, Std: 0.008\n",
      "Mean: 0.819, Std: 0.047\n",
      "Mean: 0.834, Std: 0.008\n",
      "Mean: 0.830, Std: 0.013\n",
      "Mean: 0.833, Std: 0.010\n",
      "Mean: 0.835, Std: 0.018\n",
      "Mean: 0.833, Std: 0.014\n",
      "Mean: 0.833, Std: 0.014\n",
      "Mean: 0.833, Std: 0.017\n"
     ]
    }
   ],
   "source": [
    "for j in range(10):\n",
    "    accuracies3 = []\n",
    "    times3 = []\n",
    "\n",
    "    for i in range(10):\n",
    "        train_loader, test_loader = load_data(N=1000, batch_size=50, seed=i)\n",
    "        time1 = time.perf_counter()\n",
    "        #model = SiameseBaseNet()\n",
    "        #model = BaseNet()\n",
    "        model = AuxBaseNet()\n",
    "        train(model, train_loader, 0.001, 0, 25, verbose=False, siamese=False, aux=True, alpha = j/10)\n",
    "        time2 = time.perf_counter()\n",
    "        times3.append(time2 - time1)\n",
    "\n",
    "        tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "        te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "        accuracies3.append(te_accuracy)\n",
    "    print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies3).mean(), torch.tensor(accuracies3).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-743-f9afae9d6bc5>:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  aux1 = F.softmax(x1)\n",
      "<ipython-input-743-f9afae9d6bc5>:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  aux2 = F.softmax(x2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.860, Std: 0.015\n",
      "Mean: 0.859, Std: 0.011\n",
      "Mean: 0.858, Std: 0.012\n",
      "Mean: 0.857, Std: 0.011\n",
      "Mean: 0.861, Std: 0.010\n",
      "Mean: 0.864, Std: 0.011\n",
      "Mean: 0.859, Std: 0.011\n",
      "Mean: 0.857, Std: 0.017\n",
      "Mean: 0.859, Std: 0.018\n",
      "Mean: 0.859, Std: 0.022\n"
     ]
    }
   ],
   "source": [
    "for j in range(10):\n",
    "    accuracies2 = []\n",
    "    times2 = []\n",
    "\n",
    "    for i in range(10):\n",
    "        train_loader, test_loader = load_data(N=1000, batch_size=50, seed=i)\n",
    "        time1 = time.perf_counter()\n",
    "        #model = SiameseBaseNet()\n",
    "        #model = BaseNet()\n",
    "        model = AuxsiameseBaseNet()\n",
    "        train(model, train_loader, 0.001, 0, 25, verbose=False, siamese=True, aux=True, alpha = j/10)\n",
    "        time2 = time.perf_counter()\n",
    "        times2.append(time2 - time1)\n",
    "\n",
    "        tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "        te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "        accuracies2.append(te_accuracy)\n",
    "    print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies2).mean(), torch.tensor(accuracies2).std())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.871, Std: 0.009\n",
      "Mean: 0.885, Std: 0.003\n",
      "Mean: 0.889, Std: 0.004\n",
      "Mean: 0.892, Std: 0.006\n",
      "Mean: 0.889, Std: 0.014\n",
      "Mean: 0.893, Std: 0.006\n",
      "Mean: 0.892, Std: 0.003\n",
      "Mean: 0.896, Std: 0.003\n",
      "Mean: 0.891, Std: 0.007\n",
      "Mean: 0.890, Std: 0.012\n",
      "Mean: 0.890, Std: 0.018\n"
     ]
    }
   ],
   "source": [
    "for j in range(11):\n",
    "    accuracies999 = []\n",
    "    times999 = []\n",
    "\n",
    "    for i in range(10):\n",
    "        number = torch.randint(1,50,(1,))\n",
    "        train_loader, test_loader = load_data(N=1000, batch_size=50, seed=number)\n",
    "        time1 = time.perf_counter()\n",
    "        model = AuxsiameseResNet(nb_residual_blocks = 10, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "        model.to(device)\n",
    "        #model = BaseNet()\n",
    "        train(model, train_loader, 0.001, 0, 25, verbose=False, siamese=True, aux=True, alpha = j/10)\n",
    "        time2 = time.perf_counter()\n",
    "\n",
    "        tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "        te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "        accuracies999.append(te_accuracy)\n",
    "    print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies999).mean(), torch.tensor(accuracies999).std())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.843, Std: 0.018\n",
      "Mean: 0.858, Std: 0.018\n",
      "Mean: 0.859, Std: 0.018\n",
      "Mean: 0.861, Std: 0.018\n",
      "Mean: 0.860, Std: 0.018\n",
      "Mean: 0.857, Std: 0.018\n",
      "Mean: 0.854, Std: 0.018\n",
      "Mean: 0.854, Std: 0.018\n",
      "Mean: 0.860, Std: 0.018\n",
      "Mean: 0.868, Std: 0.018\n",
      "Mean: 0.867, Std: 0.018\n"
     ]
    }
   ],
   "source": [
    "for j in range(11):\n",
    "    accuracies9999 = []\n",
    "    times9999 = []\n",
    "\n",
    "    for i in range(10):\n",
    "        number = torch.randint(1,50,(1,))\n",
    "        train_loader, test_loader = load_data(N=1000, batch_size=50, seed=number)\n",
    "        time1 = time.perf_counter()\n",
    "        model = AuxResNet(nb_residual_blocks = 10, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "        model.to(device)\n",
    "        #model = BaseNet()\n",
    "        train(model, train_loader, 0.001, 0, 25, verbose=False, siamese=False, aux=True, alpha = j/10)\n",
    "        time2 = time.perf_counter()\n",
    "        times999.append(time2 - time1)\n",
    "        \n",
    "        tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "        te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "        accuracies9999.append(te_accuracy)\n",
    "    print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies9999).mean(), torch.tensor(accuracies999).std())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
