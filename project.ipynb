{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import data_loader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import dlc_practical_prologue as prologue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def load_data(N=1000, batch_size=50, seed=42):\n",
    "    # Load data\n",
    "    train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(N)\n",
    "    train_target = torch.nn.functional.one_hot(train_target)\n",
    "    test_target = torch.nn.functional.one_hot(test_target)\n",
    "    \n",
    "    train_input = train_input.to(device)\n",
    "    train_target = train_target.to(device)\n",
    "    train_classes = train_classes.to(device)\n",
    "    test_input = test_input.to(device)\n",
    "    test_target = test_target.to(device)\n",
    "    test_classes = test_classes.to(device)\n",
    "    # Normalize data\n",
    "    mean, std = train_input.mean(), train_input.std()\n",
    "    train_input.sub_(mean).div_(std)\n",
    "    test_input.sub_(mean).div_(std)\n",
    "    \n",
    "    # Generate dataset\n",
    "    train_data = TensorDataset(train_input, train_target, train_classes)\n",
    "    test_data = TensorDataset(test_input, test_target, test_classes)\n",
    "    \n",
    "    # For reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Generate data loader\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper.py\n",
    "\n",
    "# Count the number of parameters\n",
    "def count_param(model):\n",
    "    return sum([torch.numel(param) for param in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = load_data(N=1000, batch_size=50, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, data_loader):\n",
    "\n",
    "    nb_data_errors = 0\n",
    "\n",
    "    for data_input, data_target, data_classes in data_loader:\n",
    "        output = model(data_input)\n",
    "        nb_error = torch.sum(torch.argmax(output, dim=1, keepdim=True) != torch.argmax(data_target, dim=1, keepdim=True))\n",
    "        nb_data_errors += nb_error\n",
    "        \n",
    "    return nb_data_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors_siamese(model, data_loader):\n",
    "\n",
    "    nb_data_errors = 0\n",
    "    for data_input, data_target, data_classes in data_loader:\n",
    "        data_1, data_2 = data_input.unbind(1)               \n",
    "        output = model(data_1.unsqueeze(1), data_2.unsqueeze(1))\n",
    "        nb_error = torch.sum(torch.argmax(output, dim=1, keepdim=True) != torch.argmax(data_target, dim=1, keepdim=True))\n",
    "        nb_data_errors += nb_error\n",
    "        \n",
    "    return nb_data_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors_auxsiamese(model, data_loader):\n",
    "\n",
    "    nb_data_errors = 0\n",
    "    for data_input, data_target, data_classes in data_loader:\n",
    "        data_1, data_2 = data_input.unbind(1)               \n",
    "        output, aux1, aux2 = model(data_1.unsqueeze(1), data_2.unsqueeze(1))\n",
    "        nb_error = torch.sum(torch.argmax(output, dim=1, keepdim=True) != torch.argmax(data_target, dim=1, keepdim=True))\n",
    "        nb_data_errors += nb_error\n",
    "        \n",
    "    return nb_data_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(392, 160)\n",
    "        self.fc2 = nn.Linear(160, 64) \n",
    "        self.fc3 = nn.Linear(64, 2) \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,392) # flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AuxMLP, self).__init__()\n",
    "        \n",
    "        self.fc11 = nn.Linear(196, 160)\n",
    "        self.fc12 = nn.Linear(196, 160)\n",
    "        self.fc21 = nn.Linear(160, 10)\n",
    "        self.fc22 = nn.Linear(160, 10)\n",
    "        self.fc3 = nn.Linear(20, 2) \n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = x1.view(-1,196) # flatten\n",
    "        x1 = F.relu(self.fc11(x1))\n",
    "        x1 = self.fc21(x1)\n",
    "        aux1 = F.softmax(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        \n",
    "        x2 = x2.view(-1,196) # flatten\n",
    "        x2 = F.relu(self.fc12(x2))\n",
    "        x2 = self.fc22(x2)\n",
    "        aux2 = F.softmax(x2)\n",
    "        x2 = F.relu(x2)\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "\n",
    "        #x = torch.abs(x1 - x2)\n",
    "        #x = F.relu(self.fc1(x.flatten(start_dim=1)))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        \n",
    "        return x, aux1, aux2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseMLP, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(196, 160)\n",
    "        self.fc2 = nn.Linear(160, 10)\n",
    "        self.fc3 = nn.Linear(20, 2) \n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = x1.view(-1,196) # flatten\n",
    "        x1 = F.relu(self.fc1(x1))\n",
    "        x1 = self.fc2(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        \n",
    "        x2 = x2.view(-1,196) # flatten\n",
    "        x2 = F.relu(self.fc1(x2))\n",
    "        x2 = self.fc2(x2)\n",
    "        x2 = F.relu(x2)\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "\n",
    "        #x = torch.abs(x1 - x2)\n",
    "        #x = F.relu(self.fc1(x.flatten(start_dim=1)))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxsiameseMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AuxsiameseMLP, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(196, 160)\n",
    "        self.fc2 = nn.Linear(160, 10)\n",
    "        self.fc3 = nn.Linear(20, 2) \n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = x1.view(-1,196) # flatten\n",
    "        x1 = F.relu(self.fc1(x1))\n",
    "        x1 = self.fc2(x1)\n",
    "        aux1 = F.softmax(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        \n",
    "        x2 = x2.view(-1,196) # flatten\n",
    "        x2 = F.relu(self.fc1(x2))\n",
    "        x2 = self.fc2(x2)\n",
    "        aux2 = F.softmax(x2)\n",
    "        x2 = F.relu(x2)\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "\n",
    "        #x = torch.abs(x1 - x2)\n",
    "        #x = F.relu(self.fc1(x.flatten(start_dim=1)))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        \n",
    "        return x, aux1, aux2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 32, kernel_size=3)    # size [nb, 32, 12, 12]\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)   # size [nb, 64, 4, 4]\n",
    "        self.fc1 = nn.Linear(256, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "        self.fc3 = nn.Linear(10, 2)\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2)) # size [nb, 32, 6, 6]      \n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2)) # size [nb, 64, 2, 2]\n",
    "        x = x.view(-1, 256) # size [nb, 256]\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseBaseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseBaseNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)    # size [nb, 32, 10, 10]\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)   # size [nb, 64, 4, 4]\n",
    "        self.fc1 = nn.Linear(256, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "        self.fc3 = nn.Linear(20, 2)\n",
    "        \n",
    "    def convs(self, x):        \n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2)) # size [nb, 32, 5, 5]      \n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2)) # size [nb, 64, 2, 2]\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.convs(x1)\n",
    "        x1 = x1.view(-1, 256)\n",
    "        x1 = F.relu((self.fc1(x1)))\n",
    "        x1 = F.relu(self.fc2(x1))\n",
    "        \n",
    "        x2 = self.convs(x2)\n",
    "        x2 = x2.view(-1, 256)\n",
    "        x2 = F.relu(self.fc1(x2))\n",
    "        x2 = F.relu(self.fc2(x2))\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        #x = torch.abs(x1 - x2)\n",
    "        #x = F.relu(self.fc1(x.flatten(start_dim=1)))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        \n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxsiameseBaseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AuxsiameseBaseNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)    # size [nb, 32, 10, 10]\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)   # size [nb, 64, 4, 4]\n",
    "        self.fc1 = nn.Linear(256, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "        self.fc3 = nn.Linear(20, 2)\n",
    "        \n",
    "    def convs(self, x):        \n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2)) # size [nb, 32, 5, 5]      \n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2)) # size [nb, 64, 2, 2]\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.convs(x1)\n",
    "        x1 = x1.view(-1, 256)\n",
    "        x1 = F.relu((self.fc1(x1)))\n",
    "        x1 = self.fc2(x1)\n",
    "        aux1 = F.softmax(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        \n",
    "        x2 = self.convs(x2)\n",
    "        x2 = x2.view(-1, 256)\n",
    "        x2 = F.relu(self.fc1(x2))\n",
    "        x2 = self.fc2(x2)\n",
    "        aux2 = F.softmax(x2)\n",
    "        x2 = F.relu(x2)\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        #x = torch.abs(x1 - x2)\n",
    "        #x = F.relu(self.fc1(x.flatten(start_dim=1)))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        \n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        \n",
    "        return x, aux1, aux2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxBaseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AuxBaseNet, self).__init__()\n",
    "        self.conv11 = nn.Conv2d(1, 32, kernel_size=3)    # size [nb, 32, 10, 10]\n",
    "        self.conv21 = nn.Conv2d(32, 64, kernel_size=3)   # size [nb, 64, 4, 4]\n",
    "        self.fc11 = nn.Linear(256, 200)\n",
    "        self.fc21 = nn.Linear(200, 10)\n",
    "        self.conv12 = nn.Conv2d(1, 32, kernel_size=5)    # size [nb, 32, 10, 10]\n",
    "        self.conv22 = nn.Conv2d(32, 64, kernel_size=2)   # size [nb, 64, 4, 4]\n",
    "        self.fc12 = nn.Linear(256, 200)\n",
    "        self.fc22 = nn.Linear(200, 10)\n",
    "        self.fc3 = nn.Linear(20, 2)\n",
    "        \n",
    "    def convs(self, x):        \n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2)) # size [nb, 32, 5, 5]      \n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2)) # size [nb, 64, 2, 2]\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = F.relu(F.max_pool2d(self.conv11(x1), kernel_size=2)) # size [nb, 32, 5, 5]  \n",
    "        x1 = F.relu(F.max_pool2d(self.conv21(x1), kernel_size=2)) # size [nb, 64, 2, 2]\n",
    "        x1 = x1.view(-1, 256)\n",
    "        x1 = F.relu((self.fc11(x1)))\n",
    "        x1 = self.fc21(x1)\n",
    "        aux1 = F.softmax(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        \n",
    "        x2 = F.relu(F.max_pool2d(self.conv12(x2), kernel_size=2)) # size [nb, 32, 5, 5]  \n",
    "        x2 = F.relu(F.max_pool2d(self.conv22(x2), kernel_size=2)) # size [nb, 64, 2, 2]\n",
    "        x2 = x2.view(-1, 256)\n",
    "        x2 = F.relu((self.fc12(x2)))\n",
    "        x2 = self.fc22(x2)\n",
    "        aux2 = F.softmax(x2)\n",
    "        x2 = F.relu(x2)\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        #x = torch.abs(x1 - x2)\n",
    "        #x = F.relu(self.fc1(x.flatten(start_dim=1)))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        \n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        \n",
    "        return x, aux1, aux2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNetBlock with skip-connection and batch normalization\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, nb_channels, kernel_size, dropout = 0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(nb_channels, nb_channels,\n",
    "                               kernel_size = kernel_size,\n",
    "                               padding = (kernel_size - 1) // 2)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(nb_channels)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(nb_channels, nb_channels,\n",
    "                               kernel_size = kernel_size,\n",
    "                               padding = (kernel_size - 1) // 2)\n",
    "\n",
    "        self.bn2 = nn.BatchNorm2d(nb_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        y = self.bn1(y)\n",
    "        y = self.dropout(y)\n",
    "        y = F.relu(y)\n",
    "        y = self.conv2(y)\n",
    "        y = self.bn2(y)\n",
    "        y = self.dropout(y)\n",
    "        y = y + x\n",
    "        y = F.relu(y)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, nb_residual_blocks, input_channels, nb_channels,\n",
    "                 kernel_size = 3, nb_classes = 10, dropout = 0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(input_channels, nb_channels,\n",
    "                              kernel_size = kernel_size,\n",
    "                              padding = (kernel_size - 1) // 2)\n",
    "        self.bn = nn.BatchNorm2d(nb_channels)\n",
    "\n",
    "        self.resnet_blocks = nn.Sequential(\n",
    "            *(ResNetBlock(nb_channels, kernel_size, dropout)\n",
    "              for _ in range(nb_residual_blocks))\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(288, nb_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn(self.conv(x)))\n",
    "        #x = F.relu(self.dropout(self.bn(self.conv(x))))\n",
    "        x = self.resnet_blocks(x)\n",
    "        x = F.avg_pool2d(x, 4).view(x.size(0), -1)\n",
    "        x = torch.sigmoid(self.fc(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, nb_residual_blocks, input_channels, nb_channels,\n",
    "                 kernel_size = 3, nb_classes = 10, dropout = 0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(input_channels, nb_channels,\n",
    "                              kernel_size = kernel_size,\n",
    "                              padding = (kernel_size - 1) // 2)\n",
    "        self.bn = nn.BatchNorm2d(nb_channels)\n",
    "\n",
    "        self.resnet_blocks = nn.Sequential(\n",
    "            *(ResNetBlock(nb_channels, kernel_size, dropout)\n",
    "              for _ in range(nb_residual_blocks))\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(576, nb_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = F.relu(self.bn(self.conv(x1)))\n",
    "        #x = F.relu(self.dropout(self.bn(self.conv(x))))\n",
    "        x1 = self.resnet_blocks(x1)\n",
    "        x1 = F.avg_pool2d(x1, 4).view(x1.size(0), -1)\n",
    "        \n",
    "        x2 = F.relu(self.bn(self.conv(x2)))\n",
    "        #x = F.relu(self.dropout(self.bn(self.conv(x))))\n",
    "        x2 = self.resnet_blocks(x2)\n",
    "        x2 = F.avg_pool2d(x2, 4).view(x2.size(0), -1)\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        #aux1 = F.softmax(self.fc1(x1)\n",
    "        #aux2 = F.softmax(self.fc1(x2)\n",
    "        #x = torch.abs(x1 - x2)\n",
    "        #x = F.relu(self.fc1(x.flatten(start_dim=1)))\n",
    "        #x = F.relu(self.fc2(x))      \n",
    "        x = torch.sigmoid(self.fc(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, nb_residual_blocks, input_channels, nb_channels,\n",
    "                 kernel_size = 3, nb_classes = 10, dropout = 0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(input_channels, nb_channels,\n",
    "                              kernel_size = kernel_size,\n",
    "                              padding = (kernel_size - 1) // 2)\n",
    "        self.conv2 = nn.Conv2d(input_channels, nb_channels,\n",
    "                              kernel_size = kernel_size,\n",
    "                              padding = (kernel_size - 1) // 2)\n",
    "        self.bn1 = nn.BatchNorm2d(nb_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(nb_channels)\n",
    "\n",
    "        self.resnet_blocks1 = nn.Sequential(\n",
    "            *(ResNetBlock(nb_channels, kernel_size, dropout)\n",
    "              for _ in range(nb_residual_blocks))\n",
    "        )\n",
    "        self.resnet_blocks2 = nn.Sequential(\n",
    "            *(ResNetBlock(nb_channels, kernel_size, dropout)\n",
    "              for _ in range(nb_residual_blocks))\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(576, nb_classes)\n",
    "        self.fc1 = nn.Linear(288, 10)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = F.relu(self.bn1(self.conv1(x1)))\n",
    "        #x = F.relu(self.dropout(self.bn(self.conv(x))))\n",
    "        x1 = self.resnet_blocks1(x1)\n",
    "        x1 = F.avg_pool2d(x1, 4).view(x1.size(0), -1)\n",
    "        \n",
    "        x2 = F.relu(self.bn2(self.conv2(x2)))\n",
    "        #x = F.relu(self.dropout(self.bn(self.conv(x))))\n",
    "        x2 = self.resnet_blocks2(x2)\n",
    "        x2 = F.avg_pool2d(x2, 4).view(x2.size(0), -1)\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        x = torch.sigmoid(self.fc(x))\n",
    "        aux1 = F.softmax(self.fc1(x1))\n",
    "        aux2 = F.softmax(self.fc1(x2))\n",
    "        #aux1 = F.softmax(x1)\n",
    "        #aux2 = F.softmax(x2)\n",
    "        #x = torch.abs(x1 - x2)\n",
    "        #x = F.relu(self.fc1(x.flatten(start_dim=1)))\n",
    "        #x = F.relu(self.fc2(x))      \n",
    "        \n",
    "        return x, aux1, aux2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxsiameseResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, nb_residual_blocks, input_channels, nb_channels,\n",
    "                 kernel_size = 3, nb_classes = 10, dropout = 0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(input_channels, nb_channels,\n",
    "                              kernel_size = kernel_size,\n",
    "                              padding = (kernel_size - 1) // 2)\n",
    "        self.bn = nn.BatchNorm2d(nb_channels)\n",
    "\n",
    "        self.resnet_blocks = nn.Sequential(\n",
    "            *(ResNetBlock(nb_channels, kernel_size, dropout)\n",
    "              for _ in range(nb_residual_blocks))\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(576, nb_classes)\n",
    "        self.fc1 = nn.Linear(288, 10)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = F.relu(self.bn(self.conv(x1)))\n",
    "        #x = F.relu(self.dropout(self.bn(self.conv(x))))\n",
    "        x1 = self.resnet_blocks(x1)\n",
    "        x1 = F.avg_pool2d(x1, 4).view(x1.size(0), -1)\n",
    "        \n",
    "        x2 = F.relu(self.bn(self.conv(x2)))\n",
    "        #x = F.relu(self.dropout(self.bn(self.conv(x))))\n",
    "        x2 = self.resnet_blocks(x2)\n",
    "        x2 = F.avg_pool2d(x2, 4).view(x2.size(0), -1)\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        x = torch.sigmoid(self.fc(x))\n",
    "        aux1 = F.softmax(self.fc1(x1))\n",
    "        aux2 = F.softmax(self.fc1(x2))\n",
    "        #aux1 = F.softmax(x1)\n",
    "        #aux2 = F.softmax(x2)\n",
    "        #x = torch.abs(x1 - x2)\n",
    "        #x = F.relu(self.fc1(x.flatten(start_dim=1)))\n",
    "        #x = F.relu(self.fc2(x))      \n",
    "        \n",
    "        return x, aux1, aux2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78924"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AuxsiameseResNet(nb_residual_blocks = 4, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "\n",
    "# Calculate the number of parameters in the model\n",
    "count_param(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76034"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SiameseResNet(nb_residual_blocks = 4, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "\n",
    "# Calculate the number of parameters in the model\n",
    "count_param(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153804"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AuxResNet(nb_residual_blocks = 4, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "\n",
    "# Calculate the number of parameters in the model\n",
    "count_param(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75746"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet(nb_residual_blocks = 4, input_channels = 2, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "# Calculate the number of parameters in the model\n",
    "count_param(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72536"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BaseNet()\n",
    "\n",
    "# Calculate the number of parameters in the model\n",
    "count_param(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72268"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SiameseBaseNet()\n",
    "\n",
    "# Calculate the number of parameters in the model\n",
    "count_param(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72268"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AuxsiameseBaseNet()\n",
    "\n",
    "# Calculate the number of parameters in the model\n",
    "count_param(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134766"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AuxBaseNet()\n",
    "\n",
    "# Calculate the number of parameters in the model\n",
    "count_param(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, eta, decay, n_epochs=25, verbose=False, siamese=False, aux=False, alpha = 0):\n",
    "\n",
    "    #binary_crit = nn.CrossEntropyLoss()\n",
    "    binary_crit = torch.nn.BCELoss()\n",
    "    aux_crit = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=eta, weight_decay=decay)\n",
    "    #optimizer = torch.optim.LBFGS(model.parameters(), lr=eta)\n",
    "    tr_losses = []\n",
    "    tr_accuracies = []\n",
    "\n",
    "    for e in range(n_epochs):\n",
    "        # Reset training/validation loss\n",
    "        tr_loss = 0\n",
    "\n",
    "        # Training model\n",
    "        model.train()\n",
    "\n",
    "        for train_input, train_target, train_classes in iter(train_loader):\n",
    "            #train_target = torch.nn.functional.one_hot(train_target)\n",
    "            # Forward pass\n",
    "            \n",
    "            if siamese == True:\n",
    "                train_1, train_2 = train_input.unbind(1)\n",
    "                if aux == True:\n",
    "                    output, aux1, aux2 = model(train_1.unsqueeze(1), train_2.unsqueeze(1))\n",
    "                else:\n",
    "                    output = model(train_1.unsqueeze(1), train_2.unsqueeze(1))\n",
    "            elif aux == True:\n",
    "                train_1, train_2 = train_input.unbind(1)\n",
    "                output, aux1, aux2 = model(train_1.unsqueeze(1), train_2.unsqueeze(1))\n",
    "            else:\n",
    "                output = model(train_input)\n",
    "                \n",
    "            # Binary classification loss\n",
    "            binary_loss = binary_crit(output, train_target.float())\n",
    "            total_loss = binary_loss\n",
    "            \n",
    "            # Auxiliary loss\n",
    "            if aux == True:\n",
    "\n",
    "                aux_loss1 = aux_crit(aux1, train_classes[:,0])\n",
    "                aux_loss2 = aux_crit(aux2, train_classes[:,1])\n",
    "                aux_loss = aux_loss1 + aux_loss2\n",
    "                total_loss = binary_loss + aux_loss * alpha\n",
    "        \n",
    "            # Total loss = Binary loss + aux loss * alpha\n",
    "            \n",
    "            tr_loss += total_loss\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Collect accuracy data\n",
    "        # tr_accuracies.append(compute_nb_errors_siamese(model, train_loader)/1000)\n",
    "\n",
    "        # Collect loss data\n",
    "        tr_losses.append(tr_loss)\n",
    "\n",
    "        if verbose:\n",
    "            print('Epoch %d/%d, Binary loss: %.3f' %\n",
    "                  (e+1, n_epochs, tr_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Binary loss: 96.767\n",
      "Epoch 2/25, Binary loss: 83.466\n",
      "Epoch 3/25, Binary loss: 77.176\n",
      "Epoch 4/25, Binary loss: 71.319\n",
      "Epoch 5/25, Binary loss: 69.567\n",
      "Epoch 6/25, Binary loss: 68.520\n",
      "Epoch 7/25, Binary loss: 67.732\n",
      "Epoch 8/25, Binary loss: 67.278\n",
      "Epoch 9/25, Binary loss: 66.580\n",
      "Epoch 10/25, Binary loss: 65.992\n",
      "Epoch 11/25, Binary loss: 65.673\n",
      "Epoch 12/25, Binary loss: 65.069\n",
      "Epoch 13/25, Binary loss: 64.851\n",
      "Epoch 14/25, Binary loss: 64.775\n",
      "Epoch 15/25, Binary loss: 63.923\n",
      "Epoch 16/25, Binary loss: 63.595\n",
      "Epoch 17/25, Binary loss: 63.249\n",
      "Epoch 18/25, Binary loss: 62.958\n",
      "Epoch 19/25, Binary loss: 62.756\n",
      "Epoch 20/25, Binary loss: 62.658\n",
      "Epoch 21/25, Binary loss: 62.580\n",
      "Epoch 22/25, Binary loss: 62.476\n",
      "Epoch 23/25, Binary loss: 62.411\n",
      "Epoch 24/25, Binary loss: 62.374\n",
      "Epoch 25/25, Binary loss: 62.351\n",
      "Spend 5.805825e+01 s\n",
      "tensor(1., device='cuda:0') tensor(0.8810, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "number = torch.randint(1,50,(1,))\n",
    "train_loader, test_loader = load_data(N=1000, batch_size=50, seed=number)\n",
    "time1 = time.perf_counter()\n",
    "model = AuxsiameseResNet(nb_residual_blocks = 4, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "model.to(device)\n",
    "#model = BaseNet()\n",
    "train(model, train_loader, 0.001, 0, 25, verbose=True, siamese=True, aux=True, alpha = 1)\n",
    "time2 = time.perf_counter()\n",
    "print('Spend {:e} s'.format(time2 - time1))\n",
    "\n",
    "tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "print(tr_accuracy, te_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Binary loss: 40.103\n",
      "Epoch 2/25, Binary loss: 31.625\n",
      "Epoch 3/25, Binary loss: 28.229\n",
      "Epoch 4/25, Binary loss: 26.540\n",
      "Epoch 5/25, Binary loss: 24.686\n",
      "Epoch 6/25, Binary loss: 23.148\n",
      "Epoch 7/25, Binary loss: 22.162\n",
      "Epoch 8/25, Binary loss: 21.673\n",
      "Epoch 9/25, Binary loss: 20.688\n",
      "Epoch 10/25, Binary loss: 19.981\n",
      "Epoch 11/25, Binary loss: 19.738\n",
      "Epoch 12/25, Binary loss: 19.896\n",
      "Epoch 13/25, Binary loss: 19.436\n",
      "Epoch 14/25, Binary loss: 19.325\n",
      "Epoch 15/25, Binary loss: 19.050\n",
      "Epoch 16/25, Binary loss: 18.952\n",
      "Epoch 17/25, Binary loss: 18.856\n",
      "Epoch 18/25, Binary loss: 18.774\n",
      "Epoch 19/25, Binary loss: 18.752\n",
      "Epoch 20/25, Binary loss: 17.955\n",
      "Epoch 21/25, Binary loss: 17.716\n",
      "Epoch 22/25, Binary loss: 17.694\n",
      "Epoch 23/25, Binary loss: 17.623\n",
      "Epoch 24/25, Binary loss: 17.593\n",
      "Epoch 25/25, Binary loss: 17.584\n",
      "Spend 1.226981e+02 s\n",
      "tensor(1., device='cuda:0') tensor(0.8610, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "number = torch.randint(1,50,(1,))\n",
    "train_loader, test_loader = load_data(N=1000, batch_size=50, seed=number)\n",
    "time1 = time.perf_counter()\n",
    "#model = SiameseBaseNet()\n",
    "#model = BaseNet()\n",
    "model = AuxsiameseResNet(nb_residual_blocks = 10, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "model.to(device)\n",
    "train(model, train_loader, 0.001, 0, 25, verbose=True, siamese=False, aux=True, alpha = 0.3)\n",
    "time2 = time.perf_counter()\n",
    "print('Spend {:e} s'.format(time2 - time1))\n",
    "\n",
    "tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "print(tr_accuracy, te_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1081,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Binary loss: 11.641\n",
      "Epoch 2/25, Binary loss: 8.032\n",
      "Epoch 3/25, Binary loss: 5.910\n",
      "Epoch 4/25, Binary loss: 5.168\n",
      "Epoch 5/25, Binary loss: 4.474\n",
      "Epoch 6/25, Binary loss: 3.215\n",
      "Epoch 7/25, Binary loss: 3.069\n",
      "Epoch 8/25, Binary loss: 3.053\n",
      "Epoch 9/25, Binary loss: 2.638\n",
      "Epoch 10/25, Binary loss: 2.044\n",
      "Epoch 11/25, Binary loss: 2.564\n",
      "Epoch 12/25, Binary loss: 1.829\n",
      "Epoch 13/25, Binary loss: 1.344\n",
      "Epoch 14/25, Binary loss: 0.564\n",
      "Epoch 15/25, Binary loss: 0.404\n",
      "Epoch 16/25, Binary loss: 0.158\n",
      "Epoch 17/25, Binary loss: 0.359\n",
      "Epoch 18/25, Binary loss: 0.458\n",
      "Epoch 19/25, Binary loss: 0.738\n",
      "Epoch 20/25, Binary loss: 0.517\n",
      "Epoch 21/25, Binary loss: 0.761\n",
      "Epoch 22/25, Binary loss: 0.393\n",
      "Epoch 23/25, Binary loss: 0.200\n",
      "Epoch 24/25, Binary loss: 0.130\n",
      "Epoch 25/25, Binary loss: 0.067\n",
      "Spend 4.333027e+02 s\n",
      "tensor(0.9990) tensor(0.8690)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "number = torch.randint(1,50,(1,))\n",
    "train_loader, test_loader = load_data(N=1000, batch_size=50, seed=number)\n",
    "time1 = time.perf_counter()\n",
    "model = SiameseResNet(nb_residual_blocks = 10, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "#model = BaseNet()\n",
    "train(model, train_loader, 0.001, 0, 25, verbose=True, siamese=True)\n",
    "time2 = time.perf_counter()\n",
    "print('Spend {:e} s'.format(time2 - time1))\n",
    "\n",
    "tr_accuracy = 1 - compute_nb_errors_siamese(model, train_loader)/1000\n",
    "te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "print(tr_accuracy, te_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Binary loss: 12.063\n",
      "Epoch 2/25, Binary loss: 8.823\n",
      "Epoch 3/25, Binary loss: 6.645\n",
      "Epoch 4/25, Binary loss: 5.994\n",
      "Epoch 5/25, Binary loss: 4.474\n",
      "Epoch 6/25, Binary loss: 3.104\n",
      "Epoch 7/25, Binary loss: 2.493\n",
      "Epoch 8/25, Binary loss: 1.249\n",
      "Epoch 9/25, Binary loss: 0.706\n",
      "Epoch 10/25, Binary loss: 0.315\n",
      "Epoch 11/25, Binary loss: 0.129\n",
      "Epoch 12/25, Binary loss: 0.076\n",
      "Epoch 13/25, Binary loss: 0.047\n",
      "Epoch 14/25, Binary loss: 0.038\n",
      "Epoch 15/25, Binary loss: 0.030\n",
      "Epoch 16/25, Binary loss: 0.029\n",
      "Epoch 17/25, Binary loss: 0.023\n",
      "Epoch 18/25, Binary loss: 0.019\n",
      "Epoch 19/25, Binary loss: 0.017\n",
      "Epoch 20/25, Binary loss: 0.016\n",
      "Epoch 21/25, Binary loss: 0.015\n",
      "Epoch 22/25, Binary loss: 0.012\n",
      "Epoch 23/25, Binary loss: 0.011\n",
      "Epoch 24/25, Binary loss: 0.011\n",
      "Epoch 25/25, Binary loss: 0.011\n",
      "Spend 3.890840e+01 s\n",
      "tensor(1., device='cuda:0') tensor(0.8460, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "number = torch.randint(1,50,(1,))\n",
    "train_loader, test_loader = load_data(N=1000, batch_size=50, seed=number)\n",
    "time1 = time.perf_counter()\n",
    "#model = SiameseBaseNet()\n",
    "model = ResNet(nb_residual_blocks = 4, input_channels = 2, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "model.to(device)\n",
    "train(model, train_loader, 0.001, 0, 25, verbose=True, siamese=False)\n",
    "time2 = time.perf_counter()\n",
    "print('Spend {:e} s'.format(time2 - time1))\n",
    "\n",
    "tr_accuracy = 1 - compute_nb_errors(model, train_loader)/1000\n",
    "te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "print(tr_accuracy, te_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Binary loss: 2.743\n",
      "Epoch 2/25, Binary loss: 2.704\n",
      "Epoch 3/25, Binary loss: 2.636\n",
      "Epoch 4/25, Binary loss: 2.529\n",
      "Epoch 5/25, Binary loss: 2.372\n",
      "Epoch 6/25, Binary loss: 2.187\n",
      "Epoch 7/25, Binary loss: 1.991\n",
      "Epoch 8/25, Binary loss: 1.832\n",
      "Epoch 9/25, Binary loss: 1.687\n",
      "Epoch 10/25, Binary loss: 1.552\n",
      "Epoch 11/25, Binary loss: 1.421\n",
      "Epoch 12/25, Binary loss: 1.303\n",
      "Epoch 13/25, Binary loss: 1.189\n",
      "Epoch 14/25, Binary loss: 1.096\n",
      "Epoch 15/25, Binary loss: 1.001\n",
      "Epoch 16/25, Binary loss: 0.925\n",
      "Epoch 17/25, Binary loss: 0.836\n",
      "Epoch 18/25, Binary loss: 0.760\n",
      "Epoch 19/25, Binary loss: 0.696\n",
      "Epoch 20/25, Binary loss: 0.614\n",
      "Epoch 21/25, Binary loss: 0.548\n",
      "Epoch 22/25, Binary loss: 0.487\n",
      "Epoch 23/25, Binary loss: 0.422\n",
      "Epoch 24/25, Binary loss: 0.361\n",
      "Epoch 25/25, Binary loss: 0.307\n",
      "Spend 9.664215e+00 s\n",
      "tensor(0.9890) tensor(0.8580)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "number = torch.randint(1,50,(1,))\n",
    "train_loader, test_loader = load_data(N=1000, batch_size=50, seed=number)\n",
    "time1 = time.perf_counter()\n",
    "model = SiameseBaseNet()\n",
    "#model = BaseNet()\n",
    "train(model, train_loader, 0.001, 0, 25, verbose=True, siamese=True)\n",
    "time2 = time.perf_counter()\n",
    "print('Spend {:e} s'.format(time2 - time1))\n",
    "\n",
    "tr_accuracy = 1 - compute_nb_errors_siamese(model, train_loader)/1000\n",
    "te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "print(tr_accuracy, te_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Binary loss: 13.981\n",
      "Epoch 2/25, Binary loss: 13.983\n",
      "Epoch 3/25, Binary loss: 13.972\n",
      "Epoch 4/25, Binary loss: 13.956\n",
      "Epoch 5/25, Binary loss: 13.943\n",
      "Epoch 6/25, Binary loss: 13.928\n",
      "Epoch 7/25, Binary loss: 13.918\n",
      "Epoch 8/25, Binary loss: 13.908\n",
      "Epoch 9/25, Binary loss: 13.899\n",
      "Epoch 10/25, Binary loss: 13.892\n",
      "Epoch 11/25, Binary loss: 13.886\n",
      "Epoch 12/25, Binary loss: 13.881\n",
      "Epoch 13/25, Binary loss: 13.877\n",
      "Epoch 14/25, Binary loss: 13.873\n",
      "Epoch 15/25, Binary loss: 13.871\n",
      "Epoch 16/25, Binary loss: 13.869\n",
      "Epoch 17/25, Binary loss: 13.867\n",
      "Epoch 18/25, Binary loss: 13.866\n",
      "Epoch 19/25, Binary loss: 13.865\n",
      "Epoch 20/25, Binary loss: 13.864\n",
      "Epoch 21/25, Binary loss: 13.864\n",
      "Epoch 22/25, Binary loss: 13.863\n",
      "Epoch 23/25, Binary loss: 13.863\n",
      "Epoch 24/25, Binary loss: 13.863\n",
      "Epoch 25/25, Binary loss: 13.863\n",
      "Spend 1.280331e+01 s\n",
      "tensor(0.5720, device='cuda:0') tensor(0.5230, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "number = torch.randint(1,50,(1,))\n",
    "train_loader, test_loader = load_data(N=1000, batch_size=50, seed=number)\n",
    "time1 = time.perf_counter()\n",
    "#modeF.avg_pool2dl = SiameseBaseNet()\n",
    "model = BaseNet()\n",
    "model.to(device)\n",
    "train(model, train_loader, 0.001, 100, verbose=True, siamese=False)\n",
    "time2 = time.perf_counter()\n",
    "print('Spend {:e} s'.format(time2 - time1))\n",
    "\n",
    "tr_accuracy = 1 - compute_nb_errors(model, train_loader)/1000\n",
    "te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "print(tr_accuracy, te_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Binary loss: 13.847\n",
      "Epoch 2/25, Binary loss: 13.813\n",
      "Epoch 3/25, Binary loss: 13.823\n",
      "Epoch 4/25, Binary loss: 13.830\n",
      "Epoch 5/25, Binary loss: 13.837\n",
      "Epoch 6/25, Binary loss: 13.843\n",
      "Epoch 7/25, Binary loss: 13.849\n",
      "Epoch 8/25, Binary loss: 13.853\n",
      "Epoch 9/25, Binary loss: 13.855\n",
      "Epoch 10/25, Binary loss: 13.858\n",
      "Epoch 11/25, Binary loss: 13.859\n",
      "Epoch 12/25, Binary loss: 13.860\n",
      "Epoch 13/25, Binary loss: 13.860\n",
      "Epoch 14/25, Binary loss: 13.861\n",
      "Epoch 15/25, Binary loss: 13.861\n",
      "Epoch 16/25, Binary loss: 13.861\n",
      "Epoch 17/25, Binary loss: 13.861\n",
      "Epoch 18/25, Binary loss: 13.861\n",
      "Epoch 19/25, Binary loss: 13.861\n",
      "Epoch 20/25, Binary loss: 13.861\n",
      "Epoch 21/25, Binary loss: 13.861\n",
      "Epoch 22/25, Binary loss: 13.861\n",
      "Epoch 23/25, Binary loss: 13.862\n",
      "Epoch 24/25, Binary loss: 13.861\n",
      "Epoch 25/25, Binary loss: 13.861\n",
      "Spend 8.013068e+00 s\n",
      "tensor(0.5650, device='cuda:0') tensor(0.5550, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "number = torch.randint(1,50,(1,))\n",
    "train_loader, test_loader = load_data(N=1000, batch_size=50, seed=number)\n",
    "time1 = time.perf_counter()\n",
    "#modeF.avg_pool2dl = SiameseBaseNet()\n",
    "model = MLP()\n",
    "model.to(device)\n",
    "train(model, train_loader, 0.001, 25, verbose=True, siamese=False)\n",
    "time2 = time.perf_counter()\n",
    "print('Spend {:e} s'.format(time2 - time1))\n",
    "\n",
    "tr_accuracy = 1 - compute_nb_errors(model, train_loader)/1000\n",
    "te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "print(tr_accuracy, te_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Binary loss: 40.692\n",
      "Epoch 2/25, Binary loss: 36.508\n",
      "Epoch 3/25, Binary loss: 33.839\n",
      "Epoch 4/25, Binary loss: 32.298\n",
      "Epoch 5/25, Binary loss: 31.083\n",
      "Epoch 6/25, Binary loss: 30.109\n",
      "Epoch 7/25, Binary loss: 29.243\n",
      "Epoch 8/25, Binary loss: 28.548\n",
      "Epoch 9/25, Binary loss: 27.797\n",
      "Epoch 10/25, Binary loss: 27.353\n",
      "Epoch 11/25, Binary loss: 26.681\n",
      "Epoch 12/25, Binary loss: 26.124\n",
      "Epoch 13/25, Binary loss: 25.867\n",
      "Epoch 14/25, Binary loss: 25.536\n",
      "Epoch 15/25, Binary loss: 25.161\n",
      "Epoch 16/25, Binary loss: 25.083\n",
      "Epoch 17/25, Binary loss: 25.024\n",
      "Epoch 18/25, Binary loss: 24.913\n",
      "Epoch 19/25, Binary loss: 24.722\n",
      "Epoch 20/25, Binary loss: 24.646\n",
      "Epoch 21/25, Binary loss: 24.586\n",
      "Epoch 22/25, Binary loss: 24.558\n",
      "Epoch 23/25, Binary loss: 24.581\n",
      "Epoch 24/25, Binary loss: 24.514\n",
      "Epoch 25/25, Binary loss: 24.502\n",
      "Spend 1.869615e+01 s\n",
      "tensor(1., device='cuda:0') tensor(0.8450, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "number = torch.randint(1,50,(1,))\n",
    "train_loader, test_loader = load_data(N=1000, batch_size=50, seed=number)\n",
    "time1 = time.perf_counter()\n",
    "#model = SiameseBaseNet()\n",
    "#model = BaseNet()\n",
    "model = AuxsiameseBaseNet()\n",
    "model.to(device)\n",
    "train(model, train_loader, 0.001, 0, 25, verbose=True, siamese=True, aux=True, alpha = 0.3)\n",
    "time2 = time.perf_counter()\n",
    "print('Spend {:e} s'.format(time2 - time1))\n",
    "\n",
    "tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "print(tr_accuracy, te_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Binary loss: 41.120\n",
      "Epoch 2/25, Binary loss: 38.946\n",
      "Epoch 3/25, Binary loss: 36.412\n",
      "Epoch 4/25, Binary loss: 34.846\n",
      "Epoch 5/25, Binary loss: 33.540\n",
      "Epoch 6/25, Binary loss: 32.230\n",
      "Epoch 7/25, Binary loss: 30.754\n",
      "Epoch 8/25, Binary loss: 29.528\n",
      "Epoch 9/25, Binary loss: 28.194\n",
      "Epoch 10/25, Binary loss: 26.966\n",
      "Epoch 11/25, Binary loss: 25.878\n",
      "Epoch 12/25, Binary loss: 25.103\n",
      "Epoch 13/25, Binary loss: 24.781\n",
      "Epoch 14/25, Binary loss: 23.868\n",
      "Epoch 15/25, Binary loss: 23.551\n",
      "Epoch 16/25, Binary loss: 23.321\n",
      "Epoch 17/25, Binary loss: 22.649\n",
      "Epoch 18/25, Binary loss: 22.447\n",
      "Epoch 19/25, Binary loss: 21.881\n",
      "Epoch 20/25, Binary loss: 21.647\n",
      "Epoch 21/25, Binary loss: 21.554\n",
      "Epoch 22/25, Binary loss: 21.414\n",
      "Epoch 23/25, Binary loss: 21.244\n",
      "Epoch 24/25, Binary loss: 21.043\n",
      "Epoch 25/25, Binary loss: 20.986\n",
      "Spend 2.300434e+01 s\n",
      "tensor(1., device='cuda:0') tensor(0.8680, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "number = torch.randint(1,50,(1,))\n",
    "train_loader, test_loader = load_data(N=1000, batch_size=50, seed=number)\n",
    "time1 = time.perf_counter()\n",
    "#model = SiameseBaseNet()\n",
    "#model = BaseNet()\n",
    "model = AuxBaseNet()\n",
    "model.to(device)\n",
    "train(model, train_loader, 0.001, 0, 25, verbose=True, siamese=False, aux=True, alpha = 0.3)\n",
    "time2 = time.perf_counter()\n",
    "print('Spend {:e} s'.format(time2 - time1))\n",
    "\n",
    "tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "print(tr_accuracy, te_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0') tensor(0.8280, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8090, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8190, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.7850, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8160, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.7910, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8030, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8180, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8140, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.7930, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.7890, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8050, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8040, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8180, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8190, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8090, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8080, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8190, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8070, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8100, device='cuda:0')\n",
      "Mean: 0.808, Std: 0.012\n"
     ]
    }
   ],
   "source": [
    "accuracies8 = []\n",
    "times8 = []\n",
    "\n",
    "for i in range(20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=32, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    #model = SiameseBaseNet()\n",
    "    model = MLP()\n",
    "    model.to(device)\n",
    "    train(model, train_loader, 0.001, 0, 25, verbose=False, siamese=False)\n",
    "    time2 = time.perf_counter()\n",
    "    times8.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies8.append(te_accuracy)\n",
    "\n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies8).mean(), torch.tensor(accuracies8).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 12.561, Std: 0.105\n"
     ]
    }
   ],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times8).mean(), torch.tensor(times8).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9990, device='cuda:0') tensor(0.8300, device='cuda:0')\n",
      "tensor(0.9990, device='cuda:0') tensor(0.8080, device='cuda:0')\n",
      "tensor(0.9990, device='cuda:0') tensor(0.8690, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8040, device='cuda:0')\n",
      "tensor(0.9980, device='cuda:0') tensor(0.8290, device='cuda:0')\n",
      "tensor(0.9980, device='cuda:0') tensor(0.8250, device='cuda:0')\n",
      "tensor(0.9990, device='cuda:0') tensor(0.8110, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8580, device='cuda:0')\n",
      "tensor(0.9950, device='cuda:0') tensor(0.8420, device='cuda:0')\n",
      "tensor(0.9980, device='cuda:0') tensor(0.8220, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8230, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8340, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8280, device='cuda:0')\n",
      "tensor(0.9970, device='cuda:0') tensor(0.8340, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8340, device='cuda:0')\n",
      "tensor(0.9980, device='cuda:0') tensor(0.8390, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8440, device='cuda:0')\n",
      "tensor(0.9990, device='cuda:0') tensor(0.8460, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8490, device='cuda:0')\n",
      "tensor(0.9970, device='cuda:0') tensor(0.8380, device='cuda:0')\n",
      "Mean: 0.833, Std: 0.016\n"
     ]
    }
   ],
   "source": [
    "accuracies9 = []\n",
    "times9 = []\n",
    "\n",
    "for i in range(20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=50, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    model = SiameseMLP()\n",
    "    model.to(device)\n",
    "    #model = BaseNet()\n",
    "    train(model, train_loader, 0.001, 0, 25, verbose=False, siamese=True)\n",
    "    time2 = time.perf_counter()\n",
    "    times9.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_siamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies9.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies9).mean(), torch.tensor(accuracies9).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 9.540, Std: 0.115\n"
     ]
    }
   ],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times9).mean(), torch.tensor(times9).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9810, device='cuda:0') tensor(0.7970, device='cuda:0')\n",
      "tensor(0.9700, device='cuda:0') tensor(0.8010, device='cuda:0')\n",
      "tensor(0.9830, device='cuda:0') tensor(0.8290, device='cuda:0')\n",
      "tensor(0.9930, device='cuda:0') tensor(0.8210, device='cuda:0')\n",
      "tensor(0.9850, device='cuda:0') tensor(0.8190, device='cuda:0')\n",
      "tensor(0.9850, device='cuda:0') tensor(0.8280, device='cuda:0')\n",
      "tensor(0.9950, device='cuda:0') tensor(0.8220, device='cuda:0')\n",
      "tensor(0.9900, device='cuda:0') tensor(0.8230, device='cuda:0')\n",
      "tensor(0.9810, device='cuda:0') tensor(0.8100, device='cuda:0')\n",
      "tensor(0.9910, device='cuda:0') tensor(0.8180, device='cuda:0')\n",
      "tensor(0.9870, device='cuda:0') tensor(0.8270, device='cuda:0')\n",
      "tensor(0.9850, device='cuda:0') tensor(0.8170, device='cuda:0')\n",
      "tensor(0.9620, device='cuda:0') tensor(0.8140, device='cuda:0')\n",
      "tensor(0.9830, device='cuda:0') tensor(0.8460, device='cuda:0')\n",
      "tensor(0.9760, device='cuda:0') tensor(0.8090, device='cuda:0')\n",
      "tensor(0.9950, device='cuda:0') tensor(0.7990, device='cuda:0')\n",
      "tensor(0.9600, device='cuda:0') tensor(0.8060, device='cuda:0')\n",
      "tensor(0.9930, device='cuda:0') tensor(0.8340, device='cuda:0')\n",
      "tensor(0.9840, device='cuda:0') tensor(0.8310, device='cuda:0')\n",
      "tensor(0.9850, device='cuda:0') tensor(0.8210, device='cuda:0')\n",
      "Mean: 0.819, Std: 0.012\n"
     ]
    }
   ],
   "source": [
    "accuracies10 = []\n",
    "times10 = []\n",
    "\n",
    "for i in range(20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=50, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    #model = SiameseBaseNet()\n",
    "    #model = BaseNet()\n",
    "    model = AuxMLP()\n",
    "    model.to(device)\n",
    "    train(model, train_loader, 0.001, 0, 25, verbose=False, siamese=False, aux=True, alpha = 0.6)\n",
    "    time2 = time.perf_counter()\n",
    "    times10.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies10.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies10).mean(), torch.tensor(accuracies10).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 13.360, Std: 0.120\n"
     ]
    }
   ],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times10).mean(), torch.tensor(times10).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9740, device='cuda:0') tensor(0.8540, device='cuda:0')\n",
      "tensor(0.9830, device='cuda:0') tensor(0.8210, device='cuda:0')\n",
      "tensor(0.9830, device='cuda:0') tensor(0.8740, device='cuda:0')\n",
      "tensor(0.9780, device='cuda:0') tensor(0.8280, device='cuda:0')\n",
      "tensor(0.9690, device='cuda:0') tensor(0.8430, device='cuda:0')\n",
      "tensor(0.9840, device='cuda:0') tensor(0.8280, device='cuda:0')\n",
      "tensor(0.9680, device='cuda:0') tensor(0.8280, device='cuda:0')\n",
      "tensor(0.9820, device='cuda:0') tensor(0.8460, device='cuda:0')\n",
      "tensor(0.9750, device='cuda:0') tensor(0.8490, device='cuda:0')\n",
      "tensor(0.9640, device='cuda:0') tensor(0.8510, device='cuda:0')\n",
      "tensor(0.9810, device='cuda:0') tensor(0.8580, device='cuda:0')\n",
      "tensor(0.9960, device='cuda:0') tensor(0.8570, device='cuda:0')\n",
      "tensor(0.9910, device='cuda:0') tensor(0.8290, device='cuda:0')\n",
      "tensor(0.9520, device='cuda:0') tensor(0.8390, device='cuda:0')\n",
      "tensor(0.9730, device='cuda:0') tensor(0.8470, device='cuda:0')\n",
      "tensor(0.9820, device='cuda:0') tensor(0.8440, device='cuda:0')\n",
      "tensor(0.9870, device='cuda:0') tensor(0.8560, device='cuda:0')\n",
      "tensor(0.9860, device='cuda:0') tensor(0.8460, device='cuda:0')\n",
      "tensor(0.9800, device='cuda:0') tensor(0.8550, device='cuda:0')\n",
      "tensor(0.9390, device='cuda:0') tensor(0.8250, device='cuda:0')\n",
      "Mean: 0.844, Std: 0.014\n"
     ]
    }
   ],
   "source": [
    "accuracies11 = []\n",
    "times11 = []\n",
    "\n",
    "for i in range(20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=50, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    #model = SiameseBaseNet()\n",
    "    #model = BaseNet()\n",
    "    model = AuxsiameseMLP()\n",
    "    model.to(device)\n",
    "    train(model, train_loader, 0.001, 0, 25, verbose=False, siamese=True, aux=True, alpha = 0.5)\n",
    "    time2 = time.perf_counter()\n",
    "    times11.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies11.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies11).mean(), torch.tensor(accuracies11).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 11.084, Std: 0.091\n"
     ]
    }
   ],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times11).mean(), torch.tensor(times11).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0') tensor(0.8170, device='cuda:0')\n",
      "tensor(0.9740, device='cuda:0') tensor(0.7800, device='cuda:0')\n",
      "tensor(0.5450, device='cuda:0') tensor(0.5580, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8290, device='cuda:0')\n",
      "tensor(0.9720, device='cuda:0') tensor(0.7980, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8410, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8120, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8180, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8500, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8300, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8310, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8150, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8030, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8190, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8200, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.7960, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8280, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8320, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8300, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8340, device='cuda:0')\n",
      "Mean: 0.807, Std: 0.061\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "times = []\n",
    "\n",
    "for i in range(20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=8, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    #model = SiameseBaseNet()\n",
    "    model = BaseNet()\n",
    "    model.to(device)\n",
    "    train(model, train_loader, 0.001, 0, 25, verbose=False, siamese=False)\n",
    "    time2 = time.perf_counter()\n",
    "    times12.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies.append(te_accuracy)\n",
    "\n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies).mean(), torch.tensor(accuracies).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 12.519, Std: 0.134\n"
     ]
    }
   ],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times).mean(), torch.tensor(times).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0') tensor(0.8480, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8780, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8390, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8690, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8710, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8500, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8570, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8400, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8580, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8500, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8560, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8370, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8460, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8580, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8500, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8550, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8470, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8610, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8490, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8590, device='cuda:0')\n",
      "Mean: 0.854, Std: 0.011\n"
     ]
    }
   ],
   "source": [
    "accuracies1 = []\n",
    "times1 = []\n",
    "\n",
    "for i in range(20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=50, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    model = SiameseBaseNet()\n",
    "    model.to(device)\n",
    "    #model = BaseNet()\n",
    "    train(model, train_loader, 0.001, 0, 25, verbose=False, siamese=True)\n",
    "    time2 = time.perf_counter()\n",
    "    times1.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_siamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies1.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies1).mean(), torch.tensor(accuracies1).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 16.312, Std: 0.113\n"
     ]
    }
   ],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times1).mean(), torch.tensor(times1).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0') tensor(0.8440, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8690, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8330, device='cuda:0')\n",
      "tensor(0.9990, device='cuda:0') tensor(0.8510, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8320, device='cuda:0')\n",
      "tensor(0.9980, device='cuda:0') tensor(0.8440, device='cuda:0')\n",
      "tensor(0.9920, device='cuda:0') tensor(0.8550, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8490, device='cuda:0')\n",
      "tensor(0.9980, device='cuda:0') tensor(0.8540, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8480, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8540, device='cuda:0')\n",
      "tensor(0.9970, device='cuda:0') tensor(0.8820, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8410, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8260, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8430, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8280, device='cuda:0')\n",
      "tensor(0.9880, device='cuda:0') tensor(0.8470, device='cuda:0')\n",
      "tensor(0.9960, device='cuda:0') tensor(0.8860, device='cuda:0')\n",
      "tensor(0.9890, device='cuda:0') tensor(0.8790, device='cuda:0')\n",
      "tensor(0.9960, device='cuda:0') tensor(0.8360, device='cuda:0')\n",
      "Mean: 0.850, Std: 0.017\n"
     ]
    }
   ],
   "source": [
    "accuracies2 = []\n",
    "times2 = []\n",
    "\n",
    "for i in range(20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=50, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    #model = SiameseBaseNet()\n",
    "    #model = BaseNet()\n",
    "    model = AuxBaseNet()\n",
    "    model.to(device)\n",
    "    train(model, train_loader, 0.001, 0, 25, verbose=False, siamese=False, aux=True, alpha = 0.6)\n",
    "    time2 = time.perf_counter()\n",
    "    times2.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies2.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies2).mean(), torch.tensor(accuracies2).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 23.097, Std: 0.146\n"
     ]
    }
   ],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times2).mean(), torch.tensor(times2).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0') tensor(0.8710, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8780, device='cuda:0')\n",
      "tensor(0.9990, device='cuda:0') tensor(0.8730, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8920, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8760, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8700, device='cuda:0')\n",
      "tensor(0.9620, device='cuda:0') tensor(0.8900, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8560, device='cuda:0')\n",
      "tensor(0.9990, device='cuda:0') tensor(0.8900, device='cuda:0')\n",
      "tensor(0.9920, device='cuda:0') tensor(0.8950, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8710, device='cuda:0')\n",
      "tensor(0.9990, device='cuda:0') tensor(0.8680, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8450, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8830, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8720, device='cuda:0')\n",
      "tensor(0.9920, device='cuda:0') tensor(0.8910, device='cuda:0')\n",
      "tensor(0.9980, device='cuda:0') tensor(0.8690, device='cuda:0')\n",
      "tensor(0.9970, device='cuda:0') tensor(0.8600, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8580, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8750, device='cuda:0')\n",
      "Mean: 0.874, Std: 0.013\n"
     ]
    }
   ],
   "source": [
    "accuracies3 = []\n",
    "times3 = []\n",
    "\n",
    "for i in range(20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=50, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    #model = SiameseBaseNet()\n",
    "    #model = BaseNet()\n",
    "    model = AuxsiameseBaseNet()\n",
    "    model.to(device)\n",
    "    train(model, train_loader, 0.001, 0, 25, verbose=False, siamese=True, aux=True, alpha = 0.7)\n",
    "    time2 = time.perf_counter()\n",
    "    times3.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies3.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies3).mean(), torch.tensor(accuracies3).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 18.741, Std: 0.118\n"
     ]
    }
   ],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times3).mean(), torch.tensor(times3).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0') tensor(0.8130, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8380, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8290, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8140, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8420, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8370, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8210, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8250, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8410, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8340, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8530, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8290, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8370, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8230, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8480, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8410, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8580, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8170, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8270, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8050, device='cuda:0')\n",
      "Mean: 0.832, Std: 0.014\n"
     ]
    }
   ],
   "source": [
    "accuracies4 = []\n",
    "times4 = []\n",
    "\n",
    "for i in range(20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=50, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    #model = SiameseBaseNet()\n",
    "    #model = BaseNet()\n",
    "    model = ResNet(nb_residual_blocks = 4, input_channels = 2, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "    model.to(device)\n",
    "    train(model, train_loader, 0.001, 0, 25, verbose=False, siamese=False)\n",
    "    time2 = time.perf_counter()\n",
    "    times4.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies4.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies4).mean(), torch.tensor(accuracies4).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 38.713, Std: 0.151\n"
     ]
    }
   ],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times4).mean(), torch.tensor(times4).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0') tensor(0.8620, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8580, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8730, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8670, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8600, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8600, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8570, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8520, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8830, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8770, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8900, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8630, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8540, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8840, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8750, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8510, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8610, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8660, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8690, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8480, device='cuda:0')\n",
      "Mean: 0.866, Std: 0.012\n"
     ]
    }
   ],
   "source": [
    "accuracies5 = []\n",
    "times5 = []\n",
    "\n",
    "for i in range(20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=50, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    model = SiameseResNet(nb_residual_blocks = 4, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "    #model = BaseNet()\n",
    "    model.to(device)\n",
    "    train(model, train_loader, 0.001, 0, 25, verbose=False, siamese=True)\n",
    "    time2 = time.perf_counter()\n",
    "    times5.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_siamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies5.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies5).mean(), torch.tensor(accuracies5).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 53.883, Std: 0.104\n"
     ]
    }
   ],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times5).mean(), torch.tensor(times5).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9870, device='cuda:0') tensor(0.8550, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8690, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8760, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8690, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8790, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8800, device='cuda:0')\n",
      "tensor(0.9990, device='cuda:0') tensor(0.8590, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.9010, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8720, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8780, device='cuda:0')\n",
      "tensor(0.9950, device='cuda:0') tensor(0.8660, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8760, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8860, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8740, device='cuda:0')\n",
      "tensor(0.9990, device='cuda:0') tensor(0.8920, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8590, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8710, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8830, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8830, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8850, device='cuda:0')\n",
      "Mean: 0.876, Std: 0.011\n"
     ]
    }
   ],
   "source": [
    "accuracies6 = []\n",
    "times6 = []\n",
    "\n",
    "for i in range(20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=50, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    #model = SiameseBaseNet()\n",
    "    #model = BaseNet()\n",
    "    model = AuxResNet(nb_residual_blocks = 10, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "    model.to(device)\n",
    "    train(model, train_loader, 0.001, 0, 25, verbose=False, siamese=False, aux=True, alpha = 0.9)\n",
    "    time2 = time.perf_counter()\n",
    "    times6.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies6.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies6).mean(), torch.tensor(accuracies6).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 164.991, Std: 0.467\n"
     ]
    }
   ],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times6).mean(), torch.tensor(times6).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0') tensor(0.8870, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8870, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8780, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8800, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8830, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8790, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8850, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.9090, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8820, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8700, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8770, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.9000, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8790, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8940, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.9070, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8830, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8930, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8840, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8740, device='cuda:0')\n",
      "tensor(1., device='cuda:0') tensor(0.8840, device='cuda:0')\n",
      "Mean: 0.886, Std: 0.010\n"
     ]
    }
   ],
   "source": [
    "accuracies7 = []\n",
    "times7 = []\n",
    "\n",
    "for i in range(20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=50, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    #model = SiameseBaseNet()\n",
    "    #model = BaseNet()\n",
    "    model = AuxsiameseResNet(nb_residual_blocks = 4, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "    model.to(device)\n",
    "    train(model, train_loader, 0.001, 0, 25, verbose=False, siamese=True, aux=True, alpha = 0.7)\n",
    "    time2 = time.perf_counter()\n",
    "    times7.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies7.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies7).mean(), torch.tensor(accuracies7).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 58.627, Std: 0.095\n"
     ]
    }
   ],
   "source": [
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(times7).mean(), torch.tensor(times7).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8014, 0.8035, 0.8015, 0.8056, 0.8019],\n",
      "        [0.8044, 0.8070, 0.8068, 0.8042, 0.8011],\n",
      "        [0.8072, 0.8071, 0.8053, 0.8004, 0.7953],\n",
      "        [0.7977, 0.7913, 0.7888, 0.7791, 0.7683]])\n"
     ]
    }
   ],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies1 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = MLP()\n",
    "            model.to(device)\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=False)\n",
    "            te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies1[j,k] =  torch.cuda.FloatTensor(accurate).mean()\n",
    "print(test_accuracies1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8493, 0.8444, 0.8468, 0.8429, 0.8374],\n",
      "        [0.8423, 0.8384, 0.8367, 0.8363, 0.8296],\n",
      "        [0.8406, 0.8350, 0.8342, 0.8300, 0.8140],\n",
      "        [0.8195, 0.8094, 0.7905, 0.7710, 0.7423]])\n"
     ]
    }
   ],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies2 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = SiameseMLP()\n",
    "            model.to(device)\n",
    "            #model = BaseNet()\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=True)\n",
    "            te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies2[j,k] =  torch.cuda.FloatTensor(accurate).mean()\n",
    "print(test_accuracies2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8184, 0.8225, 0.8223, 0.8176, 0.8120],\n",
      "        [0.8139, 0.8119, 0.8114, 0.8063, 0.8106],\n",
      "        [0.8130, 0.8087, 0.8095, 0.8112, 0.8017],\n",
      "        [0.8060, 0.7966, 0.7876, 0.7730, 0.7571]])\n"
     ]
    }
   ],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies3 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = AuxMLP()\n",
    "            model.to(device)\n",
    "            #model = BaseNet()\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=False, aux=True, alpha = 0.0)\n",
    "            te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies3[j,k] =  torch.cuda.FloatTensor(accurate).mean()\n",
    "print(test_accuracies3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7246, 0.7686, 0.7808, 0.8034, 0.8205],\n",
      "        [0.8241, 0.8299, 0.8253, 0.8273, 0.8256],\n",
      "        [0.8287, 0.8311, 0.8293, 0.8291, 0.8189],\n",
      "        [0.8303, 0.8192, 0.7826, 0.7837, 0.7460]])\n"
     ]
    }
   ],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies5 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = BaseNet()\n",
    "            model.to(device)\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=False)\n",
    "            te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies5[j,k] =  torch.cuda.FloatTensor(accurate).mean()\n",
    "print(test_accuracies5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8408, 0.8178, 0.8411, 0.8424, 0.8413],\n",
      "        [0.8546, 0.8564, 0.8523, 0.8487, 0.8513],\n",
      "        [0.8520, 0.8536, 0.8530, 0.8507, 0.8529],\n",
      "        [0.8486, 0.8511, 0.8439, 0.8258, 0.7881]])\n"
     ]
    }
   ],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies6 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = SiameseBaseNet()\n",
    "            model.to(device)\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=True)\n",
    "            te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies6[j,k] =  torch.cuda.FloatTensor(accurate).mean()\n",
    "print(test_accuracies6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7255, 0.7897, 0.8042, 0.7796, 0.8148],\n",
      "        [0.8354, 0.8351, 0.8372, 0.8389, 0.8329],\n",
      "        [0.8410, 0.8391, 0.8380, 0.8347, 0.8325],\n",
      "        [0.8324, 0.8310, 0.8204, 0.8055, 0.7755]])\n"
     ]
    }
   ],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies7 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = AuxBaseNet()\n",
    "            model.to(device)\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=False, aux=True, alpha = 0.0)\n",
    "            te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies7[j,k] =  torch.cuda.FloatTensor(accurate).mean()\n",
    "print(test_accuracies7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8262, 0.8285, 0.8360, 0.8349, 0.8226],\n",
      "        [0.8398, 0.8384, 0.8403, 0.8343, 0.8302],\n",
      "        [0.8354, 0.8340, 0.8184, 0.8279, 0.8218],\n",
      "        [0.8152, 0.8161, 0.8157, 0.8150, 0.8147]])\n"
     ]
    }
   ],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies9 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = ResNet(nb_residual_blocks = 4, input_channels = 2, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "            model.to(device)\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=False)\n",
    "            te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies9[j,k] =  torch.cuda.FloatTensor(accurate).mean()\n",
    "print(test_accuracies9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8643, 0.8631, 0.8770, 0.8667, 0.8742],\n",
      "        [0.8735, 0.8755, 0.8733, 0.8703, 0.8643],\n",
      "        [0.8751, 0.8685, 0.8687, 0.8601, 0.8569],\n",
      "        [0.8529, 0.8498, 0.8487, 0.8552, 0.8570]])\n"
     ]
    }
   ],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies10 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = SiameseResNet(nb_residual_blocks = 4, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "            model.to(device)\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=True)\n",
    "            te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies10[j,k] =  torch.cuda.FloatTensor(accurate).mean()\n",
    "print(test_accuracies10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\laboleb\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8342, 0.8438, 0.8438, 0.8465, 0.8444],\n",
      "        [0.8453, 0.8489, 0.8476, 0.8429, 0.8363],\n",
      "        [0.8432, 0.8423, 0.8421, 0.8375, 0.8292],\n",
      "        [0.8244, 0.8290, 0.8252, 0.8305, 0.8376]])\n"
     ]
    }
   ],
   "source": [
    "gammas = [5e-3, 1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "test_accuracies11 = torch.empty((len(gammas), len(batch_sizes)))\n",
    "\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(batch_sizes)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            model = AuxResNet(nb_residual_blocks = 4, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "            model.to(device)\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=batch_sizes[k], seed=i)\n",
    "            train(model, train_loader, gammas[j], 0, 25, verbose=False, siamese=False, aux=True, alpha = 0.0)\n",
    "            te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        test_accuracies11[j,k] =  torch.cuda.FloatTensor(accurate).mean()\n",
    "print(test_accuracies11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(11):\n",
    "    accuracies100 = []\n",
    "\n",
    "    for i in range(10):\n",
    "        train_loader, test_loader = load_data(N=1000, batch_size=16, seed=i)\n",
    "\n",
    "        #model = SiameseBaseNet()\n",
    "        #model = BaseNet()\n",
    "        model = AuxMLP()()\n",
    "        train(model, train_loader, 5e-3, 0, 25, verbose=False, siamese=False, aux=True, alpha = j/10)\n",
    "\n",
    "        te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "        accuracies100.append(te_accuracy)\n",
    "    print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies100).mean(), torch.tensor(accuracies100).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(11):\n",
    "    accuracies101 = []\n",
    "\n",
    "    for i in range(10):\n",
    "        train_loader, test_loader = load_data(N=1000, batch_size=8, seed=i)\n",
    "\n",
    "        #model = SiameseBaseNet()\n",
    "        #model = BaseNet()\n",
    "        model = AuxsiameseMLP()\n",
    "        train(model, train_loader, 5e-3, 0, 25, verbose=False, siamese=False, aux=True, alpha = j/10)\n",
    "\n",
    "        te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "        accuracies101.append(te_accuracy)\n",
    "    print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies101).mean(), torch.tensor(accuracies101).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(11):\n",
    "    accuracies102 = []\n",
    "\n",
    "    for i in range(10):\n",
    "        train_loader, test_loader = load_data(N=1000, batch_size=16, seed=i)\n",
    "\n",
    "        #model = SiameseBaseNet()\n",
    "        #model = BaseNet()\n",
    "        model = AuxsiameseBaseNet()\n",
    "        train(model, train_loader, 1e-3, 0, 25, verbose=False, siamese=False, aux=True, alpha = j/10)\n",
    "\n",
    "        te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "        accuracies102.append(te_accuracy)\n",
    "    print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies102).mean(), torch.tensor(accuracies102).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(11):\n",
    "    accuracies103 = []\n",
    "\n",
    "    for i in range(10):\n",
    "        train_loader, test_loader = load_data(N=1000, batch_size=8, seed=i)\n",
    "\n",
    "        #model = SiameseBaseNet()\n",
    "        #model = BaseNet()\n",
    "        model = AuxBaseNet()\n",
    "        train(model, train_loader, 5e-4, 0, 25, verbose=False, siamese=False, aux=True, alpha = j/10)\n",
    "\n",
    "        te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "        accuracies103.append(te_accuracy)\n",
    "    print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies103).mean(), torch.tensor(accuracies103).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(11):\n",
    "    accuracies104 = []\n",
    "\n",
    "    for i in range(10):\n",
    "        train_loader, test_loader = load_data(N=1000, batch_size=16, seed=i)\n",
    "\n",
    "        #model = SiameseBaseNet()\n",
    "        #model = BaseNet()\n",
    "        model = AuxsiameseResNet(nb_residual_blocks = 4, input_channels = 1, nb_channels = 32, kernel_size = 3, nb_classes = 2)\n",
    "        train(model, train_loader, 1e-3, 0, 25, verbose=False, siamese=False, aux=True, alpha = j/10)\n",
    "\n",
    "        te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "        accuracies104.append(te_accuracy)\n",
    "    print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies104).mean(), torch.tensor(accuracies104).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(11):\n",
    "    accuracies105 = []\n",
    "\n",
    "    for i in range(10):\n",
    "        train_loader, test_loader = load_data(N=1000, batch_size=8, seed=i)\n",
    "\n",
    "        #model = SiameseBaseNet()\n",
    "        #model = BaseNet()\n",
    "        model = AuxsiameseMLP()\n",
    "        train(model, train_loader, 5e-3, 0, 25, verbose=False, siamese=False, aux=True, alpha = j/10)\n",
    "\n",
    "        te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "        accuracies105.append(te_accuracy)\n",
    "    print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies105).mean(), torch.tensor(accuracies105).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-262-64130ef26583>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m#model = BaseNet()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAuxBaseNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msiamese\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maux\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mtime2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mtimes3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime2\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtime1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-243-62b9f86c55a4>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, eta, decay, n_epochs, verbose, siamese, aux, alpha)\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0maux\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                 \u001b[0mtrain_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m                 \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maux1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maux2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_64\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-215-593e0f3abc1f>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x1, x2)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv11\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# size [nb, 32, 5, 5]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv21\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# size [nb, 64, 2, 2]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_64\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_64\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_64\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    419\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[1;32m--> 420\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
