{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import data_loader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import dlc_practical_prologue as prologue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper.py\n",
    "\n",
    "# Count the number of parameters\n",
    "def count_param(model):\n",
    "    return sum([torch.numel(param) for param in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = load_data(N=1000, batch_size=50, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, data_loader):\n",
    "\n",
    "    nb_data_errors = 0\n",
    "\n",
    "    for data_input, data_target, data_classes in data_loader:\n",
    "        data_target = torch.nn.functional.one_hot(data_target)\n",
    "        output = model(data_input)\n",
    "        nb_error = torch.sum(torch.argmax(output, dim=1, keepdim=True) != torch.argmax(data_target, dim=1, keepdim=True))\n",
    "        nb_data_errors += nb_error\n",
    "        \n",
    "    return nb_data_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors_siamese(model, data_loader):\n",
    "\n",
    "    nb_data_errors = 0\n",
    "    for data_input, data_target, data_classes in data_loader:\n",
    "        data_1, data_2 = data_input.unbind(1)               \n",
    "        output = model(data_1.unsqueeze(1), data_2.unsqueeze(1))\n",
    "        data_target = torch.nn.functional.one_hot(data_target)\n",
    "        nb_error = torch.sum(torch.argmax(output, dim=1, keepdim=True) != torch.argmax(data_target, dim=1, keepdim=True))\n",
    "        nb_data_errors += nb_error\n",
    "        \n",
    "    return nb_data_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors_auxsiamese(model, data_loader):\n",
    "\n",
    "    nb_data_errors = 0\n",
    "    for data_input, data_target, data_classes in data_loader:\n",
    "        data_1, data_2 = data_input.unbind(1)               \n",
    "        output, aux1, aux2 = model(data_1.unsqueeze(1), data_2.unsqueeze(1))\n",
    "        data_target = torch.nn.functional.one_hot(data_target)\n",
    "        nb_error = torch.sum(torch.argmax(output, dim=1, keepdim=True) != torch.argmax(data_target, dim=1, keepdim=True))\n",
    "        nb_data_errors += nb_error\n",
    "        \n",
    "    return nb_data_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 32, kernel_size=5)    # size [nb, 32, 10, 10]\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=2)   # size [nb, 64, 4, 4]\n",
    "        self.fc1 = nn.Linear(256, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "        self.fc3 = nn.Linear(10, 2)\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2)) # size [nb, 32, 5, 5]      \n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2)) # size [nb, 64, 2, 2]\n",
    "        x = x.view(-1, 256) # size [nb, 256]\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseBaseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseBaseNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)    # size [nb, 32, 10, 10]\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=2)   # size [nb, 64, 4, 4]\n",
    "        self.fc1 = nn.Linear(256, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "        self.fc3 = nn.Linear(20, 2)\n",
    "        \n",
    "    def convs(self, x):        \n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2)) # size [nb, 32, 5, 5]      \n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2)) # size [nb, 64, 2, 2]\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.convs(x1)\n",
    "        x1 = x1.view(-1, 256)\n",
    "        x1 = F.relu((self.fc1(x1)))\n",
    "        x1 = F.relu(self.fc2(x1))\n",
    "        \n",
    "        x2 = self.convs(x2)\n",
    "        x2 = x2.view(-1, 256)\n",
    "        x2 = F.relu(self.fc1(x2))\n",
    "        x2 = F.relu(self.fc2(x2))\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        #x = torch.abs(x1 - x2)\n",
    "        #x = F.relu(self.fc1(x.flatten(start_dim=1)))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        \n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxsiameseBaseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AuxsiameseBaseNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)    # size [nb, 32, 10, 10]\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=2)   # size [nb, 64, 4, 4]\n",
    "        self.fc1 = nn.Linear(256, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "        self.fc3 = nn.Linear(20, 2)\n",
    "        \n",
    "    def convs(self, x):        \n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2)) # size [nb, 32, 5, 5]      \n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2)) # size [nb, 64, 2, 2]\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.convs(x1)\n",
    "        x1 = x1.view(-1, 256)\n",
    "        x1 = F.relu((self.fc1(x1)))\n",
    "        x1 = F.relu(self.fc2(x1))\n",
    "        \n",
    "        x2 = self.convs(x2)\n",
    "        x2 = x2.view(-1, 256)\n",
    "        x2 = F.relu(self.fc1(x2))\n",
    "        x2 = F.relu(self.fc2(x2))\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        aux1 = F.softmax(x1)\n",
    "        aux2 = F.softmax(x2)\n",
    "        #x = torch.abs(x1 - x2)\n",
    "        #x = F.relu(self.fc1(x.flatten(start_dim=1)))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        \n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        \n",
    "        return x, aux1, aux2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxBaseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AuxBaseNet, self).__init__()\n",
    "        self.conv11 = nn.Conv2d(1, 32, kernel_size=5)    # size [nb, 32, 10, 10]\n",
    "        self.conv21 = nn.Conv2d(32, 64, kernel_size=2)   # size [nb, 64, 4, 4]\n",
    "        self.fc11 = nn.Linear(256, 200)\n",
    "        self.fc21 = nn.Linear(200, 10)\n",
    "        self.conv12 = nn.Conv2d(1, 32, kernel_size=5)    # size [nb, 32, 10, 10]\n",
    "        self.conv22 = nn.Conv2d(32, 64, kernel_size=2)   # size [nb, 64, 4, 4]\n",
    "        self.fc12 = nn.Linear(256, 200)\n",
    "        self.fc22 = nn.Linear(200, 10)\n",
    "        self.fc3 = nn.Linear(20, 2)\n",
    "        \n",
    "    def convs(self, x):        \n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2)) # size [nb, 32, 5, 5]      \n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2)) # size [nb, 64, 2, 2]\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = F.relu(F.max_pool2d(self.conv11(x1), kernel_size=2)) # size [nb, 32, 5, 5]  \n",
    "        x1 = F.relu(F.max_pool2d(self.conv21(x1), kernel_size=2)) # size [nb, 64, 2, 2]\n",
    "        x1 = x1.view(-1, 256)\n",
    "        x1 = F.relu((self.fc11(x1)))\n",
    "        x1 = F.relu(self.fc21(x1))\n",
    "        \n",
    "        x2 = F.relu(F.max_pool2d(self.conv12(x2), kernel_size=2)) # size [nb, 32, 5, 5]  \n",
    "        x2 = F.relu(F.max_pool2d(self.conv22(x2), kernel_size=2)) # size [nb, 64, 2, 2]\n",
    "        x2 = x2.view(-1, 256)\n",
    "        x2 = F.relu((self.fc12(x2)))\n",
    "        x2 = F.relu(self.fc22(x2))\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        aux1 = F.softmax(x1)\n",
    "        aux2 = F.softmax(x2)\n",
    "        #x = torch.abs(x1 - x2)\n",
    "        #x = F.relu(self.fc1(x.flatten(start_dim=1)))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        \n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        \n",
    "        return x, aux1, aux2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63320"
      ]
     },
     "execution_count": 733,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BaseNet()\n",
    "\n",
    "# Calculate the number of parameters in the model\n",
    "count_param(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62540"
      ]
     },
     "execution_count": 734,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SiameseBaseNet()\n",
    "\n",
    "# Calculate the number of parameters in the model\n",
    "count_param(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62540"
      ]
     },
     "execution_count": 735,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AuxsiameseBaseNet()\n",
    "\n",
    "# Calculate the number of parameters in the model\n",
    "count_param(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125038"
      ]
     },
     "execution_count": 736,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AuxBaseNet()\n",
    "\n",
    "# Calculate the number of parameters in the model\n",
    "count_param(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, eta, decay, n_epochs=25, verbose=False, siamese=False, aux=False, alpha = 0):\n",
    "\n",
    "    #binary_crit = nn.CrossEntropyLoss()\n",
    "    binary_crit = torch.nn.BCELoss()\n",
    "    aux_crit = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=eta, weight_decay=decay)\n",
    "    \n",
    "    tr_losses = []\n",
    "    tr_accuracies = []\n",
    "\n",
    "    for e in range(n_epochs):\n",
    "        # Reset training/validation loss\n",
    "        tr_loss = 0\n",
    "\n",
    "        # Training model\n",
    "        model.train()\n",
    "\n",
    "        for train_input, train_target, train_classes in iter(train_loader):\n",
    "            train_target = torch.nn.functional.one_hot(train_target)\n",
    "            # Forward pass\n",
    "            \n",
    "            if siamese == True:\n",
    "                train_1, train_2 = train_input.unbind(1)\n",
    "                if aux == True:\n",
    "                    output, aux1, aux2 = model(train_1.unsqueeze(1), train_2.unsqueeze(1))\n",
    "                else:\n",
    "                    output = model(train_1.unsqueeze(1), train_2.unsqueeze(1))\n",
    "            elif aux == True:\n",
    "                train_1, train_2 = train_input.unbind(1)\n",
    "                output, aux1, aux2 = model(train_1.unsqueeze(1), train_2.unsqueeze(1))\n",
    "            else:\n",
    "                output = model(train_input)\n",
    "                \n",
    "            # Binary classification loss\n",
    "            binary_loss = binary_crit(output, train_target.float())\n",
    "            total_loss = binary_loss\n",
    "            \n",
    "            # Auxiliary loss\n",
    "            if aux == True:\n",
    "\n",
    "                aux_loss1 = aux_crit(aux1, train_classes[:,0])\n",
    "                aux_loss2 = aux_crit(aux2, train_classes[:,1])\n",
    "                aux_loss = aux_loss1 + aux_loss2\n",
    "                total_loss = binary_loss + aux_loss * alpha\n",
    "        \n",
    "            # Total loss = Binary loss + aux loss * alpha\n",
    "            \n",
    "            tr_loss += total_loss\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Collect accuracy data\n",
    "        # tr_accuracies.append(compute_nb_errors_siamese(model, train_loader)/1000)\n",
    "\n",
    "        # Collect loss data\n",
    "        tr_losses.append(tr_loss)\n",
    "\n",
    "        if verbose:\n",
    "            print('Epoch %d/%d, Binary loss: %.3f' %\n",
    "                  (e+1, n_epochs, tr_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Binary loss: 13.576\n",
      "Epoch 2/25, Binary loss: 11.614\n",
      "Epoch 3/25, Binary loss: 9.312\n",
      "Epoch 4/25, Binary loss: 7.733\n",
      "Epoch 5/25, Binary loss: 6.548\n",
      "Epoch 6/25, Binary loss: 5.634\n",
      "Epoch 7/25, Binary loss: 4.718\n",
      "Epoch 8/25, Binary loss: 3.844\n",
      "Epoch 9/25, Binary loss: 2.893\n",
      "Epoch 10/25, Binary loss: 2.243\n",
      "Epoch 11/25, Binary loss: 1.638\n",
      "Epoch 12/25, Binary loss: 1.064\n",
      "Epoch 13/25, Binary loss: 0.753\n",
      "Epoch 14/25, Binary loss: 0.469\n",
      "Epoch 15/25, Binary loss: 0.279\n",
      "Epoch 16/25, Binary loss: 0.192\n",
      "Epoch 17/25, Binary loss: 0.136\n",
      "Epoch 18/25, Binary loss: 0.103\n",
      "Epoch 19/25, Binary loss: 0.081\n",
      "Epoch 20/25, Binary loss: 0.065\n",
      "Epoch 21/25, Binary loss: 0.054\n",
      "Epoch 22/25, Binary loss: 0.046\n",
      "Epoch 23/25, Binary loss: 0.040\n",
      "Epoch 24/25, Binary loss: 0.035\n",
      "Epoch 25/25, Binary loss: 0.031\n",
      "Spend 1.018782e+01 s\n",
      "tensor(1.) tensor(0.8500)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "time1 = time.perf_counter()\n",
    "model = SiameseBaseNet()\n",
    "#model = BaseNet()\n",
    "train(model, train_loader, 0.001, 0, 25, verbose=True, siamese=True)\n",
    "time2 = time.perf_counter()\n",
    "print('Spend {:e} s'.format(time2 - time1))\n",
    "\n",
    "tr_accuracy = 1 - compute_nb_errors_siamese(model, train_loader)/1000\n",
    "te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "print(tr_accuracy, te_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Binary loss: 13.564\n",
      "Epoch 2/25, Binary loss: 12.049\n",
      "Epoch 3/25, Binary loss: 10.678\n",
      "Epoch 4/25, Binary loss: 9.086\n",
      "Epoch 5/25, Binary loss: 7.613\n",
      "Epoch 6/25, Binary loss: 6.530\n",
      "Epoch 7/25, Binary loss: 5.443\n",
      "Epoch 8/25, Binary loss: 4.033\n",
      "Epoch 9/25, Binary loss: 2.860\n",
      "Epoch 10/25, Binary loss: 1.917\n",
      "Epoch 11/25, Binary loss: 1.299\n",
      "Epoch 12/25, Binary loss: 0.851\n",
      "Epoch 13/25, Binary loss: 0.497\n",
      "Epoch 14/25, Binary loss: 0.293\n",
      "Epoch 15/25, Binary loss: 0.174\n",
      "Epoch 16/25, Binary loss: 0.119\n",
      "Epoch 17/25, Binary loss: 0.092\n",
      "Epoch 18/25, Binary loss: 0.074\n",
      "Epoch 19/25, Binary loss: 0.060\n",
      "Epoch 20/25, Binary loss: 0.051\n",
      "Epoch 21/25, Binary loss: 0.043\n",
      "Epoch 22/25, Binary loss: 0.039\n",
      "Epoch 23/25, Binary loss: 0.032\n",
      "Epoch 24/25, Binary loss: 0.028\n",
      "Epoch 25/25, Binary loss: 0.026\n",
      "Spend 5.444853e+00 s\n",
      "tensor(1.) tensor(0.8280)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "number = torch.randint(1,50,(1,))\n",
    "train_loader, test_loader = load_data(N=1000, batch_size=50, seed=number)\n",
    "time1 = time.perf_counter()\n",
    "#model = SiameseBaseNet()\n",
    "model = BaseNet()\n",
    "train(model, train_loader, 0.001, 0, 25, verbose=True, siamese=False)\n",
    "time2 = time.perf_counter()\n",
    "print('Spend {:e} s'.format(time2 - time1))\n",
    "\n",
    "tr_accuracy = 1 - compute_nb_errors(model, train_loader)/1000\n",
    "te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "print(tr_accuracy, te_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-673-21efc383ef87>:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  aux1 = F.softmax(x1)\n",
      "<ipython-input-673-21efc383ef87>:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  aux2 = F.softmax(x2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Binary loss: 41.301\n",
      "Epoch 2/25, Binary loss: 40.527\n",
      "Epoch 3/25, Binary loss: 39.194\n",
      "Epoch 4/25, Binary loss: 37.589\n",
      "Epoch 5/25, Binary loss: 35.413\n",
      "Epoch 6/25, Binary loss: 33.461\n",
      "Epoch 7/25, Binary loss: 32.224\n",
      "Epoch 8/25, Binary loss: 31.236\n",
      "Epoch 9/25, Binary loss: 30.435\n",
      "Epoch 10/25, Binary loss: 29.611\n",
      "Epoch 11/25, Binary loss: 29.025\n",
      "Epoch 12/25, Binary loss: 28.527\n",
      "Epoch 13/25, Binary loss: 28.140\n",
      "Epoch 14/25, Binary loss: 27.689\n",
      "Epoch 15/25, Binary loss: 27.367\n",
      "Epoch 16/25, Binary loss: 27.219\n",
      "Epoch 17/25, Binary loss: 27.102\n",
      "Epoch 18/25, Binary loss: 26.959\n",
      "Epoch 19/25, Binary loss: 26.890\n",
      "Epoch 20/25, Binary loss: 26.829\n",
      "Epoch 21/25, Binary loss: 26.785\n",
      "Epoch 22/25, Binary loss: 26.759\n",
      "Epoch 23/25, Binary loss: 26.788\n",
      "Epoch 24/25, Binary loss: 26.739\n",
      "Epoch 25/25, Binary loss: 26.726\n",
      "Spend 1.036312e+01 s\n",
      "tensor(1.) tensor(0.8700)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "number = torch.randint(1,50,(1,))\n",
    "train_loader, test_loader = load_data(N=1000, batch_size=50, seed=number)\n",
    "time1 = time.perf_counter()\n",
    "#model = SiameseBaseNet()\n",
    "#model = BaseNet()\n",
    "model = AuxsiameseBaseNet()\n",
    "train(model, train_loader, 0.001, 0, 25, verbose=True, siamese=True, aux=True, alpha = 0.3)\n",
    "time2 = time.perf_counter()\n",
    "print('Spend {:e} s'.format(time2 - time1))\n",
    "\n",
    "tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "print(tr_accuracy, te_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-699-f890ddbc2f26>:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  aux1 = F.softmax(x1)\n",
      "<ipython-input-699-f890ddbc2f26>:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  aux2 = F.softmax(x2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Binary loss: 41.354\n",
      "Epoch 2/25, Binary loss: 40.233\n",
      "Epoch 3/25, Binary loss: 38.545\n",
      "Epoch 4/25, Binary loss: 36.119\n",
      "Epoch 5/25, Binary loss: 34.638\n",
      "Epoch 6/25, Binary loss: 32.981\n",
      "Epoch 7/25, Binary loss: 32.049\n",
      "Epoch 8/25, Binary loss: 31.040\n",
      "Epoch 9/25, Binary loss: 30.221\n",
      "Epoch 10/25, Binary loss: 29.890\n",
      "Epoch 11/25, Binary loss: 29.816\n",
      "Epoch 12/25, Binary loss: 28.891\n",
      "Epoch 13/25, Binary loss: 28.312\n",
      "Epoch 14/25, Binary loss: 27.903\n",
      "Epoch 15/25, Binary loss: 27.736\n",
      "Epoch 16/25, Binary loss: 27.181\n",
      "Epoch 17/25, Binary loss: 26.733\n",
      "Epoch 18/25, Binary loss: 26.162\n",
      "Epoch 19/25, Binary loss: 26.032\n",
      "Epoch 20/25, Binary loss: 25.755\n",
      "Epoch 21/25, Binary loss: 25.475\n",
      "Epoch 22/25, Binary loss: 25.369\n",
      "Epoch 23/25, Binary loss: 25.333\n",
      "Epoch 24/25, Binary loss: 25.143\n",
      "Epoch 25/25, Binary loss: 25.044\n",
      "Spend 9.934680e+00 s\n",
      "tensor(1.) tensor(0.8390)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "number = torch.randint(1,50,(1,))\n",
    "train_loader, test_loader = load_data(N=1000, batch_size=50, seed=number)\n",
    "time1 = time.perf_counter()\n",
    "#model = SiameseBaseNet()\n",
    "#model = BaseNet()\n",
    "model = AuxBaseNet()\n",
    "train(model, train_loader, 0.001, 0, 25, verbose=True, siamese=False, aux=True, alpha = 0.3)\n",
    "time2 = time.perf_counter()\n",
    "print('Spend {:e} s'.format(time2 - time1))\n",
    "\n",
    "tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "print(tr_accuracy, te_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.) tensor(0.8210)\n",
      "tensor(1.) tensor(0.8240)\n",
      "tensor(1.) tensor(0.8120)\n",
      "tensor(1.) tensor(0.8180)\n",
      "tensor(0.9990) tensor(0.8260)\n",
      "tensor(1.) tensor(0.8080)\n",
      "tensor(1.) tensor(0.8110)\n",
      "tensor(0.9990) tensor(0.8330)\n",
      "tensor(1.) tensor(0.8210)\n",
      "tensor(1.) tensor(0.8020)\n",
      "tensor(1.) tensor(0.8190)\n",
      "tensor(1.) tensor(0.8220)\n",
      "tensor(1.) tensor(0.8370)\n",
      "tensor(1.) tensor(0.8180)\n",
      "tensor(1.) tensor(0.8280)\n",
      "tensor(1.) tensor(0.8410)\n",
      "tensor(1.) tensor(0.8280)\n",
      "tensor(1.) tensor(0.8300)\n",
      "tensor(0.9990) tensor(0.8240)\n",
      "tensor(1.) tensor(0.8010)\n",
      "Mean: 0.821, Std: 0.011\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "times = []\n",
    "\n",
    "for i in range(20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=50, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    #model = SiameseBaseNet()\n",
    "    model = BaseNet()\n",
    "    train(model, train_loader, 0.001, 0, 25, verbose=False, siamese=False)\n",
    "    time2 = time.perf_counter()\n",
    "    times.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies.append(te_accuracy)\n",
    "\n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies).mean(), torch.tensor(accuracies).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.) tensor(0.8410)\n",
      "tensor(1.) tensor(0.8490)\n",
      "tensor(1.) tensor(0.8380)\n",
      "tensor(1.) tensor(0.8480)\n",
      "tensor(1.) tensor(0.8600)\n",
      "tensor(1.) tensor(0.8450)\n",
      "tensor(1.) tensor(0.8590)\n",
      "tensor(1.) tensor(0.8410)\n",
      "tensor(1.) tensor(0.8490)\n",
      "tensor(1.) tensor(0.8570)\n",
      "tensor(1.) tensor(0.8600)\n",
      "tensor(1.) tensor(0.8430)\n",
      "tensor(1.) tensor(0.8550)\n",
      "tensor(1.) tensor(0.8360)\n",
      "tensor(1.) tensor(0.8530)\n",
      "tensor(1.) tensor(0.8470)\n",
      "tensor(1.) tensor(0.8630)\n",
      "tensor(1.) tensor(0.8550)\n",
      "tensor(1.) tensor(0.8600)\n",
      "tensor(1.) tensor(0.8500)\n",
      "Mean: 0.850, Std: 0.008\n"
     ]
    }
   ],
   "source": [
    "accuracies1 = []\n",
    "times1 = []\n",
    "\n",
    "for i in range(20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=50, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    model = SiameseBaseNet()\n",
    "    #model = BaseNet()\n",
    "    train(model, train_loader, 0.001, 0, 25, verbose=False, siamese=True)\n",
    "    time2 = time.perf_counter()\n",
    "    times1.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_siamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_siamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies1.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies1).mean(), torch.tensor(accuracies1).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-699-f890ddbc2f26>:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  aux1 = F.softmax(x1)\n",
      "<ipython-input-699-f890ddbc2f26>:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  aux2 = F.softmax(x2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9980) tensor(0.8340)\n",
      "tensor(0.9870) tensor(0.8420)\n",
      "tensor(0.9980) tensor(0.8400)\n",
      "tensor(0.9950) tensor(0.8120)\n",
      "tensor(1.) tensor(0.8480)\n",
      "tensor(0.9970) tensor(0.8580)\n",
      "tensor(0.9800) tensor(0.8440)\n",
      "tensor(1.) tensor(0.8550)\n",
      "tensor(0.9980) tensor(0.8630)\n",
      "tensor(0.9990) tensor(0.8500)\n",
      "tensor(0.9840) tensor(0.8540)\n",
      "tensor(0.9970) tensor(0.8350)\n",
      "tensor(1.) tensor(0.8450)\n",
      "tensor(0.9980) tensor(0.8360)\n",
      "tensor(0.9790) tensor(0.8440)\n",
      "tensor(1.) tensor(0.8730)\n",
      "tensor(0.9960) tensor(0.8250)\n",
      "tensor(1.) tensor(0.8390)\n",
      "tensor(0.9980) tensor(0.8430)\n",
      "tensor(0.9940) tensor(0.8150)\n",
      "Mean: 0.843, Std: 0.015\n"
     ]
    }
   ],
   "source": [
    "accuracies2 = []\n",
    "times2 = []\n",
    "\n",
    "for i in range(20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=50, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    #model = SiameseBaseNet()\n",
    "    #model = BaseNet()\n",
    "    model = AuxBaseNet()\n",
    "    train(model, train_loader, 0.001, 0, 25, verbose=False, siamese=False, aux=True, alpha = 0.8)\n",
    "    time2 = time.perf_counter()\n",
    "    times2.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies2.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies2).mean(), torch.tensor(accuracies2).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-673-21efc383ef87>:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  aux1 = F.softmax(x1)\n",
      "<ipython-input-673-21efc383ef87>:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  aux2 = F.softmax(x2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.) tensor(0.8550)\n",
      "tensor(0.9990) tensor(0.8650)\n",
      "tensor(0.9970) tensor(0.8350)\n",
      "tensor(0.9980) tensor(0.8560)\n",
      "tensor(0.9990) tensor(0.8670)\n",
      "tensor(0.9990) tensor(0.8540)\n",
      "tensor(0.9990) tensor(0.8580)\n",
      "tensor(1.) tensor(0.8560)\n",
      "tensor(1.) tensor(0.8590)\n",
      "tensor(1.) tensor(0.8530)\n",
      "tensor(0.9910) tensor(0.8660)\n",
      "tensor(1.) tensor(0.8630)\n",
      "tensor(0.9920) tensor(0.8740)\n",
      "tensor(1.) tensor(0.8440)\n",
      "tensor(1.) tensor(0.8570)\n",
      "tensor(0.9870) tensor(0.8540)\n",
      "tensor(0.9980) tensor(0.8770)\n",
      "tensor(0.9890) tensor(0.8290)\n",
      "tensor(1.) tensor(0.8590)\n",
      "tensor(0.9840) tensor(0.8520)\n",
      "Mean: 0.857, Std: 0.011\n"
     ]
    }
   ],
   "source": [
    "accuracies3 = []\n",
    "times3 = []\n",
    "\n",
    "for i in range(20):\n",
    "    train_loader, test_loader = load_data(N=1000, batch_size=50, seed=i)\n",
    "    time1 = time.perf_counter()\n",
    "    #model = SiameseBaseNet()\n",
    "    #model = BaseNet()\n",
    "    model = AuxsiameseBaseNet()\n",
    "    train(model, train_loader, 0.001, 0, 25, verbose=False, siamese=True, aux=True, alpha = 0.8)\n",
    "    time2 = time.perf_counter()\n",
    "    times3.append(time2 - time1)\n",
    "\n",
    "    tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "    te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "    print(tr_accuracy, te_accuracy)\n",
    "    accuracies3.append(te_accuracy)\n",
    "    \n",
    "print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies3).mean(), torch.tensor(accuracies3).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-419-1ee55fdbefe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgammas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mte_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcompute_nb_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0maccurate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mte_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-405-c75a86d473c7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, eta, decay, n_epochs, verbose)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mbinary_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Collect accuracy data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'betas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    109\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gammas = torch.logspace(start=-4, end=-2, steps=5)\n",
    "decays = torch.logspace(start=-13, end=-8, steps=6)\n",
    "accuracies = torch.empty((len(gammas), len(decays)))\n",
    "model = BaseNet()\n",
    "for j in range(len(gammas)):\n",
    "    for k in range(len(decays)):\n",
    "        accurate = []\n",
    "        for i in range(10):\n",
    "            train_loader, test_loader = load_data(N=1000, batch_size=50, seed=42)\n",
    "            train(model, train_loader, gammas[j], decays[k], 25, verbose=False)\n",
    "            te_accuracy = 1 - compute_nb_errors(model, test_loader)/1000\n",
    "            accurate.append(te_accuracy)\n",
    "        accuracies[j,k] = torch.Tensor(accurate).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-699-f890ddbc2f26>:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  aux1 = F.softmax(x1)\n",
      "<ipython-input-699-f890ddbc2f26>:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  aux2 = F.softmax(x2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.838, Std: 0.009\n",
      "Mean: 0.839, Std: 0.011\n",
      "Mean: 0.837, Std: 0.012\n",
      "Mean: 0.844, Std: 0.012\n",
      "Mean: 0.840, Std: 0.011\n",
      "Mean: 0.841, Std: 0.012\n",
      "Mean: 0.838, Std: 0.014\n",
      "Mean: 0.843, Std: 0.016\n",
      "Mean: 0.846, Std: 0.014\n",
      "Mean: 0.846, Std: 0.020\n"
     ]
    }
   ],
   "source": [
    "for j in range(10):\n",
    "    accuracies3 = []\n",
    "    times3 = []\n",
    "\n",
    "    for i in range(10):\n",
    "        train_loader, test_loader = load_data(N=1000, batch_size=50, seed=i)\n",
    "        time1 = time.perf_counter()\n",
    "        #model = SiameseBaseNet()\n",
    "        #model = BaseNet()\n",
    "        model = AuxBaseNet()\n",
    "        train(model, train_loader, 0.001, 0, 25, verbose=False, siamese=False, aux=True, alpha = j/10)\n",
    "        time2 = time.perf_counter()\n",
    "        times3.append(time2 - time1)\n",
    "\n",
    "        tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "        te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "        accuracies3.append(te_accuracy)\n",
    "    print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies3).mean(), torch.tensor(accuracies3).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-673-21efc383ef87>:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  aux1 = F.softmax(x1)\n",
      "<ipython-input-673-21efc383ef87>:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  aux2 = F.softmax(x2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.849, Std: 0.007\n",
      "Mean: 0.851, Std: 0.008\n",
      "Mean: 0.852, Std: 0.011\n",
      "Mean: 0.852, Std: 0.012\n",
      "Mean: 0.852, Std: 0.009\n",
      "Mean: 0.850, Std: 0.013\n",
      "Mean: 0.854, Std: 0.007\n",
      "Mean: 0.842, Std: 0.031\n",
      "Mean: 0.857, Std: 0.009\n",
      "Mean: 0.857, Std: 0.007\n"
     ]
    }
   ],
   "source": [
    "for j in range(10):\n",
    "    accuracies2 = []\n",
    "    times2 = []\n",
    "\n",
    "    for i in range(10):\n",
    "        train_loader, test_loader = load_data(N=1000, batch_size=50, seed=i)\n",
    "        time1 = time.perf_counter()\n",
    "        #model = SiameseBaseNet()\n",
    "        #model = BaseNet()\n",
    "        model = AuxsiameseBaseNet()\n",
    "        train(model, train_loader, 0.001, 0, 25, verbose=False, siamese=True, aux=True, alpha = j/10)\n",
    "        time2 = time.perf_counter()\n",
    "        times2.append(time2 - time1)\n",
    "\n",
    "        tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "        te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "        accuracies2.append(te_accuracy)\n",
    "    print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies2).mean(), torch.tensor(accuracies2).std())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-673-21efc383ef87>:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  aux1 = F.softmax(x1)\n",
      "<ipython-input-673-21efc383ef87>:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  aux2 = F.softmax(x2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.851, Std: 0.013\n",
      "Mean: 0.852, Std: 0.023\n",
      "Mean: 0.851, Std: 0.021\n",
      "Mean: 0.852, Std: 0.019\n",
      "Mean: 0.855, Std: 0.021\n",
      "Mean: 0.849, Std: 0.027\n",
      "Mean: 0.843, Std: 0.038\n",
      "Mean: 0.835, Std: 0.054\n",
      "Mean: 0.852, Std: 0.036\n",
      "Mean: 0.830, Std: 0.054\n"
     ]
    }
   ],
   "source": [
    "for j in range(10,20):\n",
    "    accuracies2 = []\n",
    "    times2 = []\n",
    "\n",
    "    for i in range(10):\n",
    "        train_loader, test_loader = load_data(N=1000, batch_size=50, seed=i)\n",
    "        time1 = time.perf_counter()\n",
    "        #model = SiameseBaseNet()\n",
    "        #model = BaseNet()\n",
    "        model = AuxsiameseBaseNet()\n",
    "        train(model, train_loader, 0.001, 0, 25, verbose=False, siamese=True, aux=True, alpha = j/10)\n",
    "        time2 = time.perf_counter()\n",
    "        times2.append(time2 - time1)\n",
    "\n",
    "        tr_accuracy = 1 - compute_nb_errors_auxsiamese(model, train_loader)/1000\n",
    "        te_accuracy = 1 - compute_nb_errors_auxsiamese(model, test_loader)/1000\n",
    "        accuracies2.append(te_accuracy)\n",
    "    print('Mean: %.3f, Std: %.3f' %(torch.tensor(accuracies2).mean(), torch.tensor(accuracies2).std())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
